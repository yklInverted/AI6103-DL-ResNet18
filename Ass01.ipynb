{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import all libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#torch.manual_seed(0)\n",
    "\n",
    "# these are commonly used data augmentations\n",
    "# random cropping and random horizontal flip\n",
    "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=(0.4914, 0.4822, 0.4465), inplace=False),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "  \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "tempset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "trainset, valset = data.random_split(tempset, [40000, 10000], generator= torch.Generator().manual_seed(0))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    valset, batch_size=128, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=False, transform=transform_test)\n",
    "\n",
    "# we can use a larger batch size during test, because we do not save \n",
    "# intermediate variables for gradient computation, which leaves more memory\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2472ae65b70>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trainset[4][0].permute((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices = train_test_split(list(range(len(tempset.targets))), test_size=0.2, random_state = 0, stratify=tempset.targets)\n",
    "trainset = torch.utils.data.Subset(tempset, train_indices)\n",
    "valset = torch.utils.data.Subset(tempset, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_temp = np.zeros(len(classes))\n",
    "count_train = np.zeros(len(classes))\n",
    "\n",
    "for im, label in trainset:\n",
    "    count_temp[label] += 1\n",
    "    count_train[label] += 1\n",
    "\n",
    "for im, label in valset:\n",
    "    count_temp[label] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.bar(classes, count_temp, color = 'b', label = 'lala')\n",
    "plt.bar(classes, count_train, color = 'y', label = 'haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "       4000.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.array([4014., 4002., 3995., 3977., 3982., 4019., 3975., 4010., 4010.,\n",
    "       4016.]);\n",
    "\n",
    "count_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch, net, criterion, trainloader, scheduler, optimizer):\n",
    "    device = 'cuda'\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if (batch_idx+1) % 50 == 0:\n",
    "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
    "\n",
    "    scheduler.step()\n",
    "    return train_loss/(batch_idx+1), 100.*correct/total\n",
    "\n",
    "def test(epoch, net, criterion, testloader):\n",
    "    device = 'cuda'\n",
    "    net.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return valid_loss/(batch_idx+1), 100.*correct/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(net, acc, epoch):\n",
    "    # Save checkpoint.\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/ckpt.pth')\n",
    "\n",
    "def save_result(name, train_loss, valid_loss, test_loss, train_acc, valid_acc, test_acc):\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'tl': train_loss,\n",
    "        'vl': valid_loss,\n",
    "        'fl': test_loss,\n",
    "        'ta': train_acc,  \n",
    "        'va': valid_acc,  \n",
    "        'fa': test_acc,\n",
    "    }\n",
    "\n",
    "    if not os.path.isdir('result'):\n",
    "        os.mkdir('result')\n",
    "    torch.save(state, f'./result/{name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining resnet models\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # This is the \"stem\"\n",
    "        # For CIFAR (32x32 images), it does not perform downsampling\n",
    "        # It should downsample for ImageNet\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # four stages with three downsampling\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test_resnet18():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "iteration :  50, loss : 1.9753, accuracy : 26.61\n",
      "iteration : 100, loss : 1.8615, accuracy : 31.40\n",
      "iteration : 150, loss : 1.7730, accuracy : 34.62\n",
      "iteration : 200, loss : 1.7167, accuracy : 36.92\n",
      "iteration : 250, loss : 1.6577, accuracy : 39.26\n",
      "iteration : 300, loss : 1.6051, accuracy : 41.20\n",
      "Epoch :   0, training loss : 1.5930, training accuracy : 41.66, validation loss : 1.5226, validation accuracy : 46.10\n",
      "\n",
      "Epoch: 1\n",
      "iteration :  50, loss : 1.2430, accuracy : 54.25\n",
      "iteration : 100, loss : 1.2244, accuracy : 55.10\n",
      "iteration : 150, loss : 1.2031, accuracy : 56.28\n",
      "iteration : 200, loss : 1.1850, accuracy : 56.93\n",
      "iteration : 250, loss : 1.1712, accuracy : 57.68\n",
      "iteration : 300, loss : 1.1558, accuracy : 58.41\n",
      "Epoch :   1, training loss : 1.1515, training accuracy : 58.59, validation loss : 1.1926, validation accuracy : 59.13\n",
      "\n",
      "Epoch: 2\n",
      "iteration :  50, loss : 1.0270, accuracy : 63.55\n",
      "iteration : 100, loss : 1.0146, accuracy : 64.12\n",
      "iteration : 150, loss : 1.0007, accuracy : 64.38\n",
      "iteration : 200, loss : 0.9909, accuracy : 64.82\n",
      "iteration : 250, loss : 0.9776, accuracy : 65.32\n",
      "iteration : 300, loss : 0.9659, accuracy : 65.73\n",
      "Epoch :   2, training loss : 0.9627, training accuracy : 65.86, validation loss : 1.0422, validation accuracy : 64.24\n",
      "\n",
      "Epoch: 3\n",
      "iteration :  50, loss : 0.8432, accuracy : 70.27\n",
      "iteration : 100, loss : 0.8560, accuracy : 69.96\n",
      "iteration : 150, loss : 0.8423, accuracy : 70.51\n",
      "iteration : 200, loss : 0.8434, accuracy : 70.36\n",
      "iteration : 250, loss : 0.8368, accuracy : 70.51\n",
      "iteration : 300, loss : 0.8257, accuracy : 70.96\n",
      "Epoch :   3, training loss : 0.8224, training accuracy : 71.05, validation loss : 0.9760, validation accuracy : 66.74\n",
      "\n",
      "Epoch: 4\n",
      "iteration :  50, loss : 0.7798, accuracy : 72.53\n",
      "iteration : 100, loss : 0.7769, accuracy : 72.66\n",
      "iteration : 150, loss : 0.7635, accuracy : 73.08\n",
      "iteration : 200, loss : 0.7519, accuracy : 73.53\n",
      "iteration : 250, loss : 0.7459, accuracy : 73.68\n",
      "iteration : 300, loss : 0.7394, accuracy : 73.92\n",
      "Epoch :   4, training loss : 0.7360, training accuracy : 73.99, validation loss : 0.8619, validation accuracy : 70.33\n",
      "\n",
      "Epoch: 5\n",
      "iteration :  50, loss : 0.6959, accuracy : 75.22\n",
      "iteration : 100, loss : 0.6928, accuracy : 75.20\n",
      "iteration : 150, loss : 0.6823, accuracy : 75.56\n",
      "iteration : 200, loss : 0.6819, accuracy : 75.83\n",
      "iteration : 250, loss : 0.6769, accuracy : 75.97\n",
      "iteration : 300, loss : 0.6730, accuracy : 76.17\n",
      "Epoch :   5, training loss : 0.6707, training accuracy : 76.26, validation loss : 0.7125, validation accuracy : 75.02\n",
      "\n",
      "Epoch: 6\n",
      "iteration :  50, loss : 0.6124, accuracy : 78.52\n",
      "iteration : 100, loss : 0.6093, accuracy : 78.59\n",
      "iteration : 150, loss : 0.6130, accuracy : 78.55\n",
      "iteration : 200, loss : 0.6138, accuracy : 78.60\n",
      "iteration : 250, loss : 0.6165, accuracy : 78.51\n",
      "iteration : 300, loss : 0.6189, accuracy : 78.48\n",
      "Epoch :   6, training loss : 0.6173, training accuracy : 78.53, validation loss : 0.7502, validation accuracy : 74.96\n",
      "\n",
      "Epoch: 7\n",
      "iteration :  50, loss : 0.5956, accuracy : 78.59\n",
      "iteration : 100, loss : 0.5764, accuracy : 79.68\n",
      "iteration : 150, loss : 0.5799, accuracy : 79.51\n",
      "iteration : 200, loss : 0.5825, accuracy : 79.54\n",
      "iteration : 250, loss : 0.5838, accuracy : 79.60\n",
      "iteration : 300, loss : 0.5774, accuracy : 79.82\n",
      "Epoch :   7, training loss : 0.5773, training accuracy : 79.83, validation loss : 0.7044, validation accuracy : 76.07\n",
      "\n",
      "Epoch: 8\n",
      "iteration :  50, loss : 0.5724, accuracy : 79.81\n",
      "iteration : 100, loss : 0.5553, accuracy : 80.40\n",
      "iteration : 150, loss : 0.5437, accuracy : 80.81\n",
      "iteration : 200, loss : 0.5407, accuracy : 80.89\n",
      "iteration : 250, loss : 0.5396, accuracy : 80.96\n",
      "iteration : 300, loss : 0.5398, accuracy : 80.95\n",
      "Epoch :   8, training loss : 0.5400, training accuracy : 80.95, validation loss : 0.5900, validation accuracy : 79.43\n",
      "\n",
      "Epoch: 9\n",
      "iteration :  50, loss : 0.5030, accuracy : 82.02\n",
      "iteration : 100, loss : 0.5150, accuracy : 81.73\n",
      "iteration : 150, loss : 0.5132, accuracy : 81.94\n",
      "iteration : 200, loss : 0.5222, accuracy : 81.69\n",
      "iteration : 250, loss : 0.5215, accuracy : 81.72\n",
      "iteration : 300, loss : 0.5189, accuracy : 81.79\n",
      "Epoch :   9, training loss : 0.5173, training accuracy : 81.86, validation loss : 0.6536, validation accuracy : 77.38\n",
      "\n",
      "Epoch: 10\n",
      "iteration :  50, loss : 0.4636, accuracy : 83.66\n",
      "iteration : 100, loss : 0.4655, accuracy : 83.69\n",
      "iteration : 150, loss : 0.4713, accuracy : 83.57\n",
      "iteration : 200, loss : 0.4739, accuracy : 83.53\n",
      "iteration : 250, loss : 0.4787, accuracy : 83.22\n",
      "iteration : 300, loss : 0.4752, accuracy : 83.32\n",
      "Epoch :  10, training loss : 0.4766, training accuracy : 83.27, validation loss : 0.5940, validation accuracy : 79.87\n",
      "\n",
      "Epoch: 11\n",
      "iteration :  50, loss : 0.4623, accuracy : 83.83\n",
      "iteration : 100, loss : 0.4616, accuracy : 84.05\n",
      "iteration : 150, loss : 0.4497, accuracy : 84.30\n",
      "iteration : 200, loss : 0.4528, accuracy : 84.13\n",
      "iteration : 250, loss : 0.4537, accuracy : 84.14\n",
      "iteration : 300, loss : 0.4536, accuracy : 84.20\n",
      "Epoch :  11, training loss : 0.4537, training accuracy : 84.22, validation loss : 0.5676, validation accuracy : 80.48\n",
      "\n",
      "Epoch: 12\n",
      "iteration :  50, loss : 0.4370, accuracy : 84.53\n",
      "iteration : 100, loss : 0.4369, accuracy : 84.53\n",
      "iteration : 150, loss : 0.4331, accuracy : 84.83\n",
      "iteration : 200, loss : 0.4385, accuracy : 84.64\n",
      "iteration : 250, loss : 0.4398, accuracy : 84.67\n",
      "iteration : 300, loss : 0.4388, accuracy : 84.63\n",
      "Epoch :  12, training loss : 0.4380, training accuracy : 84.64, validation loss : 0.5682, validation accuracy : 80.88\n",
      "\n",
      "Epoch: 13\n",
      "iteration :  50, loss : 0.4041, accuracy : 86.08\n",
      "iteration : 100, loss : 0.4081, accuracy : 85.86\n",
      "iteration : 150, loss : 0.4142, accuracy : 85.62\n",
      "iteration : 200, loss : 0.4186, accuracy : 85.50\n",
      "iteration : 250, loss : 0.4198, accuracy : 85.45\n",
      "iteration : 300, loss : 0.4222, accuracy : 85.37\n",
      "Epoch :  13, training loss : 0.4231, training accuracy : 85.34, validation loss : 0.5886, validation accuracy : 80.45\n",
      "\n",
      "Epoch: 14\n",
      "iteration :  50, loss : 0.3801, accuracy : 86.80\n",
      "iteration : 100, loss : 0.3920, accuracy : 86.21\n",
      "iteration : 150, loss : 0.3934, accuracy : 86.24\n",
      "iteration : 200, loss : 0.3930, accuracy : 86.21\n",
      "iteration : 250, loss : 0.3958, accuracy : 86.08\n",
      "iteration : 300, loss : 0.3998, accuracy : 86.03\n",
      "Epoch :  14, training loss : 0.3997, training accuracy : 86.06, validation loss : 0.6354, validation accuracy : 79.27\n",
      "\n",
      "Epoch: 15\n",
      "iteration :  50, loss : 0.3657, accuracy : 87.27\n",
      "iteration : 100, loss : 0.3724, accuracy : 86.89\n",
      "iteration : 150, loss : 0.3718, accuracy : 86.81\n",
      "iteration : 200, loss : 0.3733, accuracy : 86.72\n",
      "iteration : 250, loss : 0.3760, accuracy : 86.71\n",
      "iteration : 300, loss : 0.3755, accuracy : 86.82\n",
      "Epoch :  15, training loss : 0.3762, training accuracy : 86.81, validation loss : 0.6399, validation accuracy : 80.07\n",
      "\n",
      "Epoch: 16\n",
      "iteration :  50, loss : 0.3585, accuracy : 87.36\n",
      "iteration : 100, loss : 0.3675, accuracy : 86.76\n",
      "iteration : 150, loss : 0.3667, accuracy : 86.92\n",
      "iteration : 200, loss : 0.3650, accuracy : 86.95\n",
      "iteration : 250, loss : 0.3708, accuracy : 86.78\n",
      "iteration : 300, loss : 0.3688, accuracy : 86.90\n",
      "Epoch :  16, training loss : 0.3684, training accuracy : 86.93, validation loss : 0.4568, validation accuracy : 84.63\n",
      "\n",
      "Epoch: 17\n",
      "iteration :  50, loss : 0.3563, accuracy : 87.16\n",
      "iteration : 100, loss : 0.3575, accuracy : 87.29\n",
      "iteration : 150, loss : 0.3602, accuracy : 87.31\n",
      "iteration : 200, loss : 0.3620, accuracy : 87.13\n",
      "iteration : 250, loss : 0.3619, accuracy : 87.16\n",
      "iteration : 300, loss : 0.3641, accuracy : 87.15\n",
      "Epoch :  17, training loss : 0.3639, training accuracy : 87.19, validation loss : 0.4779, validation accuracy : 83.40\n",
      "\n",
      "Epoch: 18\n",
      "iteration :  50, loss : 0.3323, accuracy : 88.47\n",
      "iteration : 100, loss : 0.3429, accuracy : 88.18\n",
      "iteration : 150, loss : 0.3400, accuracy : 88.17\n",
      "iteration : 200, loss : 0.3385, accuracy : 88.17\n",
      "iteration : 250, loss : 0.3400, accuracy : 88.14\n",
      "iteration : 300, loss : 0.3422, accuracy : 88.04\n",
      "Epoch :  18, training loss : 0.3434, training accuracy : 88.00, validation loss : 0.5126, validation accuracy : 82.96\n",
      "\n",
      "Epoch: 19\n",
      "iteration :  50, loss : 0.3386, accuracy : 88.56\n",
      "iteration : 100, loss : 0.3356, accuracy : 88.58\n",
      "iteration : 150, loss : 0.3364, accuracy : 88.50\n",
      "iteration : 200, loss : 0.3352, accuracy : 88.36\n",
      "iteration : 250, loss : 0.3335, accuracy : 88.38\n",
      "iteration : 300, loss : 0.3327, accuracy : 88.41\n",
      "Epoch :  19, training loss : 0.3331, training accuracy : 88.38, validation loss : 0.5517, validation accuracy : 81.78\n",
      "\n",
      "Epoch: 20\n",
      "iteration :  50, loss : 0.3257, accuracy : 88.36\n",
      "iteration : 100, loss : 0.3229, accuracy : 88.77\n",
      "iteration : 150, loss : 0.3259, accuracy : 88.64\n",
      "iteration : 200, loss : 0.3257, accuracy : 88.65\n",
      "iteration : 250, loss : 0.3237, accuracy : 88.73\n",
      "iteration : 300, loss : 0.3250, accuracy : 88.70\n",
      "Epoch :  20, training loss : 0.3247, training accuracy : 88.71, validation loss : 0.4454, validation accuracy : 84.75\n",
      "\n",
      "Epoch: 21\n",
      "iteration :  50, loss : 0.3039, accuracy : 89.22\n",
      "iteration : 100, loss : 0.3006, accuracy : 89.44\n",
      "iteration : 150, loss : 0.3061, accuracy : 89.19\n",
      "iteration : 200, loss : 0.3087, accuracy : 89.06\n",
      "iteration : 250, loss : 0.3170, accuracy : 88.78\n",
      "iteration : 300, loss : 0.3180, accuracy : 88.74\n",
      "Epoch :  21, training loss : 0.3180, training accuracy : 88.77, validation loss : 0.5755, validation accuracy : 81.83\n",
      "\n",
      "Epoch: 22\n",
      "iteration :  50, loss : 0.2866, accuracy : 90.05\n",
      "iteration : 100, loss : 0.2978, accuracy : 89.70\n",
      "iteration : 150, loss : 0.2993, accuracy : 89.55\n",
      "iteration : 200, loss : 0.3033, accuracy : 89.52\n",
      "iteration : 250, loss : 0.3036, accuracy : 89.46\n",
      "iteration : 300, loss : 0.3014, accuracy : 89.56\n",
      "Epoch :  22, training loss : 0.3022, training accuracy : 89.50, validation loss : 0.4882, validation accuracy : 84.00\n",
      "\n",
      "Epoch: 23\n",
      "iteration :  50, loss : 0.2876, accuracy : 90.31\n",
      "iteration : 100, loss : 0.2888, accuracy : 90.02\n",
      "iteration : 150, loss : 0.2900, accuracy : 89.93\n",
      "iteration : 200, loss : 0.2920, accuracy : 89.80\n",
      "iteration : 250, loss : 0.2934, accuracy : 89.69\n",
      "iteration : 300, loss : 0.2947, accuracy : 89.68\n",
      "Epoch :  23, training loss : 0.2951, training accuracy : 89.67, validation loss : 0.4571, validation accuracy : 84.67\n",
      "\n",
      "Epoch: 24\n",
      "iteration :  50, loss : 0.2801, accuracy : 90.25\n",
      "iteration : 100, loss : 0.2776, accuracy : 90.28\n",
      "iteration : 150, loss : 0.2782, accuracy : 90.22\n",
      "iteration : 200, loss : 0.2824, accuracy : 90.05\n",
      "iteration : 250, loss : 0.2867, accuracy : 89.85\n",
      "iteration : 300, loss : 0.2850, accuracy : 89.92\n",
      "Epoch :  24, training loss : 0.2846, training accuracy : 89.93, validation loss : 0.5654, validation accuracy : 81.72\n",
      "\n",
      "Epoch: 25\n",
      "iteration :  50, loss : 0.2785, accuracy : 90.34\n",
      "iteration : 100, loss : 0.2726, accuracy : 90.51\n",
      "iteration : 150, loss : 0.2734, accuracy : 90.52\n",
      "iteration : 200, loss : 0.2757, accuracy : 90.43\n",
      "iteration : 250, loss : 0.2781, accuracy : 90.27\n",
      "iteration : 300, loss : 0.2822, accuracy : 90.10\n",
      "Epoch :  25, training loss : 0.2815, training accuracy : 90.12, validation loss : 0.4819, validation accuracy : 83.80\n",
      "\n",
      "Epoch: 26\n",
      "iteration :  50, loss : 0.2663, accuracy : 90.84\n",
      "iteration : 100, loss : 0.2571, accuracy : 91.20\n",
      "iteration : 150, loss : 0.2586, accuracy : 91.03\n",
      "iteration : 200, loss : 0.2627, accuracy : 90.86\n",
      "iteration : 250, loss : 0.2672, accuracy : 90.65\n",
      "iteration : 300, loss : 0.2688, accuracy : 90.61\n",
      "Epoch :  26, training loss : 0.2692, training accuracy : 90.60, validation loss : 0.5155, validation accuracy : 83.68\n",
      "\n",
      "Epoch: 27\n",
      "iteration :  50, loss : 0.2592, accuracy : 90.89\n",
      "iteration : 100, loss : 0.2530, accuracy : 90.94\n",
      "iteration : 150, loss : 0.2628, accuracy : 90.65\n",
      "iteration : 200, loss : 0.2594, accuracy : 90.75\n",
      "iteration : 250, loss : 0.2639, accuracy : 90.64\n",
      "iteration : 300, loss : 0.2642, accuracy : 90.61\n",
      "Epoch :  27, training loss : 0.2657, training accuracy : 90.55, validation loss : 0.5212, validation accuracy : 83.39\n",
      "\n",
      "Epoch: 28\n",
      "iteration :  50, loss : 0.2542, accuracy : 91.16\n",
      "iteration : 100, loss : 0.2438, accuracy : 91.39\n",
      "iteration : 150, loss : 0.2489, accuracy : 91.21\n",
      "iteration : 200, loss : 0.2556, accuracy : 91.01\n",
      "iteration : 250, loss : 0.2597, accuracy : 90.90\n",
      "iteration : 300, loss : 0.2606, accuracy : 90.92\n",
      "Epoch :  28, training loss : 0.2616, training accuracy : 90.84, validation loss : 0.4399, validation accuracy : 85.73\n",
      "\n",
      "Epoch: 29\n",
      "iteration :  50, loss : 0.2369, accuracy : 91.53\n",
      "iteration : 100, loss : 0.2398, accuracy : 91.52\n",
      "iteration : 150, loss : 0.2432, accuracy : 91.42\n",
      "iteration : 200, loss : 0.2460, accuracy : 91.22\n",
      "iteration : 250, loss : 0.2467, accuracy : 91.21\n",
      "iteration : 300, loss : 0.2522, accuracy : 91.04\n",
      "Epoch :  29, training loss : 0.2532, training accuracy : 91.04, validation loss : 0.4839, validation accuracy : 84.50\n",
      "\n",
      "Epoch: 30\n",
      "iteration :  50, loss : 0.2411, accuracy : 91.12\n",
      "iteration : 100, loss : 0.2446, accuracy : 91.30\n",
      "iteration : 150, loss : 0.2433, accuracy : 91.33\n",
      "iteration : 200, loss : 0.2445, accuracy : 91.22\n",
      "iteration : 250, loss : 0.2414, accuracy : 91.35\n",
      "iteration : 300, loss : 0.2423, accuracy : 91.28\n",
      "Epoch :  30, training loss : 0.2430, training accuracy : 91.28, validation loss : 0.4865, validation accuracy : 84.47\n",
      "\n",
      "Epoch: 31\n",
      "iteration :  50, loss : 0.2386, accuracy : 91.50\n",
      "iteration : 100, loss : 0.2351, accuracy : 91.63\n",
      "iteration : 150, loss : 0.2386, accuracy : 91.58\n",
      "iteration : 200, loss : 0.2408, accuracy : 91.50\n",
      "iteration : 250, loss : 0.2398, accuracy : 91.56\n",
      "iteration : 300, loss : 0.2391, accuracy : 91.53\n",
      "Epoch :  31, training loss : 0.2384, training accuracy : 91.54, validation loss : 0.4039, validation accuracy : 87.20\n",
      "\n",
      "Epoch: 32\n",
      "iteration :  50, loss : 0.2366, accuracy : 91.48\n",
      "iteration : 100, loss : 0.2320, accuracy : 91.82\n",
      "iteration : 150, loss : 0.2323, accuracy : 91.80\n",
      "iteration : 200, loss : 0.2342, accuracy : 91.73\n",
      "iteration : 250, loss : 0.2343, accuracy : 91.71\n",
      "iteration : 300, loss : 0.2324, accuracy : 91.85\n",
      "Epoch :  32, training loss : 0.2327, training accuracy : 91.81, validation loss : 0.4578, validation accuracy : 85.31\n",
      "\n",
      "Epoch: 33\n",
      "iteration :  50, loss : 0.2154, accuracy : 92.22\n",
      "iteration : 100, loss : 0.2151, accuracy : 92.24\n",
      "iteration : 150, loss : 0.2136, accuracy : 92.37\n",
      "iteration : 200, loss : 0.2138, accuracy : 92.41\n",
      "iteration : 250, loss : 0.2199, accuracy : 92.28\n",
      "iteration : 300, loss : 0.2251, accuracy : 92.13\n",
      "Epoch :  33, training loss : 0.2259, training accuracy : 92.09, validation loss : 0.5211, validation accuracy : 83.53\n",
      "\n",
      "Epoch: 34\n",
      "iteration :  50, loss : 0.2169, accuracy : 92.48\n",
      "iteration : 100, loss : 0.2126, accuracy : 92.64\n",
      "iteration : 150, loss : 0.2146, accuracy : 92.66\n",
      "iteration : 200, loss : 0.2177, accuracy : 92.47\n",
      "iteration : 250, loss : 0.2190, accuracy : 92.37\n",
      "iteration : 300, loss : 0.2208, accuracy : 92.31\n",
      "Epoch :  34, training loss : 0.2217, training accuracy : 92.30, validation loss : 0.4252, validation accuracy : 86.40\n",
      "\n",
      "Epoch: 35\n",
      "iteration :  50, loss : 0.2133, accuracy : 92.78\n",
      "iteration : 100, loss : 0.2148, accuracy : 92.75\n",
      "iteration : 150, loss : 0.2184, accuracy : 92.54\n",
      "iteration : 200, loss : 0.2161, accuracy : 92.60\n",
      "iteration : 250, loss : 0.2169, accuracy : 92.58\n",
      "iteration : 300, loss : 0.2172, accuracy : 92.53\n",
      "Epoch :  35, training loss : 0.2168, training accuracy : 92.55, validation loss : 0.4126, validation accuracy : 86.26\n",
      "\n",
      "Epoch: 36\n",
      "iteration :  50, loss : 0.2072, accuracy : 92.72\n",
      "iteration : 100, loss : 0.2179, accuracy : 92.38\n",
      "iteration : 150, loss : 0.2206, accuracy : 92.23\n",
      "iteration : 200, loss : 0.2209, accuracy : 92.21\n",
      "iteration : 250, loss : 0.2223, accuracy : 92.24\n",
      "iteration : 300, loss : 0.2217, accuracy : 92.26\n",
      "Epoch :  36, training loss : 0.2226, training accuracy : 92.23, validation loss : 0.5339, validation accuracy : 83.79\n",
      "\n",
      "Epoch: 37\n",
      "iteration :  50, loss : 0.1928, accuracy : 93.28\n",
      "iteration : 100, loss : 0.1924, accuracy : 93.28\n",
      "iteration : 150, loss : 0.1968, accuracy : 93.12\n",
      "iteration : 200, loss : 0.1960, accuracy : 93.10\n",
      "iteration : 250, loss : 0.1995, accuracy : 92.96\n",
      "iteration : 300, loss : 0.2020, accuracy : 92.87\n",
      "Epoch :  37, training loss : 0.2017, training accuracy : 92.89, validation loss : 0.4222, validation accuracy : 86.04\n",
      "\n",
      "Epoch: 38\n",
      "iteration :  50, loss : 0.2120, accuracy : 92.59\n",
      "iteration : 100, loss : 0.2173, accuracy : 92.41\n",
      "iteration : 150, loss : 0.2165, accuracy : 92.44\n",
      "iteration : 200, loss : 0.2136, accuracy : 92.49\n",
      "iteration : 250, loss : 0.2126, accuracy : 92.56\n",
      "iteration : 300, loss : 0.2152, accuracy : 92.48\n",
      "Epoch :  38, training loss : 0.2146, training accuracy : 92.51, validation loss : 0.4187, validation accuracy : 86.37\n",
      "\n",
      "Epoch: 39\n",
      "iteration :  50, loss : 0.2074, accuracy : 93.12\n",
      "iteration : 100, loss : 0.2008, accuracy : 93.05\n",
      "iteration : 150, loss : 0.1982, accuracy : 93.06\n",
      "iteration : 200, loss : 0.1986, accuracy : 93.09\n",
      "iteration : 250, loss : 0.1970, accuracy : 93.12\n",
      "iteration : 300, loss : 0.1981, accuracy : 93.09\n",
      "Epoch :  39, training loss : 0.1993, training accuracy : 93.04, validation loss : 0.4241, validation accuracy : 86.62\n",
      "\n",
      "Epoch: 40\n",
      "iteration :  50, loss : 0.1930, accuracy : 93.41\n",
      "iteration : 100, loss : 0.1922, accuracy : 93.27\n",
      "iteration : 150, loss : 0.2007, accuracy : 93.05\n",
      "iteration : 200, loss : 0.2000, accuracy : 93.06\n",
      "iteration : 250, loss : 0.2007, accuracy : 93.05\n",
      "iteration : 300, loss : 0.2008, accuracy : 93.03\n",
      "Epoch :  40, training loss : 0.2008, training accuracy : 93.04, validation loss : 0.4038, validation accuracy : 86.81\n",
      "\n",
      "Epoch: 41\n",
      "iteration :  50, loss : 0.1867, accuracy : 93.41\n",
      "iteration : 100, loss : 0.1851, accuracy : 93.44\n",
      "iteration : 150, loss : 0.1842, accuracy : 93.47\n",
      "iteration : 200, loss : 0.1816, accuracy : 93.60\n",
      "iteration : 250, loss : 0.1865, accuracy : 93.44\n",
      "iteration : 300, loss : 0.1880, accuracy : 93.41\n",
      "Epoch :  41, training loss : 0.1878, training accuracy : 93.38, validation loss : 0.4120, validation accuracy : 86.87\n",
      "\n",
      "Epoch: 42\n",
      "iteration :  50, loss : 0.1758, accuracy : 94.00\n",
      "iteration : 100, loss : 0.1795, accuracy : 93.75\n",
      "iteration : 150, loss : 0.1845, accuracy : 93.58\n",
      "iteration : 200, loss : 0.1861, accuracy : 93.54\n",
      "iteration : 250, loss : 0.1858, accuracy : 93.53\n",
      "iteration : 300, loss : 0.1889, accuracy : 93.41\n",
      "Epoch :  42, training loss : 0.1891, training accuracy : 93.39, validation loss : 0.3848, validation accuracy : 87.78\n",
      "\n",
      "Epoch: 43\n",
      "iteration :  50, loss : 0.1794, accuracy : 93.59\n",
      "iteration : 100, loss : 0.1831, accuracy : 93.72\n",
      "iteration : 150, loss : 0.1865, accuracy : 93.55\n",
      "iteration : 200, loss : 0.1867, accuracy : 93.57\n",
      "iteration : 250, loss : 0.1856, accuracy : 93.59\n",
      "iteration : 300, loss : 0.1858, accuracy : 93.56\n",
      "Epoch :  43, training loss : 0.1861, training accuracy : 93.56, validation loss : 0.4753, validation accuracy : 84.95\n",
      "\n",
      "Epoch: 44\n",
      "iteration :  50, loss : 0.1786, accuracy : 93.50\n",
      "iteration : 100, loss : 0.1774, accuracy : 93.60\n",
      "iteration : 150, loss : 0.1784, accuracy : 93.63\n",
      "iteration : 200, loss : 0.1781, accuracy : 93.70\n",
      "iteration : 250, loss : 0.1809, accuracy : 93.58\n",
      "iteration : 300, loss : 0.1840, accuracy : 93.46\n",
      "Epoch :  44, training loss : 0.1853, training accuracy : 93.42, validation loss : 0.4413, validation accuracy : 86.39\n",
      "\n",
      "Epoch: 45\n",
      "iteration :  50, loss : 0.1827, accuracy : 93.55\n",
      "iteration : 100, loss : 0.1722, accuracy : 94.00\n",
      "iteration : 150, loss : 0.1693, accuracy : 94.12\n",
      "iteration : 200, loss : 0.1729, accuracy : 93.98\n",
      "iteration : 250, loss : 0.1745, accuracy : 93.97\n",
      "iteration : 300, loss : 0.1753, accuracy : 93.92\n",
      "Epoch :  45, training loss : 0.1745, training accuracy : 93.95, validation loss : 0.4349, validation accuracy : 86.26\n",
      "\n",
      "Epoch: 46\n",
      "iteration :  50, loss : 0.1647, accuracy : 94.05\n",
      "iteration : 100, loss : 0.1707, accuracy : 94.00\n",
      "iteration : 150, loss : 0.1705, accuracy : 93.99\n",
      "iteration : 200, loss : 0.1737, accuracy : 93.85\n",
      "iteration : 250, loss : 0.1796, accuracy : 93.68\n",
      "iteration : 300, loss : 0.1802, accuracy : 93.62\n",
      "Epoch :  46, training loss : 0.1803, training accuracy : 93.63, validation loss : 0.4040, validation accuracy : 86.81\n",
      "\n",
      "Epoch: 47\n",
      "iteration :  50, loss : 0.1702, accuracy : 93.91\n",
      "iteration : 100, loss : 0.1724, accuracy : 93.97\n",
      "iteration : 150, loss : 0.1736, accuracy : 93.92\n",
      "iteration : 200, loss : 0.1748, accuracy : 93.88\n",
      "iteration : 250, loss : 0.1774, accuracy : 93.71\n",
      "iteration : 300, loss : 0.1777, accuracy : 93.71\n",
      "Epoch :  47, training loss : 0.1784, training accuracy : 93.69, validation loss : 0.4298, validation accuracy : 86.58\n",
      "\n",
      "Epoch: 48\n",
      "iteration :  50, loss : 0.1688, accuracy : 94.02\n",
      "iteration : 100, loss : 0.1681, accuracy : 94.00\n",
      "iteration : 150, loss : 0.1692, accuracy : 94.05\n",
      "iteration : 200, loss : 0.1724, accuracy : 94.00\n",
      "iteration : 250, loss : 0.1740, accuracy : 93.92\n",
      "iteration : 300, loss : 0.1724, accuracy : 93.97\n",
      "Epoch :  48, training loss : 0.1733, training accuracy : 93.93, validation loss : 0.4362, validation accuracy : 86.69\n",
      "\n",
      "Epoch: 49\n",
      "iteration :  50, loss : 0.1635, accuracy : 94.44\n",
      "iteration : 100, loss : 0.1667, accuracy : 94.19\n",
      "iteration : 150, loss : 0.1692, accuracy : 94.10\n",
      "iteration : 200, loss : 0.1661, accuracy : 94.17\n",
      "iteration : 250, loss : 0.1690, accuracy : 94.09\n",
      "iteration : 300, loss : 0.1676, accuracy : 94.16\n",
      "Epoch :  49, training loss : 0.1678, training accuracy : 94.14, validation loss : 0.4160, validation accuracy : 86.73\n",
      "\n",
      "Epoch: 50\n",
      "iteration :  50, loss : 0.1631, accuracy : 94.02\n",
      "iteration : 100, loss : 0.1611, accuracy : 94.23\n",
      "iteration : 150, loss : 0.1666, accuracy : 94.06\n",
      "iteration : 200, loss : 0.1699, accuracy : 93.93\n",
      "iteration : 250, loss : 0.1750, accuracy : 93.71\n",
      "iteration : 300, loss : 0.1747, accuracy : 93.75\n",
      "Epoch :  50, training loss : 0.1754, training accuracy : 93.74, validation loss : 0.3974, validation accuracy : 87.75\n",
      "\n",
      "Epoch: 51\n",
      "iteration :  50, loss : 0.1609, accuracy : 94.36\n",
      "iteration : 100, loss : 0.1586, accuracy : 94.56\n",
      "iteration : 150, loss : 0.1622, accuracy : 94.46\n",
      "iteration : 200, loss : 0.1607, accuracy : 94.45\n",
      "iteration : 250, loss : 0.1615, accuracy : 94.37\n",
      "iteration : 300, loss : 0.1634, accuracy : 94.30\n",
      "Epoch :  51, training loss : 0.1633, training accuracy : 94.28, validation loss : 0.4005, validation accuracy : 87.14\n",
      "\n",
      "Epoch: 52\n",
      "iteration :  50, loss : 0.1432, accuracy : 95.09\n",
      "iteration : 100, loss : 0.1495, accuracy : 94.81\n",
      "iteration : 150, loss : 0.1561, accuracy : 94.66\n",
      "iteration : 200, loss : 0.1546, accuracy : 94.68\n",
      "iteration : 250, loss : 0.1560, accuracy : 94.58\n",
      "iteration : 300, loss : 0.1567, accuracy : 94.57\n",
      "Epoch :  52, training loss : 0.1574, training accuracy : 94.56, validation loss : 0.4009, validation accuracy : 87.52\n",
      "\n",
      "Epoch: 53\n",
      "iteration :  50, loss : 0.1426, accuracy : 95.44\n",
      "iteration : 100, loss : 0.1492, accuracy : 94.91\n",
      "iteration : 150, loss : 0.1546, accuracy : 94.67\n",
      "iteration : 200, loss : 0.1581, accuracy : 94.52\n",
      "iteration : 250, loss : 0.1598, accuracy : 94.47\n",
      "iteration : 300, loss : 0.1602, accuracy : 94.46\n",
      "Epoch :  53, training loss : 0.1604, training accuracy : 94.47, validation loss : 0.3854, validation accuracy : 87.87\n",
      "\n",
      "Epoch: 54\n",
      "iteration :  50, loss : 0.1497, accuracy : 95.03\n",
      "iteration : 100, loss : 0.1399, accuracy : 95.32\n",
      "iteration : 150, loss : 0.1415, accuracy : 95.21\n",
      "iteration : 200, loss : 0.1468, accuracy : 94.91\n",
      "iteration : 250, loss : 0.1494, accuracy : 94.84\n",
      "iteration : 300, loss : 0.1520, accuracy : 94.74\n",
      "Epoch :  54, training loss : 0.1525, training accuracy : 94.73, validation loss : 0.3958, validation accuracy : 87.96\n",
      "\n",
      "Epoch: 55\n",
      "iteration :  50, loss : 0.1400, accuracy : 95.03\n",
      "iteration : 100, loss : 0.1475, accuracy : 94.89\n",
      "iteration : 150, loss : 0.1470, accuracy : 94.78\n",
      "iteration : 200, loss : 0.1501, accuracy : 94.69\n",
      "iteration : 250, loss : 0.1555, accuracy : 94.45\n",
      "iteration : 300, loss : 0.1574, accuracy : 94.45\n",
      "Epoch :  55, training loss : 0.1560, training accuracy : 94.47, validation loss : 0.3831, validation accuracy : 88.08\n",
      "\n",
      "Epoch: 56\n",
      "iteration :  50, loss : 0.1455, accuracy : 94.78\n",
      "iteration : 100, loss : 0.1513, accuracy : 94.75\n",
      "iteration : 150, loss : 0.1519, accuracy : 94.74\n",
      "iteration : 200, loss : 0.1475, accuracy : 94.90\n",
      "iteration : 250, loss : 0.1474, accuracy : 94.84\n",
      "iteration : 300, loss : 0.1498, accuracy : 94.74\n",
      "Epoch :  56, training loss : 0.1496, training accuracy : 94.76, validation loss : 0.3888, validation accuracy : 87.35\n",
      "\n",
      "Epoch: 57\n",
      "iteration :  50, loss : 0.1390, accuracy : 94.78\n",
      "iteration : 100, loss : 0.1431, accuracy : 94.88\n",
      "iteration : 150, loss : 0.1440, accuracy : 94.88\n",
      "iteration : 200, loss : 0.1459, accuracy : 94.77\n",
      "iteration : 250, loss : 0.1472, accuracy : 94.75\n",
      "iteration : 300, loss : 0.1471, accuracy : 94.78\n",
      "Epoch :  57, training loss : 0.1469, training accuracy : 94.81, validation loss : 0.4216, validation accuracy : 87.14\n",
      "\n",
      "Epoch: 58\n",
      "iteration :  50, loss : 0.1449, accuracy : 95.02\n",
      "iteration : 100, loss : 0.1493, accuracy : 94.78\n",
      "iteration : 150, loss : 0.1475, accuracy : 94.83\n",
      "iteration : 200, loss : 0.1471, accuracy : 94.81\n",
      "iteration : 250, loss : 0.1464, accuracy : 94.83\n",
      "iteration : 300, loss : 0.1473, accuracy : 94.80\n",
      "Epoch :  58, training loss : 0.1477, training accuracy : 94.76, validation loss : 0.3799, validation accuracy : 88.32\n",
      "\n",
      "Epoch: 59\n",
      "iteration :  50, loss : 0.1323, accuracy : 95.62\n",
      "iteration : 100, loss : 0.1318, accuracy : 95.62\n",
      "iteration : 150, loss : 0.1343, accuracy : 95.41\n",
      "iteration : 200, loss : 0.1385, accuracy : 95.21\n",
      "iteration : 250, loss : 0.1421, accuracy : 95.08\n",
      "iteration : 300, loss : 0.1430, accuracy : 95.01\n",
      "Epoch :  59, training loss : 0.1430, training accuracy : 95.02, validation loss : 0.3911, validation accuracy : 87.77\n",
      "\n",
      "Epoch: 60\n",
      "iteration :  50, loss : 0.1477, accuracy : 94.73\n",
      "iteration : 100, loss : 0.1435, accuracy : 94.99\n",
      "iteration : 150, loss : 0.1426, accuracy : 94.99\n",
      "iteration : 200, loss : 0.1435, accuracy : 95.02\n",
      "iteration : 250, loss : 0.1469, accuracy : 94.92\n",
      "iteration : 300, loss : 0.1486, accuracy : 94.91\n",
      "Epoch :  60, training loss : 0.1483, training accuracy : 94.94, validation loss : 0.3708, validation accuracy : 88.32\n",
      "\n",
      "Epoch: 61\n",
      "iteration :  50, loss : 0.1327, accuracy : 95.67\n",
      "iteration : 100, loss : 0.1321, accuracy : 95.60\n",
      "iteration : 150, loss : 0.1331, accuracy : 95.47\n",
      "iteration : 200, loss : 0.1341, accuracy : 95.40\n",
      "iteration : 250, loss : 0.1383, accuracy : 95.23\n",
      "iteration : 300, loss : 0.1424, accuracy : 95.09\n",
      "Epoch :  61, training loss : 0.1432, training accuracy : 95.08, validation loss : 0.3866, validation accuracy : 88.04\n",
      "\n",
      "Epoch: 62\n",
      "iteration :  50, loss : 0.1498, accuracy : 94.86\n",
      "iteration : 100, loss : 0.1396, accuracy : 95.26\n",
      "iteration : 150, loss : 0.1355, accuracy : 95.42\n",
      "iteration : 200, loss : 0.1327, accuracy : 95.50\n",
      "iteration : 250, loss : 0.1350, accuracy : 95.42\n",
      "iteration : 300, loss : 0.1348, accuracy : 95.41\n",
      "Epoch :  62, training loss : 0.1357, training accuracy : 95.37, validation loss : 0.3705, validation accuracy : 88.46\n",
      "\n",
      "Epoch: 63\n",
      "iteration :  50, loss : 0.1225, accuracy : 95.83\n",
      "iteration : 100, loss : 0.1281, accuracy : 95.45\n",
      "iteration : 150, loss : 0.1321, accuracy : 95.35\n",
      "iteration : 200, loss : 0.1362, accuracy : 95.19\n",
      "iteration : 250, loss : 0.1375, accuracy : 95.16\n",
      "iteration : 300, loss : 0.1407, accuracy : 95.01\n",
      "Epoch :  63, training loss : 0.1402, training accuracy : 95.03, validation loss : 0.3557, validation accuracy : 88.54\n",
      "\n",
      "Epoch: 64\n",
      "iteration :  50, loss : 0.1243, accuracy : 95.55\n",
      "iteration : 100, loss : 0.1347, accuracy : 95.22\n",
      "iteration : 150, loss : 0.1346, accuracy : 95.29\n",
      "iteration : 200, loss : 0.1349, accuracy : 95.27\n",
      "iteration : 250, loss : 0.1353, accuracy : 95.27\n",
      "iteration : 300, loss : 0.1351, accuracy : 95.26\n",
      "Epoch :  64, training loss : 0.1353, training accuracy : 95.26, validation loss : 0.3956, validation accuracy : 87.68\n",
      "\n",
      "Epoch: 65\n",
      "iteration :  50, loss : 0.1277, accuracy : 95.47\n",
      "iteration : 100, loss : 0.1295, accuracy : 95.41\n",
      "iteration : 150, loss : 0.1349, accuracy : 95.27\n",
      "iteration : 200, loss : 0.1352, accuracy : 95.25\n",
      "iteration : 250, loss : 0.1366, accuracy : 95.23\n",
      "iteration : 300, loss : 0.1365, accuracy : 95.25\n",
      "Epoch :  65, training loss : 0.1371, training accuracy : 95.23, validation loss : 0.4014, validation accuracy : 87.74\n",
      "\n",
      "Epoch: 66\n",
      "iteration :  50, loss : 0.1304, accuracy : 95.58\n",
      "iteration : 100, loss : 0.1303, accuracy : 95.62\n",
      "iteration : 150, loss : 0.1284, accuracy : 95.62\n",
      "iteration : 200, loss : 0.1308, accuracy : 95.50\n",
      "iteration : 250, loss : 0.1285, accuracy : 95.61\n",
      "iteration : 300, loss : 0.1296, accuracy : 95.54\n",
      "Epoch :  66, training loss : 0.1301, training accuracy : 95.52, validation loss : 0.3871, validation accuracy : 87.77\n",
      "\n",
      "Epoch: 67\n",
      "iteration :  50, loss : 0.1315, accuracy : 95.52\n",
      "iteration : 100, loss : 0.1287, accuracy : 95.49\n",
      "iteration : 150, loss : 0.1273, accuracy : 95.54\n",
      "iteration : 200, loss : 0.1288, accuracy : 95.48\n",
      "iteration : 250, loss : 0.1309, accuracy : 95.51\n",
      "iteration : 300, loss : 0.1325, accuracy : 95.47\n",
      "Epoch :  67, training loss : 0.1331, training accuracy : 95.45, validation loss : 0.4006, validation accuracy : 87.77\n",
      "\n",
      "Epoch: 68\n",
      "iteration :  50, loss : 0.1177, accuracy : 96.00\n",
      "iteration : 100, loss : 0.1205, accuracy : 95.90\n",
      "iteration : 150, loss : 0.1207, accuracy : 95.96\n",
      "iteration : 200, loss : 0.1214, accuracy : 95.86\n",
      "iteration : 250, loss : 0.1239, accuracy : 95.71\n",
      "iteration : 300, loss : 0.1269, accuracy : 95.61\n",
      "Epoch :  68, training loss : 0.1271, training accuracy : 95.62, validation loss : 0.3893, validation accuracy : 87.70\n",
      "\n",
      "Epoch: 69\n",
      "iteration :  50, loss : 0.1271, accuracy : 95.69\n",
      "iteration : 100, loss : 0.1226, accuracy : 95.71\n",
      "iteration : 150, loss : 0.1246, accuracy : 95.70\n",
      "iteration : 200, loss : 0.1240, accuracy : 95.75\n",
      "iteration : 250, loss : 0.1240, accuracy : 95.76\n",
      "iteration : 300, loss : 0.1264, accuracy : 95.70\n",
      "Epoch :  69, training loss : 0.1260, training accuracy : 95.72, validation loss : 0.4378, validation accuracy : 86.90\n",
      "\n",
      "Epoch: 70\n",
      "iteration :  50, loss : 0.1232, accuracy : 95.78\n",
      "iteration : 100, loss : 0.1216, accuracy : 95.73\n",
      "iteration : 150, loss : 0.1238, accuracy : 95.65\n",
      "iteration : 200, loss : 0.1265, accuracy : 95.59\n",
      "iteration : 250, loss : 0.1289, accuracy : 95.53\n",
      "iteration : 300, loss : 0.1299, accuracy : 95.48\n",
      "Epoch :  70, training loss : 0.1303, training accuracy : 95.46, validation loss : 0.3596, validation accuracy : 88.65\n",
      "\n",
      "Epoch: 71\n",
      "iteration :  50, loss : 0.1158, accuracy : 96.06\n",
      "iteration : 100, loss : 0.1227, accuracy : 95.79\n",
      "iteration : 150, loss : 0.1217, accuracy : 95.85\n",
      "iteration : 200, loss : 0.1232, accuracy : 95.80\n",
      "iteration : 250, loss : 0.1225, accuracy : 95.78\n",
      "iteration : 300, loss : 0.1233, accuracy : 95.77\n",
      "Epoch :  71, training loss : 0.1238, training accuracy : 95.77, validation loss : 0.3679, validation accuracy : 88.20\n",
      "\n",
      "Epoch: 72\n",
      "iteration :  50, loss : 0.1319, accuracy : 95.38\n",
      "iteration : 100, loss : 0.1288, accuracy : 95.65\n",
      "iteration : 150, loss : 0.1244, accuracy : 95.72\n",
      "iteration : 200, loss : 0.1248, accuracy : 95.73\n",
      "iteration : 250, loss : 0.1291, accuracy : 95.58\n",
      "iteration : 300, loss : 0.1323, accuracy : 95.42\n",
      "Epoch :  72, training loss : 0.1330, training accuracy : 95.41, validation loss : 0.3761, validation accuracy : 88.43\n",
      "\n",
      "Epoch: 73\n",
      "iteration :  50, loss : 0.1270, accuracy : 95.67\n",
      "iteration : 100, loss : 0.1240, accuracy : 95.72\n",
      "iteration : 150, loss : 0.1255, accuracy : 95.70\n",
      "iteration : 200, loss : 0.1290, accuracy : 95.57\n",
      "iteration : 250, loss : 0.1269, accuracy : 95.64\n",
      "iteration : 300, loss : 0.1262, accuracy : 95.65\n",
      "Epoch :  73, training loss : 0.1263, training accuracy : 95.65, validation loss : 0.3883, validation accuracy : 87.72\n",
      "\n",
      "Epoch: 74\n",
      "iteration :  50, loss : 0.1134, accuracy : 96.05\n",
      "iteration : 100, loss : 0.1123, accuracy : 96.20\n",
      "iteration : 150, loss : 0.1133, accuracy : 96.13\n",
      "iteration : 200, loss : 0.1170, accuracy : 96.08\n",
      "iteration : 250, loss : 0.1181, accuracy : 95.99\n",
      "iteration : 300, loss : 0.1172, accuracy : 96.01\n",
      "Epoch :  74, training loss : 0.1172, training accuracy : 96.01, validation loss : 0.3314, validation accuracy : 89.17\n",
      "\n",
      "Epoch: 75\n",
      "iteration :  50, loss : 0.1222, accuracy : 95.77\n",
      "iteration : 100, loss : 0.1218, accuracy : 95.70\n",
      "iteration : 150, loss : 0.1172, accuracy : 95.94\n",
      "iteration : 200, loss : 0.1177, accuracy : 95.96\n",
      "iteration : 250, loss : 0.1200, accuracy : 95.95\n",
      "iteration : 300, loss : 0.1228, accuracy : 95.80\n",
      "Epoch :  75, training loss : 0.1231, training accuracy : 95.81, validation loss : 0.3930, validation accuracy : 87.94\n",
      "\n",
      "Epoch: 76\n",
      "iteration :  50, loss : 0.1251, accuracy : 95.91\n",
      "iteration : 100, loss : 0.1177, accuracy : 96.09\n",
      "iteration : 150, loss : 0.1165, accuracy : 96.14\n",
      "iteration : 200, loss : 0.1177, accuracy : 96.02\n",
      "iteration : 250, loss : 0.1170, accuracy : 95.99\n",
      "iteration : 300, loss : 0.1170, accuracy : 96.03\n",
      "Epoch :  76, training loss : 0.1172, training accuracy : 96.03, validation loss : 0.3771, validation accuracy : 88.75\n",
      "\n",
      "Epoch: 77\n",
      "iteration :  50, loss : 0.1025, accuracy : 96.45\n",
      "iteration : 100, loss : 0.1037, accuracy : 96.55\n",
      "iteration : 150, loss : 0.1069, accuracy : 96.45\n",
      "iteration : 200, loss : 0.1134, accuracy : 96.19\n",
      "iteration : 250, loss : 0.1135, accuracy : 96.16\n",
      "iteration : 300, loss : 0.1165, accuracy : 96.04\n",
      "Epoch :  77, training loss : 0.1168, training accuracy : 96.02, validation loss : 0.3888, validation accuracy : 87.93\n",
      "\n",
      "Epoch: 78\n",
      "iteration :  50, loss : 0.1268, accuracy : 95.55\n",
      "iteration : 100, loss : 0.1204, accuracy : 95.84\n",
      "iteration : 150, loss : 0.1183, accuracy : 95.92\n",
      "iteration : 200, loss : 0.1150, accuracy : 96.04\n",
      "iteration : 250, loss : 0.1149, accuracy : 96.05\n",
      "iteration : 300, loss : 0.1147, accuracy : 96.05\n",
      "Epoch :  78, training loss : 0.1150, training accuracy : 96.02, validation loss : 0.3667, validation accuracy : 88.67\n",
      "\n",
      "Epoch: 79\n",
      "iteration :  50, loss : 0.1133, accuracy : 96.08\n",
      "iteration : 100, loss : 0.1159, accuracy : 95.88\n",
      "iteration : 150, loss : 0.1159, accuracy : 95.86\n",
      "iteration : 200, loss : 0.1172, accuracy : 95.87\n",
      "iteration : 250, loss : 0.1188, accuracy : 95.82\n",
      "iteration : 300, loss : 0.1199, accuracy : 95.77\n",
      "Epoch :  79, training loss : 0.1200, training accuracy : 95.75, validation loss : 0.3688, validation accuracy : 88.63\n",
      "\n",
      "Epoch: 80\n",
      "iteration :  50, loss : 0.1104, accuracy : 96.16\n",
      "iteration : 100, loss : 0.1151, accuracy : 96.08\n",
      "iteration : 150, loss : 0.1164, accuracy : 96.03\n",
      "iteration : 200, loss : 0.1196, accuracy : 95.89\n",
      "iteration : 250, loss : 0.1196, accuracy : 95.90\n",
      "iteration : 300, loss : 0.1205, accuracy : 95.85\n",
      "Epoch :  80, training loss : 0.1204, training accuracy : 95.85, validation loss : 0.3625, validation accuracy : 88.82\n",
      "\n",
      "Epoch: 81\n",
      "iteration :  50, loss : 0.1126, accuracy : 96.27\n",
      "iteration : 100, loss : 0.1103, accuracy : 96.29\n",
      "iteration : 150, loss : 0.1147, accuracy : 96.06\n",
      "iteration : 200, loss : 0.1172, accuracy : 95.95\n",
      "iteration : 250, loss : 0.1185, accuracy : 95.96\n",
      "iteration : 300, loss : 0.1193, accuracy : 95.90\n",
      "Epoch :  81, training loss : 0.1201, training accuracy : 95.89, validation loss : 0.3701, validation accuracy : 88.53\n",
      "\n",
      "Epoch: 82\n",
      "iteration :  50, loss : 0.1121, accuracy : 96.36\n",
      "iteration : 100, loss : 0.1089, accuracy : 96.43\n",
      "iteration : 150, loss : 0.1084, accuracy : 96.38\n",
      "iteration : 200, loss : 0.1127, accuracy : 96.21\n",
      "iteration : 250, loss : 0.1162, accuracy : 96.02\n",
      "iteration : 300, loss : 0.1176, accuracy : 95.96\n",
      "Epoch :  82, training loss : 0.1173, training accuracy : 95.98, validation loss : 0.3457, validation accuracy : 88.81\n",
      "\n",
      "Epoch: 83\n",
      "iteration :  50, loss : 0.1107, accuracy : 96.48\n",
      "iteration : 100, loss : 0.1095, accuracy : 96.48\n",
      "iteration : 150, loss : 0.1093, accuracy : 96.38\n",
      "iteration : 200, loss : 0.1082, accuracy : 96.40\n",
      "iteration : 250, loss : 0.1099, accuracy : 96.29\n",
      "iteration : 300, loss : 0.1109, accuracy : 96.23\n",
      "Epoch :  83, training loss : 0.1107, training accuracy : 96.23, validation loss : 0.3436, validation accuracy : 89.52\n",
      "\n",
      "Epoch: 84\n",
      "iteration :  50, loss : 0.1192, accuracy : 95.70\n",
      "iteration : 100, loss : 0.1192, accuracy : 95.64\n",
      "iteration : 150, loss : 0.1131, accuracy : 95.91\n",
      "iteration : 200, loss : 0.1154, accuracy : 95.86\n",
      "iteration : 250, loss : 0.1139, accuracy : 95.92\n",
      "iteration : 300, loss : 0.1157, accuracy : 95.88\n",
      "Epoch :  84, training loss : 0.1152, training accuracy : 95.90, validation loss : 0.3466, validation accuracy : 89.08\n",
      "\n",
      "Epoch: 85\n",
      "iteration :  50, loss : 0.1046, accuracy : 96.45\n",
      "iteration : 100, loss : 0.1066, accuracy : 96.27\n",
      "iteration : 150, loss : 0.1051, accuracy : 96.43\n",
      "iteration : 200, loss : 0.1072, accuracy : 96.32\n",
      "iteration : 250, loss : 0.1084, accuracy : 96.28\n",
      "iteration : 300, loss : 0.1111, accuracy : 96.18\n",
      "Epoch :  85, training loss : 0.1112, training accuracy : 96.19, validation loss : 0.3416, validation accuracy : 89.11\n",
      "\n",
      "Epoch: 86\n",
      "iteration :  50, loss : 0.1059, accuracy : 96.45\n",
      "iteration : 100, loss : 0.1098, accuracy : 96.23\n",
      "iteration : 150, loss : 0.1083, accuracy : 96.33\n",
      "iteration : 200, loss : 0.1115, accuracy : 96.17\n",
      "iteration : 250, loss : 0.1103, accuracy : 96.25\n",
      "iteration : 300, loss : 0.1113, accuracy : 96.23\n",
      "Epoch :  86, training loss : 0.1107, training accuracy : 96.25, validation loss : 0.3609, validation accuracy : 88.68\n",
      "\n",
      "Epoch: 87\n",
      "iteration :  50, loss : 0.0986, accuracy : 96.56\n",
      "iteration : 100, loss : 0.0988, accuracy : 96.62\n",
      "iteration : 150, loss : 0.0992, accuracy : 96.67\n",
      "iteration : 200, loss : 0.1017, accuracy : 96.58\n",
      "iteration : 250, loss : 0.1019, accuracy : 96.58\n",
      "iteration : 300, loss : 0.1044, accuracy : 96.49\n",
      "Epoch :  87, training loss : 0.1041, training accuracy : 96.48, validation loss : 0.3658, validation accuracy : 89.09\n",
      "\n",
      "Epoch: 88\n",
      "iteration :  50, loss : 0.1082, accuracy : 96.09\n",
      "iteration : 100, loss : 0.1079, accuracy : 96.31\n",
      "iteration : 150, loss : 0.1055, accuracy : 96.39\n",
      "iteration : 200, loss : 0.1061, accuracy : 96.36\n",
      "iteration : 250, loss : 0.1101, accuracy : 96.18\n",
      "iteration : 300, loss : 0.1076, accuracy : 96.30\n",
      "Epoch :  88, training loss : 0.1082, training accuracy : 96.30, validation loss : 0.3334, validation accuracy : 89.32\n",
      "\n",
      "Epoch: 89\n",
      "iteration :  50, loss : 0.1189, accuracy : 95.92\n",
      "iteration : 100, loss : 0.1089, accuracy : 96.28\n",
      "iteration : 150, loss : 0.1126, accuracy : 96.16\n",
      "iteration : 200, loss : 0.1097, accuracy : 96.27\n",
      "iteration : 250, loss : 0.1074, accuracy : 96.36\n",
      "iteration : 300, loss : 0.1077, accuracy : 96.35\n",
      "Epoch :  89, training loss : 0.1076, training accuracy : 96.35, validation loss : 0.3299, validation accuracy : 89.79\n",
      "\n",
      "Epoch: 90\n",
      "iteration :  50, loss : 0.1030, accuracy : 96.42\n",
      "iteration : 100, loss : 0.1011, accuracy : 96.53\n",
      "iteration : 150, loss : 0.1036, accuracy : 96.44\n",
      "iteration : 200, loss : 0.1058, accuracy : 96.39\n",
      "iteration : 250, loss : 0.1058, accuracy : 96.39\n",
      "iteration : 300, loss : 0.1061, accuracy : 96.42\n",
      "Epoch :  90, training loss : 0.1046, training accuracy : 96.46, validation loss : 0.3462, validation accuracy : 89.28\n",
      "\n",
      "Epoch: 91\n",
      "iteration :  50, loss : 0.1028, accuracy : 96.59\n",
      "iteration : 100, loss : 0.1035, accuracy : 96.62\n",
      "iteration : 150, loss : 0.1056, accuracy : 96.46\n",
      "iteration : 200, loss : 0.1064, accuracy : 96.38\n",
      "iteration : 250, loss : 0.1089, accuracy : 96.30\n",
      "iteration : 300, loss : 0.1114, accuracy : 96.22\n",
      "Epoch :  91, training loss : 0.1116, training accuracy : 96.19, validation loss : 0.3736, validation accuracy : 88.63\n",
      "\n",
      "Epoch: 92\n",
      "iteration :  50, loss : 0.1119, accuracy : 96.23\n",
      "iteration : 100, loss : 0.1095, accuracy : 96.26\n",
      "iteration : 150, loss : 0.1047, accuracy : 96.49\n",
      "iteration : 200, loss : 0.1057, accuracy : 96.47\n",
      "iteration : 250, loss : 0.1052, accuracy : 96.41\n",
      "iteration : 300, loss : 0.1077, accuracy : 96.30\n",
      "Epoch :  92, training loss : 0.1081, training accuracy : 96.29, validation loss : 0.3677, validation accuracy : 88.50\n",
      "\n",
      "Epoch: 93\n",
      "iteration :  50, loss : 0.1038, accuracy : 96.42\n",
      "iteration : 100, loss : 0.1056, accuracy : 96.33\n",
      "iteration : 150, loss : 0.1063, accuracy : 96.38\n",
      "iteration : 200, loss : 0.1055, accuracy : 96.38\n",
      "iteration : 250, loss : 0.1086, accuracy : 96.28\n",
      "iteration : 300, loss : 0.1074, accuracy : 96.33\n",
      "Epoch :  93, training loss : 0.1074, training accuracy : 96.34, validation loss : 0.3273, validation accuracy : 89.65\n",
      "\n",
      "Epoch: 94\n",
      "iteration :  50, loss : 0.0916, accuracy : 96.78\n",
      "iteration : 100, loss : 0.0973, accuracy : 96.52\n",
      "iteration : 150, loss : 0.1014, accuracy : 96.43\n",
      "iteration : 200, loss : 0.1010, accuracy : 96.43\n",
      "iteration : 250, loss : 0.1015, accuracy : 96.41\n",
      "iteration : 300, loss : 0.1032, accuracy : 96.39\n",
      "Epoch :  94, training loss : 0.1027, training accuracy : 96.40, validation loss : 0.3627, validation accuracy : 88.87\n",
      "\n",
      "Epoch: 95\n",
      "iteration :  50, loss : 0.0990, accuracy : 96.56\n",
      "iteration : 100, loss : 0.0991, accuracy : 96.69\n",
      "iteration : 150, loss : 0.1016, accuracy : 96.56\n",
      "iteration : 200, loss : 0.1016, accuracy : 96.57\n",
      "iteration : 250, loss : 0.1026, accuracy : 96.53\n",
      "iteration : 300, loss : 0.1032, accuracy : 96.52\n",
      "Epoch :  95, training loss : 0.1037, training accuracy : 96.52, validation loss : 0.3440, validation accuracy : 89.56\n",
      "\n",
      "Epoch: 96\n",
      "iteration :  50, loss : 0.0966, accuracy : 96.73\n",
      "iteration : 100, loss : 0.1024, accuracy : 96.53\n",
      "iteration : 150, loss : 0.1037, accuracy : 96.51\n",
      "iteration : 200, loss : 0.1053, accuracy : 96.43\n",
      "iteration : 250, loss : 0.1078, accuracy : 96.36\n",
      "iteration : 300, loss : 0.1071, accuracy : 96.36\n",
      "Epoch :  96, training loss : 0.1067, training accuracy : 96.36, validation loss : 0.3443, validation accuracy : 89.38\n",
      "\n",
      "Epoch: 97\n",
      "iteration :  50, loss : 0.0981, accuracy : 96.72\n",
      "iteration : 100, loss : 0.1039, accuracy : 96.42\n",
      "iteration : 150, loss : 0.1032, accuracy : 96.49\n",
      "iteration : 200, loss : 0.1028, accuracy : 96.50\n",
      "iteration : 250, loss : 0.0995, accuracy : 96.67\n",
      "iteration : 300, loss : 0.1000, accuracy : 96.64\n",
      "Epoch :  97, training loss : 0.1009, training accuracy : 96.63, validation loss : 0.3383, validation accuracy : 89.36\n",
      "\n",
      "Epoch: 98\n",
      "iteration :  50, loss : 0.1027, accuracy : 96.56\n",
      "iteration : 100, loss : 0.1006, accuracy : 96.65\n",
      "iteration : 150, loss : 0.0987, accuracy : 96.70\n",
      "iteration : 200, loss : 0.0993, accuracy : 96.65\n",
      "iteration : 250, loss : 0.1002, accuracy : 96.63\n",
      "iteration : 300, loss : 0.1009, accuracy : 96.58\n",
      "Epoch :  98, training loss : 0.1018, training accuracy : 96.55, validation loss : 0.3640, validation accuracy : 88.75\n",
      "\n",
      "Epoch: 99\n",
      "iteration :  50, loss : 0.1092, accuracy : 96.17\n",
      "iteration : 100, loss : 0.1004, accuracy : 96.55\n",
      "iteration : 150, loss : 0.0986, accuracy : 96.64\n",
      "iteration : 200, loss : 0.0945, accuracy : 96.86\n",
      "iteration : 250, loss : 0.0947, accuracy : 96.86\n",
      "iteration : 300, loss : 0.0975, accuracy : 96.77\n",
      "Epoch :  99, training loss : 0.0986, training accuracy : 96.73, validation loss : 0.3908, validation accuracy : 88.59\n",
      "\n",
      "Epoch: 100\n",
      "iteration :  50, loss : 0.0907, accuracy : 97.06\n",
      "iteration : 100, loss : 0.0927, accuracy : 96.89\n",
      "iteration : 150, loss : 0.0948, accuracy : 96.79\n",
      "iteration : 200, loss : 0.0938, accuracy : 96.82\n",
      "iteration : 250, loss : 0.0961, accuracy : 96.70\n",
      "iteration : 300, loss : 0.0971, accuracy : 96.68\n",
      "Epoch : 100, training loss : 0.0975, training accuracy : 96.70, validation loss : 0.3475, validation accuracy : 89.22\n",
      "\n",
      "Epoch: 101\n",
      "iteration :  50, loss : 0.0978, accuracy : 96.44\n",
      "iteration : 100, loss : 0.0915, accuracy : 96.79\n",
      "iteration : 150, loss : 0.0918, accuracy : 96.87\n",
      "iteration : 200, loss : 0.0964, accuracy : 96.71\n",
      "iteration : 250, loss : 0.0968, accuracy : 96.75\n",
      "iteration : 300, loss : 0.0971, accuracy : 96.73\n",
      "Epoch : 101, training loss : 0.0971, training accuracy : 96.70, validation loss : 0.3413, validation accuracy : 89.41\n",
      "\n",
      "Epoch: 102\n",
      "iteration :  50, loss : 0.0907, accuracy : 97.14\n",
      "iteration : 100, loss : 0.0910, accuracy : 96.98\n",
      "iteration : 150, loss : 0.0946, accuracy : 96.76\n",
      "iteration : 200, loss : 0.0985, accuracy : 96.58\n",
      "iteration : 250, loss : 0.0975, accuracy : 96.61\n",
      "iteration : 300, loss : 0.0986, accuracy : 96.59\n",
      "Epoch : 102, training loss : 0.0986, training accuracy : 96.60, validation loss : 0.3950, validation accuracy : 87.86\n",
      "\n",
      "Epoch: 103\n",
      "iteration :  50, loss : 0.0989, accuracy : 96.48\n",
      "iteration : 100, loss : 0.0966, accuracy : 96.66\n",
      "iteration : 150, loss : 0.0951, accuracy : 96.71\n",
      "iteration : 200, loss : 0.0978, accuracy : 96.66\n",
      "iteration : 250, loss : 0.1000, accuracy : 96.58\n",
      "iteration : 300, loss : 0.0978, accuracy : 96.69\n",
      "Epoch : 103, training loss : 0.0989, training accuracy : 96.67, validation loss : 0.3506, validation accuracy : 89.76\n",
      "\n",
      "Epoch: 104\n",
      "iteration :  50, loss : 0.0964, accuracy : 96.84\n",
      "iteration : 100, loss : 0.0927, accuracy : 96.81\n",
      "iteration : 150, loss : 0.0945, accuracy : 96.84\n",
      "iteration : 200, loss : 0.0983, accuracy : 96.73\n",
      "iteration : 250, loss : 0.0971, accuracy : 96.76\n",
      "iteration : 300, loss : 0.0981, accuracy : 96.69\n",
      "Epoch : 104, training loss : 0.0983, training accuracy : 96.68, validation loss : 0.3440, validation accuracy : 89.42\n",
      "\n",
      "Epoch: 105\n",
      "iteration :  50, loss : 0.0964, accuracy : 96.80\n",
      "iteration : 100, loss : 0.0959, accuracy : 96.79\n",
      "iteration : 150, loss : 0.0970, accuracy : 96.82\n",
      "iteration : 200, loss : 0.0970, accuracy : 96.83\n",
      "iteration : 250, loss : 0.0997, accuracy : 96.74\n",
      "iteration : 300, loss : 0.0988, accuracy : 96.72\n",
      "Epoch : 105, training loss : 0.0985, training accuracy : 96.72, validation loss : 0.3261, validation accuracy : 89.86\n",
      "\n",
      "Epoch: 106\n",
      "iteration :  50, loss : 0.0953, accuracy : 96.84\n",
      "iteration : 100, loss : 0.0956, accuracy : 96.78\n",
      "iteration : 150, loss : 0.0940, accuracy : 96.83\n",
      "iteration : 200, loss : 0.0935, accuracy : 96.89\n",
      "iteration : 250, loss : 0.0930, accuracy : 96.90\n",
      "iteration : 300, loss : 0.0935, accuracy : 96.89\n",
      "Epoch : 106, training loss : 0.0939, training accuracy : 96.88, validation loss : 0.3647, validation accuracy : 89.58\n",
      "\n",
      "Epoch: 107\n",
      "iteration :  50, loss : 0.1033, accuracy : 96.69\n",
      "iteration : 100, loss : 0.1011, accuracy : 96.60\n",
      "iteration : 150, loss : 0.0985, accuracy : 96.78\n",
      "iteration : 200, loss : 0.0976, accuracy : 96.78\n",
      "iteration : 250, loss : 0.0959, accuracy : 96.85\n",
      "iteration : 300, loss : 0.0951, accuracy : 96.85\n",
      "Epoch : 107, training loss : 0.0949, training accuracy : 96.84, validation loss : 0.3671, validation accuracy : 89.29\n",
      "\n",
      "Epoch: 108\n",
      "iteration :  50, loss : 0.0958, accuracy : 96.78\n",
      "iteration : 100, loss : 0.0918, accuracy : 97.04\n",
      "iteration : 150, loss : 0.0910, accuracy : 96.97\n",
      "iteration : 200, loss : 0.0903, accuracy : 96.97\n",
      "iteration : 250, loss : 0.0902, accuracy : 97.02\n",
      "iteration : 300, loss : 0.0908, accuracy : 96.96\n",
      "Epoch : 108, training loss : 0.0915, training accuracy : 96.94, validation loss : 0.3784, validation accuracy : 88.93\n",
      "\n",
      "Epoch: 109\n",
      "iteration :  50, loss : 0.0862, accuracy : 97.09\n",
      "iteration : 100, loss : 0.0932, accuracy : 96.92\n",
      "iteration : 150, loss : 0.0972, accuracy : 96.76\n",
      "iteration : 200, loss : 0.0987, accuracy : 96.68\n",
      "iteration : 250, loss : 0.0993, accuracy : 96.67\n",
      "iteration : 300, loss : 0.1002, accuracy : 96.65\n",
      "Epoch : 109, training loss : 0.1005, training accuracy : 96.64, validation loss : 0.3808, validation accuracy : 88.68\n",
      "\n",
      "Epoch: 110\n",
      "iteration :  50, loss : 0.0821, accuracy : 97.23\n",
      "iteration : 100, loss : 0.0821, accuracy : 97.22\n",
      "iteration : 150, loss : 0.0853, accuracy : 97.10\n",
      "iteration : 200, loss : 0.0881, accuracy : 96.95\n",
      "iteration : 250, loss : 0.0902, accuracy : 96.91\n",
      "iteration : 300, loss : 0.0904, accuracy : 96.93\n",
      "Epoch : 110, training loss : 0.0908, training accuracy : 96.92, validation loss : 0.3445, validation accuracy : 89.51\n",
      "\n",
      "Epoch: 111\n",
      "iteration :  50, loss : 0.0738, accuracy : 97.53\n",
      "iteration : 100, loss : 0.0813, accuracy : 97.30\n",
      "iteration : 150, loss : 0.0864, accuracy : 97.00\n",
      "iteration : 200, loss : 0.0883, accuracy : 96.98\n",
      "iteration : 250, loss : 0.0877, accuracy : 97.01\n",
      "iteration : 300, loss : 0.0869, accuracy : 97.03\n",
      "Epoch : 111, training loss : 0.0872, training accuracy : 97.03, validation loss : 0.3543, validation accuracy : 89.34\n",
      "\n",
      "Epoch: 112\n",
      "iteration :  50, loss : 0.0908, accuracy : 96.97\n",
      "iteration : 100, loss : 0.0860, accuracy : 97.11\n",
      "iteration : 150, loss : 0.0875, accuracy : 97.12\n",
      "iteration : 200, loss : 0.0906, accuracy : 97.01\n",
      "iteration : 250, loss : 0.0917, accuracy : 96.97\n",
      "iteration : 300, loss : 0.0925, accuracy : 96.94\n",
      "Epoch : 112, training loss : 0.0924, training accuracy : 96.95, validation loss : 0.3276, validation accuracy : 90.05\n",
      "\n",
      "Epoch: 113\n",
      "iteration :  50, loss : 0.0883, accuracy : 97.11\n",
      "iteration : 100, loss : 0.0823, accuracy : 97.33\n",
      "iteration : 150, loss : 0.0807, accuracy : 97.33\n",
      "iteration : 200, loss : 0.0817, accuracy : 97.26\n",
      "iteration : 250, loss : 0.0843, accuracy : 97.18\n",
      "iteration : 300, loss : 0.0865, accuracy : 97.11\n",
      "Epoch : 113, training loss : 0.0865, training accuracy : 97.11, validation loss : 0.3821, validation accuracy : 88.87\n",
      "\n",
      "Epoch: 114\n",
      "iteration :  50, loss : 0.0904, accuracy : 97.09\n",
      "iteration : 100, loss : 0.0924, accuracy : 96.91\n",
      "iteration : 150, loss : 0.0913, accuracy : 96.95\n",
      "iteration : 200, loss : 0.0913, accuracy : 96.90\n",
      "iteration : 250, loss : 0.0899, accuracy : 96.94\n",
      "iteration : 300, loss : 0.0908, accuracy : 96.89\n",
      "Epoch : 114, training loss : 0.0907, training accuracy : 96.90, validation loss : 0.3204, validation accuracy : 90.03\n",
      "\n",
      "Epoch: 115\n",
      "iteration :  50, loss : 0.0846, accuracy : 97.00\n",
      "iteration : 100, loss : 0.0906, accuracy : 96.80\n",
      "iteration : 150, loss : 0.0893, accuracy : 96.85\n",
      "iteration : 200, loss : 0.0885, accuracy : 96.91\n",
      "iteration : 250, loss : 0.0890, accuracy : 96.96\n",
      "iteration : 300, loss : 0.0906, accuracy : 96.91\n",
      "Epoch : 115, training loss : 0.0910, training accuracy : 96.91, validation loss : 0.3426, validation accuracy : 89.85\n",
      "\n",
      "Epoch: 116\n",
      "iteration :  50, loss : 0.0943, accuracy : 96.80\n",
      "iteration : 100, loss : 0.0907, accuracy : 96.94\n",
      "iteration : 150, loss : 0.0892, accuracy : 96.96\n",
      "iteration : 200, loss : 0.0887, accuracy : 97.01\n",
      "iteration : 250, loss : 0.0888, accuracy : 97.02\n",
      "iteration : 300, loss : 0.0895, accuracy : 96.99\n",
      "Epoch : 116, training loss : 0.0894, training accuracy : 97.01, validation loss : 0.3392, validation accuracy : 89.59\n",
      "\n",
      "Epoch: 117\n",
      "iteration :  50, loss : 0.0875, accuracy : 96.92\n",
      "iteration : 100, loss : 0.0902, accuracy : 96.95\n",
      "iteration : 150, loss : 0.0905, accuracy : 96.93\n",
      "iteration : 200, loss : 0.0893, accuracy : 96.97\n",
      "iteration : 250, loss : 0.0897, accuracy : 96.95\n",
      "iteration : 300, loss : 0.0903, accuracy : 96.95\n",
      "Epoch : 117, training loss : 0.0902, training accuracy : 96.95, validation loss : 0.3869, validation accuracy : 88.78\n",
      "\n",
      "Epoch: 118\n",
      "iteration :  50, loss : 0.0850, accuracy : 97.22\n",
      "iteration : 100, loss : 0.0841, accuracy : 97.20\n",
      "iteration : 150, loss : 0.0846, accuracy : 97.17\n",
      "iteration : 200, loss : 0.0858, accuracy : 97.07\n",
      "iteration : 250, loss : 0.0874, accuracy : 97.05\n",
      "iteration : 300, loss : 0.0874, accuracy : 97.06\n",
      "Epoch : 118, training loss : 0.0885, training accuracy : 97.03, validation loss : 0.3724, validation accuracy : 89.01\n",
      "\n",
      "Epoch: 119\n",
      "iteration :  50, loss : 0.0962, accuracy : 96.80\n",
      "iteration : 100, loss : 0.0938, accuracy : 96.81\n",
      "iteration : 150, loss : 0.0930, accuracy : 96.94\n",
      "iteration : 200, loss : 0.0911, accuracy : 97.00\n",
      "iteration : 250, loss : 0.0917, accuracy : 96.97\n",
      "iteration : 300, loss : 0.0935, accuracy : 96.92\n",
      "Epoch : 119, training loss : 0.0934, training accuracy : 96.91, validation loss : 0.3358, validation accuracy : 89.71\n",
      "\n",
      "Epoch: 120\n",
      "iteration :  50, loss : 0.0888, accuracy : 97.11\n",
      "iteration : 100, loss : 0.0885, accuracy : 97.08\n",
      "iteration : 150, loss : 0.0857, accuracy : 97.22\n",
      "iteration : 200, loss : 0.0833, accuracy : 97.26\n",
      "iteration : 250, loss : 0.0835, accuracy : 97.19\n",
      "iteration : 300, loss : 0.0840, accuracy : 97.16\n",
      "Epoch : 120, training loss : 0.0845, training accuracy : 97.15, validation loss : 0.3384, validation accuracy : 89.46\n",
      "\n",
      "Epoch: 121\n",
      "iteration :  50, loss : 0.0863, accuracy : 96.88\n",
      "iteration : 100, loss : 0.0878, accuracy : 97.02\n",
      "iteration : 150, loss : 0.0854, accuracy : 97.18\n",
      "iteration : 200, loss : 0.0845, accuracy : 97.20\n",
      "iteration : 250, loss : 0.0857, accuracy : 97.15\n",
      "iteration : 300, loss : 0.0865, accuracy : 97.11\n",
      "Epoch : 121, training loss : 0.0873, training accuracy : 97.08, validation loss : 0.3475, validation accuracy : 89.93\n",
      "\n",
      "Epoch: 122\n",
      "iteration :  50, loss : 0.0845, accuracy : 97.22\n",
      "iteration : 100, loss : 0.0854, accuracy : 97.24\n",
      "iteration : 150, loss : 0.0870, accuracy : 97.14\n",
      "iteration : 200, loss : 0.0865, accuracy : 97.13\n",
      "iteration : 250, loss : 0.0887, accuracy : 97.05\n",
      "iteration : 300, loss : 0.0897, accuracy : 96.99\n",
      "Epoch : 122, training loss : 0.0895, training accuracy : 97.01, validation loss : 0.3207, validation accuracy : 90.43\n",
      "\n",
      "Epoch: 123\n",
      "iteration :  50, loss : 0.0824, accuracy : 97.23\n",
      "iteration : 100, loss : 0.0831, accuracy : 97.27\n",
      "iteration : 150, loss : 0.0856, accuracy : 97.15\n",
      "iteration : 200, loss : 0.0835, accuracy : 97.20\n",
      "iteration : 250, loss : 0.0834, accuracy : 97.20\n",
      "iteration : 300, loss : 0.0844, accuracy : 97.16\n",
      "Epoch : 123, training loss : 0.0842, training accuracy : 97.17, validation loss : 0.3408, validation accuracy : 89.88\n",
      "\n",
      "Epoch: 124\n",
      "iteration :  50, loss : 0.0775, accuracy : 97.52\n",
      "iteration : 100, loss : 0.0776, accuracy : 97.47\n",
      "iteration : 150, loss : 0.0756, accuracy : 97.54\n",
      "iteration : 200, loss : 0.0736, accuracy : 97.59\n",
      "iteration : 250, loss : 0.0757, accuracy : 97.49\n",
      "iteration : 300, loss : 0.0783, accuracy : 97.39\n",
      "Epoch : 124, training loss : 0.0793, training accuracy : 97.36, validation loss : 0.3725, validation accuracy : 88.72\n",
      "\n",
      "Epoch: 125\n",
      "iteration :  50, loss : 0.0794, accuracy : 97.41\n",
      "iteration : 100, loss : 0.0821, accuracy : 97.17\n",
      "iteration : 150, loss : 0.0810, accuracy : 97.24\n",
      "iteration : 200, loss : 0.0822, accuracy : 97.23\n",
      "iteration : 250, loss : 0.0850, accuracy : 97.15\n",
      "iteration : 300, loss : 0.0843, accuracy : 97.18\n",
      "Epoch : 125, training loss : 0.0847, training accuracy : 97.16, validation loss : 0.3281, validation accuracy : 89.78\n",
      "\n",
      "Epoch: 126\n",
      "iteration :  50, loss : 0.0809, accuracy : 97.25\n",
      "iteration : 100, loss : 0.0785, accuracy : 97.28\n",
      "iteration : 150, loss : 0.0794, accuracy : 97.31\n",
      "iteration : 200, loss : 0.0793, accuracy : 97.36\n",
      "iteration : 250, loss : 0.0810, accuracy : 97.29\n",
      "iteration : 300, loss : 0.0822, accuracy : 97.24\n",
      "Epoch : 126, training loss : 0.0828, training accuracy : 97.23, validation loss : 0.3166, validation accuracy : 90.12\n",
      "\n",
      "Epoch: 127\n",
      "iteration :  50, loss : 0.0814, accuracy : 97.30\n",
      "iteration : 100, loss : 0.0833, accuracy : 97.31\n",
      "iteration : 150, loss : 0.0825, accuracy : 97.25\n",
      "iteration : 200, loss : 0.0830, accuracy : 97.25\n",
      "iteration : 250, loss : 0.0849, accuracy : 97.21\n",
      "iteration : 300, loss : 0.0835, accuracy : 97.23\n",
      "Epoch : 127, training loss : 0.0829, training accuracy : 97.25, validation loss : 0.3142, validation accuracy : 90.26\n",
      "\n",
      "Epoch: 128\n",
      "iteration :  50, loss : 0.0733, accuracy : 97.47\n",
      "iteration : 100, loss : 0.0712, accuracy : 97.59\n",
      "iteration : 150, loss : 0.0755, accuracy : 97.42\n",
      "iteration : 200, loss : 0.0793, accuracy : 97.31\n",
      "iteration : 250, loss : 0.0804, accuracy : 97.27\n",
      "iteration : 300, loss : 0.0819, accuracy : 97.21\n",
      "Epoch : 128, training loss : 0.0824, training accuracy : 97.18, validation loss : 0.3313, validation accuracy : 90.02\n",
      "\n",
      "Epoch: 129\n",
      "iteration :  50, loss : 0.0781, accuracy : 97.31\n",
      "iteration : 100, loss : 0.0759, accuracy : 97.36\n",
      "iteration : 150, loss : 0.0743, accuracy : 97.35\n",
      "iteration : 200, loss : 0.0761, accuracy : 97.34\n",
      "iteration : 250, loss : 0.0776, accuracy : 97.31\n",
      "iteration : 300, loss : 0.0782, accuracy : 97.30\n",
      "Epoch : 129, training loss : 0.0783, training accuracy : 97.30, validation loss : 0.3378, validation accuracy : 89.78\n",
      "\n",
      "Epoch: 130\n",
      "iteration :  50, loss : 0.0808, accuracy : 97.27\n",
      "iteration : 100, loss : 0.0790, accuracy : 97.37\n",
      "iteration : 150, loss : 0.0775, accuracy : 97.40\n",
      "iteration : 200, loss : 0.0761, accuracy : 97.49\n",
      "iteration : 250, loss : 0.0793, accuracy : 97.41\n",
      "iteration : 300, loss : 0.0788, accuracy : 97.43\n",
      "Epoch : 130, training loss : 0.0789, training accuracy : 97.42, validation loss : 0.3410, validation accuracy : 89.68\n",
      "\n",
      "Epoch: 131\n",
      "iteration :  50, loss : 0.0811, accuracy : 97.44\n",
      "iteration : 100, loss : 0.0814, accuracy : 97.38\n",
      "iteration : 150, loss : 0.0813, accuracy : 97.33\n",
      "iteration : 200, loss : 0.0807, accuracy : 97.38\n",
      "iteration : 250, loss : 0.0794, accuracy : 97.40\n",
      "iteration : 300, loss : 0.0794, accuracy : 97.34\n",
      "Epoch : 131, training loss : 0.0795, training accuracy : 97.33, validation loss : 0.3362, validation accuracy : 90.30\n",
      "\n",
      "Epoch: 132\n",
      "iteration :  50, loss : 0.0704, accuracy : 97.61\n",
      "iteration : 100, loss : 0.0739, accuracy : 97.50\n",
      "iteration : 150, loss : 0.0769, accuracy : 97.37\n",
      "iteration : 200, loss : 0.0792, accuracy : 97.32\n",
      "iteration : 250, loss : 0.0800, accuracy : 97.31\n",
      "iteration : 300, loss : 0.0783, accuracy : 97.40\n",
      "Epoch : 132, training loss : 0.0791, training accuracy : 97.37, validation loss : 0.3140, validation accuracy : 90.37\n",
      "\n",
      "Epoch: 133\n",
      "iteration :  50, loss : 0.0815, accuracy : 97.08\n",
      "iteration : 100, loss : 0.0810, accuracy : 97.23\n",
      "iteration : 150, loss : 0.0786, accuracy : 97.36\n",
      "iteration : 200, loss : 0.0782, accuracy : 97.38\n",
      "iteration : 250, loss : 0.0792, accuracy : 97.32\n",
      "iteration : 300, loss : 0.0792, accuracy : 97.34\n",
      "Epoch : 133, training loss : 0.0794, training accuracy : 97.33, validation loss : 0.3624, validation accuracy : 89.47\n",
      "\n",
      "Epoch: 134\n",
      "iteration :  50, loss : 0.0806, accuracy : 97.39\n",
      "iteration : 100, loss : 0.0777, accuracy : 97.52\n",
      "iteration : 150, loss : 0.0757, accuracy : 97.61\n",
      "iteration : 200, loss : 0.0768, accuracy : 97.53\n",
      "iteration : 250, loss : 0.0783, accuracy : 97.47\n",
      "iteration : 300, loss : 0.0789, accuracy : 97.43\n",
      "Epoch : 134, training loss : 0.0790, training accuracy : 97.42, validation loss : 0.3510, validation accuracy : 89.20\n",
      "\n",
      "Epoch: 135\n",
      "iteration :  50, loss : 0.0729, accuracy : 97.55\n",
      "iteration : 100, loss : 0.0765, accuracy : 97.41\n",
      "iteration : 150, loss : 0.0763, accuracy : 97.45\n",
      "iteration : 200, loss : 0.0742, accuracy : 97.49\n",
      "iteration : 250, loss : 0.0754, accuracy : 97.46\n",
      "iteration : 300, loss : 0.0765, accuracy : 97.42\n",
      "Epoch : 135, training loss : 0.0769, training accuracy : 97.42, validation loss : 0.3205, validation accuracy : 90.20\n",
      "\n",
      "Epoch: 136\n",
      "iteration :  50, loss : 0.0705, accuracy : 97.77\n",
      "iteration : 100, loss : 0.0719, accuracy : 97.73\n",
      "iteration : 150, loss : 0.0791, accuracy : 97.42\n",
      "iteration : 200, loss : 0.0781, accuracy : 97.43\n",
      "iteration : 250, loss : 0.0778, accuracy : 97.43\n",
      "iteration : 300, loss : 0.0781, accuracy : 97.40\n",
      "Epoch : 136, training loss : 0.0778, training accuracy : 97.42, validation loss : 0.3327, validation accuracy : 89.84\n",
      "\n",
      "Epoch: 137\n",
      "iteration :  50, loss : 0.0782, accuracy : 97.30\n",
      "iteration : 100, loss : 0.0784, accuracy : 97.44\n",
      "iteration : 150, loss : 0.0777, accuracy : 97.39\n",
      "iteration : 200, loss : 0.0788, accuracy : 97.36\n",
      "iteration : 250, loss : 0.0780, accuracy : 97.35\n",
      "iteration : 300, loss : 0.0784, accuracy : 97.33\n",
      "Epoch : 137, training loss : 0.0791, training accuracy : 97.31, validation loss : 0.3472, validation accuracy : 89.66\n",
      "\n",
      "Epoch: 138\n",
      "iteration :  50, loss : 0.0810, accuracy : 97.08\n",
      "iteration : 100, loss : 0.0808, accuracy : 97.09\n",
      "iteration : 150, loss : 0.0800, accuracy : 97.23\n",
      "iteration : 200, loss : 0.0797, accuracy : 97.29\n",
      "iteration : 250, loss : 0.0770, accuracy : 97.38\n",
      "iteration : 300, loss : 0.0766, accuracy : 97.43\n",
      "Epoch : 138, training loss : 0.0762, training accuracy : 97.45, validation loss : 0.3334, validation accuracy : 89.95\n",
      "\n",
      "Epoch: 139\n",
      "iteration :  50, loss : 0.0768, accuracy : 97.42\n",
      "iteration : 100, loss : 0.0716, accuracy : 97.64\n",
      "iteration : 150, loss : 0.0694, accuracy : 97.69\n",
      "iteration : 200, loss : 0.0692, accuracy : 97.73\n",
      "iteration : 250, loss : 0.0711, accuracy : 97.66\n",
      "iteration : 300, loss : 0.0714, accuracy : 97.66\n",
      "Epoch : 139, training loss : 0.0718, training accuracy : 97.65, validation loss : 0.3184, validation accuracy : 90.76\n",
      "\n",
      "Epoch: 140\n",
      "iteration :  50, loss : 0.0707, accuracy : 97.64\n",
      "iteration : 100, loss : 0.0746, accuracy : 97.43\n",
      "iteration : 150, loss : 0.0746, accuracy : 97.46\n",
      "iteration : 200, loss : 0.0746, accuracy : 97.47\n",
      "iteration : 250, loss : 0.0734, accuracy : 97.52\n",
      "iteration : 300, loss : 0.0733, accuracy : 97.54\n",
      "Epoch : 140, training loss : 0.0734, training accuracy : 97.56, validation loss : 0.3424, validation accuracy : 89.59\n",
      "\n",
      "Epoch: 141\n",
      "iteration :  50, loss : 0.0649, accuracy : 97.94\n",
      "iteration : 100, loss : 0.0653, accuracy : 97.82\n",
      "iteration : 150, loss : 0.0705, accuracy : 97.66\n",
      "iteration : 200, loss : 0.0730, accuracy : 97.59\n",
      "iteration : 250, loss : 0.0755, accuracy : 97.47\n",
      "iteration : 300, loss : 0.0756, accuracy : 97.50\n",
      "Epoch : 141, training loss : 0.0755, training accuracy : 97.50, validation loss : 0.3327, validation accuracy : 90.21\n",
      "\n",
      "Epoch: 142\n",
      "iteration :  50, loss : 0.0734, accuracy : 97.31\n",
      "iteration : 100, loss : 0.0697, accuracy : 97.56\n",
      "iteration : 150, loss : 0.0710, accuracy : 97.55\n",
      "iteration : 200, loss : 0.0704, accuracy : 97.62\n",
      "iteration : 250, loss : 0.0704, accuracy : 97.62\n",
      "iteration : 300, loss : 0.0698, accuracy : 97.64\n",
      "Epoch : 142, training loss : 0.0694, training accuracy : 97.65, validation loss : 0.3127, validation accuracy : 90.59\n",
      "\n",
      "Epoch: 143\n",
      "iteration :  50, loss : 0.0660, accuracy : 97.77\n",
      "iteration : 100, loss : 0.0668, accuracy : 97.71\n",
      "iteration : 150, loss : 0.0711, accuracy : 97.56\n",
      "iteration : 200, loss : 0.0703, accuracy : 97.58\n",
      "iteration : 250, loss : 0.0711, accuracy : 97.57\n",
      "iteration : 300, loss : 0.0703, accuracy : 97.62\n",
      "Epoch : 143, training loss : 0.0705, training accuracy : 97.62, validation loss : 0.3239, validation accuracy : 90.44\n",
      "\n",
      "Epoch: 144\n",
      "iteration :  50, loss : 0.0734, accuracy : 97.53\n",
      "iteration : 100, loss : 0.0686, accuracy : 97.62\n",
      "iteration : 150, loss : 0.0682, accuracy : 97.70\n",
      "iteration : 200, loss : 0.0685, accuracy : 97.67\n",
      "iteration : 250, loss : 0.0700, accuracy : 97.61\n",
      "iteration : 300, loss : 0.0717, accuracy : 97.52\n",
      "Epoch : 144, training loss : 0.0720, training accuracy : 97.52, validation loss : 0.3402, validation accuracy : 89.99\n",
      "\n",
      "Epoch: 145\n",
      "iteration :  50, loss : 0.0675, accuracy : 97.77\n",
      "iteration : 100, loss : 0.0659, accuracy : 97.73\n",
      "iteration : 150, loss : 0.0676, accuracy : 97.73\n",
      "iteration : 200, loss : 0.0678, accuracy : 97.73\n",
      "iteration : 250, loss : 0.0661, accuracy : 97.81\n",
      "iteration : 300, loss : 0.0660, accuracy : 97.81\n",
      "Epoch : 145, training loss : 0.0664, training accuracy : 97.82, validation loss : 0.3203, validation accuracy : 90.48\n",
      "\n",
      "Epoch: 146\n",
      "iteration :  50, loss : 0.0771, accuracy : 97.42\n",
      "iteration : 100, loss : 0.0745, accuracy : 97.52\n",
      "iteration : 150, loss : 0.0755, accuracy : 97.55\n",
      "iteration : 200, loss : 0.0723, accuracy : 97.68\n",
      "iteration : 250, loss : 0.0715, accuracy : 97.68\n",
      "iteration : 300, loss : 0.0724, accuracy : 97.62\n",
      "Epoch : 146, training loss : 0.0724, training accuracy : 97.61, validation loss : 0.3157, validation accuracy : 90.38\n",
      "\n",
      "Epoch: 147\n",
      "iteration :  50, loss : 0.0698, accuracy : 97.78\n",
      "iteration : 100, loss : 0.0689, accuracy : 97.80\n",
      "iteration : 150, loss : 0.0681, accuracy : 97.77\n",
      "iteration : 200, loss : 0.0688, accuracy : 97.76\n",
      "iteration : 250, loss : 0.0695, accuracy : 97.75\n",
      "iteration : 300, loss : 0.0705, accuracy : 97.70\n",
      "Epoch : 147, training loss : 0.0711, training accuracy : 97.69, validation loss : 0.3229, validation accuracy : 90.42\n",
      "\n",
      "Epoch: 148\n",
      "iteration :  50, loss : 0.0802, accuracy : 97.28\n",
      "iteration : 100, loss : 0.0759, accuracy : 97.44\n",
      "iteration : 150, loss : 0.0765, accuracy : 97.38\n",
      "iteration : 200, loss : 0.0731, accuracy : 97.54\n",
      "iteration : 250, loss : 0.0727, accuracy : 97.56\n",
      "iteration : 300, loss : 0.0727, accuracy : 97.57\n",
      "Epoch : 148, training loss : 0.0723, training accuracy : 97.58, validation loss : 0.3268, validation accuracy : 90.23\n",
      "\n",
      "Epoch: 149\n",
      "iteration :  50, loss : 0.0625, accuracy : 97.83\n",
      "iteration : 100, loss : 0.0692, accuracy : 97.73\n",
      "iteration : 150, loss : 0.0682, accuracy : 97.70\n",
      "iteration : 200, loss : 0.0692, accuracy : 97.65\n",
      "iteration : 250, loss : 0.0708, accuracy : 97.59\n",
      "iteration : 300, loss : 0.0706, accuracy : 97.59\n",
      "Epoch : 149, training loss : 0.0706, training accuracy : 97.59, validation loss : 0.3243, validation accuracy : 90.00\n",
      "\n",
      "Epoch: 150\n",
      "iteration :  50, loss : 0.0670, accuracy : 97.66\n",
      "iteration : 100, loss : 0.0716, accuracy : 97.49\n",
      "iteration : 150, loss : 0.0700, accuracy : 97.62\n",
      "iteration : 200, loss : 0.0696, accuracy : 97.64\n",
      "iteration : 250, loss : 0.0697, accuracy : 97.66\n",
      "iteration : 300, loss : 0.0700, accuracy : 97.68\n",
      "Epoch : 150, training loss : 0.0697, training accuracy : 97.69, validation loss : 0.3290, validation accuracy : 90.48\n",
      "\n",
      "Epoch: 151\n",
      "iteration :  50, loss : 0.0668, accuracy : 97.72\n",
      "iteration : 100, loss : 0.0657, accuracy : 97.76\n",
      "iteration : 150, loss : 0.0637, accuracy : 97.85\n",
      "iteration : 200, loss : 0.0651, accuracy : 97.82\n",
      "iteration : 250, loss : 0.0658, accuracy : 97.77\n",
      "iteration : 300, loss : 0.0671, accuracy : 97.75\n",
      "Epoch : 151, training loss : 0.0670, training accuracy : 97.75, validation loss : 0.3202, validation accuracy : 90.47\n",
      "\n",
      "Epoch: 152\n",
      "iteration :  50, loss : 0.0583, accuracy : 98.27\n",
      "iteration : 100, loss : 0.0693, accuracy : 97.87\n",
      "iteration : 150, loss : 0.0703, accuracy : 97.72\n",
      "iteration : 200, loss : 0.0688, accuracy : 97.78\n",
      "iteration : 250, loss : 0.0687, accuracy : 97.77\n",
      "iteration : 300, loss : 0.0686, accuracy : 97.79\n",
      "Epoch : 152, training loss : 0.0682, training accuracy : 97.80, validation loss : 0.3113, validation accuracy : 90.80\n",
      "\n",
      "Epoch: 153\n",
      "iteration :  50, loss : 0.0673, accuracy : 97.78\n",
      "iteration : 100, loss : 0.0618, accuracy : 98.00\n",
      "iteration : 150, loss : 0.0625, accuracy : 97.91\n",
      "iteration : 200, loss : 0.0623, accuracy : 97.93\n",
      "iteration : 250, loss : 0.0620, accuracy : 97.95\n",
      "iteration : 300, loss : 0.0623, accuracy : 97.97\n",
      "Epoch : 153, training loss : 0.0626, training accuracy : 97.94, validation loss : 0.3271, validation accuracy : 90.63\n",
      "\n",
      "Epoch: 154\n",
      "iteration :  50, loss : 0.0684, accuracy : 97.91\n",
      "iteration : 100, loss : 0.0640, accuracy : 97.95\n",
      "iteration : 150, loss : 0.0623, accuracy : 97.98\n",
      "iteration : 200, loss : 0.0635, accuracy : 97.92\n",
      "iteration : 250, loss : 0.0633, accuracy : 97.96\n",
      "iteration : 300, loss : 0.0639, accuracy : 97.95\n",
      "Epoch : 154, training loss : 0.0641, training accuracy : 97.94, validation loss : 0.3248, validation accuracy : 90.52\n",
      "\n",
      "Epoch: 155\n",
      "iteration :  50, loss : 0.0602, accuracy : 98.03\n",
      "iteration : 100, loss : 0.0618, accuracy : 97.93\n",
      "iteration : 150, loss : 0.0615, accuracy : 97.98\n",
      "iteration : 200, loss : 0.0622, accuracy : 97.97\n",
      "iteration : 250, loss : 0.0625, accuracy : 97.97\n",
      "iteration : 300, loss : 0.0626, accuracy : 97.92\n",
      "Epoch : 155, training loss : 0.0626, training accuracy : 97.92, validation loss : 0.3582, validation accuracy : 89.65\n",
      "\n",
      "Epoch: 156\n",
      "iteration :  50, loss : 0.0624, accuracy : 97.91\n",
      "iteration : 100, loss : 0.0623, accuracy : 97.92\n",
      "iteration : 150, loss : 0.0617, accuracy : 97.98\n",
      "iteration : 200, loss : 0.0632, accuracy : 97.97\n",
      "iteration : 250, loss : 0.0642, accuracy : 97.94\n",
      "iteration : 300, loss : 0.0631, accuracy : 97.97\n",
      "Epoch : 156, training loss : 0.0631, training accuracy : 97.97, validation loss : 0.3309, validation accuracy : 90.28\n",
      "\n",
      "Epoch: 157\n",
      "iteration :  50, loss : 0.0675, accuracy : 97.61\n",
      "iteration : 100, loss : 0.0676, accuracy : 97.73\n",
      "iteration : 150, loss : 0.0678, accuracy : 97.77\n",
      "iteration : 200, loss : 0.0671, accuracy : 97.78\n",
      "iteration : 250, loss : 0.0667, accuracy : 97.79\n",
      "iteration : 300, loss : 0.0652, accuracy : 97.84\n",
      "Epoch : 157, training loss : 0.0657, training accuracy : 97.84, validation loss : 0.3199, validation accuracy : 90.65\n",
      "\n",
      "Epoch: 158\n",
      "iteration :  50, loss : 0.0700, accuracy : 97.73\n",
      "iteration : 100, loss : 0.0697, accuracy : 97.62\n",
      "iteration : 150, loss : 0.0668, accuracy : 97.77\n",
      "iteration : 200, loss : 0.0663, accuracy : 97.81\n",
      "iteration : 250, loss : 0.0660, accuracy : 97.84\n",
      "iteration : 300, loss : 0.0661, accuracy : 97.80\n",
      "Epoch : 158, training loss : 0.0661, training accuracy : 97.81, validation loss : 0.3293, validation accuracy : 90.55\n",
      "\n",
      "Epoch: 159\n",
      "iteration :  50, loss : 0.0614, accuracy : 98.03\n",
      "iteration : 100, loss : 0.0571, accuracy : 98.09\n",
      "iteration : 150, loss : 0.0570, accuracy : 98.10\n",
      "iteration : 200, loss : 0.0575, accuracy : 98.07\n",
      "iteration : 250, loss : 0.0602, accuracy : 97.97\n",
      "iteration : 300, loss : 0.0617, accuracy : 97.93\n",
      "Epoch : 159, training loss : 0.0615, training accuracy : 97.94, validation loss : 0.3175, validation accuracy : 90.36\n",
      "\n",
      "Epoch: 160\n",
      "iteration :  50, loss : 0.0632, accuracy : 98.06\n",
      "iteration : 100, loss : 0.0654, accuracy : 97.95\n",
      "iteration : 150, loss : 0.0662, accuracy : 97.92\n",
      "iteration : 200, loss : 0.0656, accuracy : 97.86\n",
      "iteration : 250, loss : 0.0644, accuracy : 97.90\n",
      "iteration : 300, loss : 0.0652, accuracy : 97.90\n",
      "Epoch : 160, training loss : 0.0652, training accuracy : 97.90, validation loss : 0.2978, validation accuracy : 90.82\n",
      "\n",
      "Epoch: 161\n",
      "iteration :  50, loss : 0.0643, accuracy : 97.94\n",
      "iteration : 100, loss : 0.0646, accuracy : 97.80\n",
      "iteration : 150, loss : 0.0636, accuracy : 97.90\n",
      "iteration : 200, loss : 0.0635, accuracy : 97.91\n",
      "iteration : 250, loss : 0.0644, accuracy : 97.88\n",
      "iteration : 300, loss : 0.0648, accuracy : 97.88\n",
      "Epoch : 161, training loss : 0.0655, training accuracy : 97.84, validation loss : 0.3313, validation accuracy : 90.23\n",
      "\n",
      "Epoch: 162\n",
      "iteration :  50, loss : 0.0588, accuracy : 98.11\n",
      "iteration : 100, loss : 0.0577, accuracy : 98.12\n",
      "iteration : 150, loss : 0.0575, accuracy : 98.21\n",
      "iteration : 200, loss : 0.0566, accuracy : 98.23\n",
      "iteration : 250, loss : 0.0570, accuracy : 98.18\n",
      "iteration : 300, loss : 0.0568, accuracy : 98.20\n",
      "Epoch : 162, training loss : 0.0565, training accuracy : 98.20, validation loss : 0.3080, validation accuracy : 90.87\n",
      "\n",
      "Epoch: 163\n",
      "iteration :  50, loss : 0.0622, accuracy : 97.98\n",
      "iteration : 100, loss : 0.0620, accuracy : 97.98\n",
      "iteration : 150, loss : 0.0625, accuracy : 97.92\n",
      "iteration : 200, loss : 0.0604, accuracy : 97.98\n",
      "iteration : 250, loss : 0.0619, accuracy : 97.96\n",
      "iteration : 300, loss : 0.0613, accuracy : 97.97\n",
      "Epoch : 163, training loss : 0.0614, training accuracy : 97.97, validation loss : 0.3337, validation accuracy : 90.46\n",
      "\n",
      "Epoch: 164\n",
      "iteration :  50, loss : 0.0649, accuracy : 98.00\n",
      "iteration : 100, loss : 0.0617, accuracy : 98.07\n",
      "iteration : 150, loss : 0.0599, accuracy : 98.04\n",
      "iteration : 200, loss : 0.0605, accuracy : 98.04\n",
      "iteration : 250, loss : 0.0595, accuracy : 98.07\n",
      "iteration : 300, loss : 0.0595, accuracy : 98.07\n",
      "Epoch : 164, training loss : 0.0600, training accuracy : 98.05, validation loss : 0.3290, validation accuracy : 90.81\n",
      "\n",
      "Epoch: 165\n",
      "iteration :  50, loss : 0.0598, accuracy : 97.98\n",
      "iteration : 100, loss : 0.0582, accuracy : 98.11\n",
      "iteration : 150, loss : 0.0624, accuracy : 97.97\n",
      "iteration : 200, loss : 0.0644, accuracy : 97.91\n",
      "iteration : 250, loss : 0.0621, accuracy : 97.97\n",
      "iteration : 300, loss : 0.0628, accuracy : 97.93\n",
      "Epoch : 165, training loss : 0.0628, training accuracy : 97.92, validation loss : 0.3247, validation accuracy : 90.45\n",
      "\n",
      "Epoch: 166\n",
      "iteration :  50, loss : 0.0537, accuracy : 98.22\n",
      "iteration : 100, loss : 0.0548, accuracy : 98.26\n",
      "iteration : 150, loss : 0.0542, accuracy : 98.28\n",
      "iteration : 200, loss : 0.0541, accuracy : 98.30\n",
      "iteration : 250, loss : 0.0535, accuracy : 98.31\n",
      "iteration : 300, loss : 0.0549, accuracy : 98.28\n",
      "Epoch : 166, training loss : 0.0552, training accuracy : 98.25, validation loss : 0.3282, validation accuracy : 91.18\n",
      "\n",
      "Epoch: 167\n",
      "iteration :  50, loss : 0.0551, accuracy : 98.20\n",
      "iteration : 100, loss : 0.0606, accuracy : 98.06\n",
      "iteration : 150, loss : 0.0610, accuracy : 98.08\n",
      "iteration : 200, loss : 0.0603, accuracy : 98.11\n",
      "iteration : 250, loss : 0.0609, accuracy : 98.08\n",
      "iteration : 300, loss : 0.0619, accuracy : 98.04\n",
      "Epoch : 167, training loss : 0.0622, training accuracy : 98.03, validation loss : 0.2995, validation accuracy : 90.98\n",
      "\n",
      "Epoch: 168\n",
      "iteration :  50, loss : 0.0496, accuracy : 98.31\n",
      "iteration : 100, loss : 0.0559, accuracy : 98.05\n",
      "iteration : 150, loss : 0.0614, accuracy : 97.88\n",
      "iteration : 200, loss : 0.0623, accuracy : 97.81\n",
      "iteration : 250, loss : 0.0619, accuracy : 97.87\n",
      "iteration : 300, loss : 0.0616, accuracy : 97.88\n",
      "Epoch : 168, training loss : 0.0615, training accuracy : 97.88, validation loss : 0.3262, validation accuracy : 90.60\n",
      "\n",
      "Epoch: 169\n",
      "iteration :  50, loss : 0.0592, accuracy : 98.03\n",
      "iteration : 100, loss : 0.0595, accuracy : 98.05\n",
      "iteration : 150, loss : 0.0581, accuracy : 98.15\n",
      "iteration : 200, loss : 0.0564, accuracy : 98.21\n",
      "iteration : 250, loss : 0.0564, accuracy : 98.20\n",
      "iteration : 300, loss : 0.0559, accuracy : 98.20\n",
      "Epoch : 169, training loss : 0.0550, training accuracy : 98.22, validation loss : 0.3021, validation accuracy : 91.00\n",
      "\n",
      "Epoch: 170\n",
      "iteration :  50, loss : 0.0490, accuracy : 98.36\n",
      "iteration : 100, loss : 0.0527, accuracy : 98.20\n",
      "iteration : 150, loss : 0.0565, accuracy : 98.15\n",
      "iteration : 200, loss : 0.0572, accuracy : 98.12\n",
      "iteration : 250, loss : 0.0581, accuracy : 98.08\n",
      "iteration : 300, loss : 0.0571, accuracy : 98.11\n",
      "Epoch : 170, training loss : 0.0574, training accuracy : 98.11, validation loss : 0.2919, validation accuracy : 91.58\n",
      "\n",
      "Epoch: 171\n",
      "iteration :  50, loss : 0.0556, accuracy : 98.12\n",
      "iteration : 100, loss : 0.0513, accuracy : 98.28\n",
      "iteration : 150, loss : 0.0529, accuracy : 98.24\n",
      "iteration : 200, loss : 0.0554, accuracy : 98.18\n",
      "iteration : 250, loss : 0.0558, accuracy : 98.17\n",
      "iteration : 300, loss : 0.0542, accuracy : 98.23\n",
      "Epoch : 171, training loss : 0.0541, training accuracy : 98.24, validation loss : 0.3163, validation accuracy : 90.52\n",
      "\n",
      "Epoch: 172\n",
      "iteration :  50, loss : 0.0523, accuracy : 98.36\n",
      "iteration : 100, loss : 0.0505, accuracy : 98.44\n",
      "iteration : 150, loss : 0.0508, accuracy : 98.39\n",
      "iteration : 200, loss : 0.0501, accuracy : 98.41\n",
      "iteration : 250, loss : 0.0503, accuracy : 98.40\n",
      "iteration : 300, loss : 0.0500, accuracy : 98.41\n",
      "Epoch : 172, training loss : 0.0512, training accuracy : 98.38, validation loss : 0.3161, validation accuracy : 90.74\n",
      "\n",
      "Epoch: 173\n",
      "iteration :  50, loss : 0.0503, accuracy : 98.30\n",
      "iteration : 100, loss : 0.0548, accuracy : 98.12\n",
      "iteration : 150, loss : 0.0558, accuracy : 98.14\n",
      "iteration : 200, loss : 0.0550, accuracy : 98.14\n",
      "iteration : 250, loss : 0.0540, accuracy : 98.20\n",
      "iteration : 300, loss : 0.0543, accuracy : 98.21\n",
      "Epoch : 173, training loss : 0.0547, training accuracy : 98.20, validation loss : 0.3073, validation accuracy : 90.60\n",
      "\n",
      "Epoch: 174\n",
      "iteration :  50, loss : 0.0497, accuracy : 98.22\n",
      "iteration : 100, loss : 0.0478, accuracy : 98.36\n",
      "iteration : 150, loss : 0.0483, accuracy : 98.34\n",
      "iteration : 200, loss : 0.0495, accuracy : 98.34\n",
      "iteration : 250, loss : 0.0491, accuracy : 98.36\n",
      "iteration : 300, loss : 0.0500, accuracy : 98.30\n",
      "Epoch : 174, training loss : 0.0502, training accuracy : 98.30, validation loss : 0.2957, validation accuracy : 91.14\n",
      "\n",
      "Epoch: 175\n",
      "iteration :  50, loss : 0.0518, accuracy : 98.31\n",
      "iteration : 100, loss : 0.0554, accuracy : 98.16\n",
      "iteration : 150, loss : 0.0540, accuracy : 98.21\n",
      "iteration : 200, loss : 0.0542, accuracy : 98.23\n",
      "iteration : 250, loss : 0.0534, accuracy : 98.24\n",
      "iteration : 300, loss : 0.0539, accuracy : 98.20\n",
      "Epoch : 175, training loss : 0.0539, training accuracy : 98.20, validation loss : 0.2968, validation accuracy : 91.20\n",
      "\n",
      "Epoch: 176\n",
      "iteration :  50, loss : 0.0513, accuracy : 98.31\n",
      "iteration : 100, loss : 0.0538, accuracy : 98.23\n",
      "iteration : 150, loss : 0.0550, accuracy : 98.15\n",
      "iteration : 200, loss : 0.0526, accuracy : 98.30\n",
      "iteration : 250, loss : 0.0520, accuracy : 98.32\n",
      "iteration : 300, loss : 0.0524, accuracy : 98.32\n",
      "Epoch : 176, training loss : 0.0524, training accuracy : 98.31, validation loss : 0.3064, validation accuracy : 91.32\n",
      "\n",
      "Epoch: 177\n",
      "iteration :  50, loss : 0.0418, accuracy : 98.70\n",
      "iteration : 100, loss : 0.0436, accuracy : 98.61\n",
      "iteration : 150, loss : 0.0445, accuracy : 98.55\n",
      "iteration : 200, loss : 0.0475, accuracy : 98.45\n",
      "iteration : 250, loss : 0.0478, accuracy : 98.41\n",
      "iteration : 300, loss : 0.0492, accuracy : 98.40\n",
      "Epoch : 177, training loss : 0.0497, training accuracy : 98.38, validation loss : 0.2942, validation accuracy : 91.41\n",
      "\n",
      "Epoch: 178\n",
      "iteration :  50, loss : 0.0541, accuracy : 98.17\n",
      "iteration : 100, loss : 0.0538, accuracy : 98.20\n",
      "iteration : 150, loss : 0.0526, accuracy : 98.23\n",
      "iteration : 200, loss : 0.0502, accuracy : 98.30\n",
      "iteration : 250, loss : 0.0509, accuracy : 98.27\n",
      "iteration : 300, loss : 0.0514, accuracy : 98.27\n",
      "Epoch : 178, training loss : 0.0516, training accuracy : 98.26, validation loss : 0.3091, validation accuracy : 91.25\n",
      "\n",
      "Epoch: 179\n",
      "iteration :  50, loss : 0.0453, accuracy : 98.52\n",
      "iteration : 100, loss : 0.0461, accuracy : 98.51\n",
      "iteration : 150, loss : 0.0489, accuracy : 98.41\n",
      "iteration : 200, loss : 0.0498, accuracy : 98.37\n",
      "iteration : 250, loss : 0.0511, accuracy : 98.34\n",
      "iteration : 300, loss : 0.0511, accuracy : 98.31\n",
      "Epoch : 179, training loss : 0.0512, training accuracy : 98.31, validation loss : 0.2997, validation accuracy : 91.15\n",
      "\n",
      "Epoch: 180\n",
      "iteration :  50, loss : 0.0490, accuracy : 98.39\n",
      "iteration : 100, loss : 0.0537, accuracy : 98.23\n",
      "iteration : 150, loss : 0.0523, accuracy : 98.29\n",
      "iteration : 200, loss : 0.0508, accuracy : 98.36\n",
      "iteration : 250, loss : 0.0519, accuracy : 98.32\n",
      "iteration : 300, loss : 0.0526, accuracy : 98.30\n",
      "Epoch : 180, training loss : 0.0523, training accuracy : 98.30, validation loss : 0.2866, validation accuracy : 91.48\n",
      "\n",
      "Epoch: 181\n",
      "iteration :  50, loss : 0.0505, accuracy : 98.36\n",
      "iteration : 100, loss : 0.0505, accuracy : 98.41\n",
      "iteration : 150, loss : 0.0510, accuracy : 98.34\n",
      "iteration : 200, loss : 0.0496, accuracy : 98.39\n",
      "iteration : 250, loss : 0.0496, accuracy : 98.39\n",
      "iteration : 300, loss : 0.0500, accuracy : 98.35\n",
      "Epoch : 181, training loss : 0.0501, training accuracy : 98.34, validation loss : 0.3281, validation accuracy : 90.86\n",
      "\n",
      "Epoch: 182\n",
      "iteration :  50, loss : 0.0449, accuracy : 98.47\n",
      "iteration : 100, loss : 0.0447, accuracy : 98.52\n",
      "iteration : 150, loss : 0.0463, accuracy : 98.45\n",
      "iteration : 200, loss : 0.0456, accuracy : 98.48\n",
      "iteration : 250, loss : 0.0472, accuracy : 98.46\n",
      "iteration : 300, loss : 0.0482, accuracy : 98.41\n",
      "Epoch : 182, training loss : 0.0486, training accuracy : 98.41, validation loss : 0.3163, validation accuracy : 90.69\n",
      "\n",
      "Epoch: 183\n",
      "iteration :  50, loss : 0.0459, accuracy : 98.38\n",
      "iteration : 100, loss : 0.0482, accuracy : 98.28\n",
      "iteration : 150, loss : 0.0493, accuracy : 98.25\n",
      "iteration : 200, loss : 0.0482, accuracy : 98.32\n",
      "iteration : 250, loss : 0.0486, accuracy : 98.32\n",
      "iteration : 300, loss : 0.0484, accuracy : 98.34\n",
      "Epoch : 183, training loss : 0.0486, training accuracy : 98.34, validation loss : 0.2998, validation accuracy : 91.19\n",
      "\n",
      "Epoch: 184\n",
      "iteration :  50, loss : 0.0530, accuracy : 98.12\n",
      "iteration : 100, loss : 0.0533, accuracy : 98.27\n",
      "iteration : 150, loss : 0.0503, accuracy : 98.35\n",
      "iteration : 200, loss : 0.0494, accuracy : 98.37\n",
      "iteration : 250, loss : 0.0505, accuracy : 98.33\n",
      "iteration : 300, loss : 0.0492, accuracy : 98.38\n",
      "Epoch : 184, training loss : 0.0492, training accuracy : 98.37, validation loss : 0.3058, validation accuracy : 91.25\n",
      "\n",
      "Epoch: 185\n",
      "iteration :  50, loss : 0.0367, accuracy : 98.78\n",
      "iteration : 100, loss : 0.0399, accuracy : 98.70\n",
      "iteration : 150, loss : 0.0418, accuracy : 98.64\n",
      "iteration : 200, loss : 0.0451, accuracy : 98.54\n",
      "iteration : 250, loss : 0.0461, accuracy : 98.47\n",
      "iteration : 300, loss : 0.0466, accuracy : 98.44\n",
      "Epoch : 185, training loss : 0.0472, training accuracy : 98.43, validation loss : 0.2965, validation accuracy : 91.14\n",
      "\n",
      "Epoch: 186\n",
      "iteration :  50, loss : 0.0479, accuracy : 98.62\n",
      "iteration : 100, loss : 0.0505, accuracy : 98.41\n",
      "iteration : 150, loss : 0.0488, accuracy : 98.46\n",
      "iteration : 200, loss : 0.0477, accuracy : 98.48\n",
      "iteration : 250, loss : 0.0482, accuracy : 98.46\n",
      "iteration : 300, loss : 0.0472, accuracy : 98.49\n",
      "Epoch : 186, training loss : 0.0472, training accuracy : 98.48, validation loss : 0.2794, validation accuracy : 91.64\n",
      "\n",
      "Epoch: 187\n",
      "iteration :  50, loss : 0.0453, accuracy : 98.66\n",
      "iteration : 100, loss : 0.0453, accuracy : 98.53\n",
      "iteration : 150, loss : 0.0456, accuracy : 98.52\n",
      "iteration : 200, loss : 0.0463, accuracy : 98.53\n",
      "iteration : 250, loss : 0.0450, accuracy : 98.57\n",
      "iteration : 300, loss : 0.0450, accuracy : 98.57\n",
      "Epoch : 187, training loss : 0.0447, training accuracy : 98.58, validation loss : 0.3059, validation accuracy : 91.38\n",
      "\n",
      "Epoch: 188\n",
      "iteration :  50, loss : 0.0385, accuracy : 98.77\n",
      "iteration : 100, loss : 0.0387, accuracy : 98.80\n",
      "iteration : 150, loss : 0.0406, accuracy : 98.77\n",
      "iteration : 200, loss : 0.0421, accuracy : 98.68\n",
      "iteration : 250, loss : 0.0431, accuracy : 98.62\n",
      "iteration : 300, loss : 0.0433, accuracy : 98.62\n",
      "Epoch : 188, training loss : 0.0433, training accuracy : 98.62, validation loss : 0.3008, validation accuracy : 91.33\n",
      "\n",
      "Epoch: 189\n",
      "iteration :  50, loss : 0.0438, accuracy : 98.72\n",
      "iteration : 100, loss : 0.0444, accuracy : 98.65\n",
      "iteration : 150, loss : 0.0441, accuracy : 98.63\n",
      "iteration : 200, loss : 0.0438, accuracy : 98.64\n",
      "iteration : 250, loss : 0.0432, accuracy : 98.65\n",
      "iteration : 300, loss : 0.0445, accuracy : 98.59\n",
      "Epoch : 189, training loss : 0.0445, training accuracy : 98.58, validation loss : 0.2744, validation accuracy : 91.74\n",
      "\n",
      "Epoch: 190\n",
      "iteration :  50, loss : 0.0384, accuracy : 98.67\n",
      "iteration : 100, loss : 0.0374, accuracy : 98.79\n",
      "iteration : 150, loss : 0.0389, accuracy : 98.76\n",
      "iteration : 200, loss : 0.0402, accuracy : 98.73\n",
      "iteration : 250, loss : 0.0410, accuracy : 98.73\n",
      "iteration : 300, loss : 0.0422, accuracy : 98.68\n",
      "Epoch : 190, training loss : 0.0423, training accuracy : 98.66, validation loss : 0.2952, validation accuracy : 91.47\n",
      "\n",
      "Epoch: 191\n",
      "iteration :  50, loss : 0.0478, accuracy : 98.33\n",
      "iteration : 100, loss : 0.0446, accuracy : 98.47\n",
      "iteration : 150, loss : 0.0432, accuracy : 98.54\n",
      "iteration : 200, loss : 0.0427, accuracy : 98.57\n",
      "iteration : 250, loss : 0.0426, accuracy : 98.58\n",
      "iteration : 300, loss : 0.0428, accuracy : 98.59\n",
      "Epoch : 191, training loss : 0.0423, training accuracy : 98.59, validation loss : 0.2708, validation accuracy : 91.98\n",
      "\n",
      "Epoch: 192\n",
      "iteration :  50, loss : 0.0382, accuracy : 98.61\n",
      "iteration : 100, loss : 0.0372, accuracy : 98.70\n",
      "iteration : 150, loss : 0.0398, accuracy : 98.62\n",
      "iteration : 200, loss : 0.0404, accuracy : 98.64\n",
      "iteration : 250, loss : 0.0405, accuracy : 98.67\n",
      "iteration : 300, loss : 0.0416, accuracy : 98.63\n",
      "Epoch : 192, training loss : 0.0419, training accuracy : 98.64, validation loss : 0.2922, validation accuracy : 91.68\n",
      "\n",
      "Epoch: 193\n",
      "iteration :  50, loss : 0.0387, accuracy : 98.72\n",
      "iteration : 100, loss : 0.0410, accuracy : 98.65\n",
      "iteration : 150, loss : 0.0394, accuracy : 98.70\n",
      "iteration : 200, loss : 0.0403, accuracy : 98.70\n",
      "iteration : 250, loss : 0.0412, accuracy : 98.65\n",
      "iteration : 300, loss : 0.0429, accuracy : 98.59\n",
      "Epoch : 193, training loss : 0.0430, training accuracy : 98.59, validation loss : 0.2928, validation accuracy : 91.41\n",
      "\n",
      "Epoch: 194\n",
      "iteration :  50, loss : 0.0427, accuracy : 98.72\n",
      "iteration : 100, loss : 0.0415, accuracy : 98.70\n",
      "iteration : 150, loss : 0.0393, accuracy : 98.79\n",
      "iteration : 200, loss : 0.0418, accuracy : 98.68\n",
      "iteration : 250, loss : 0.0432, accuracy : 98.61\n",
      "iteration : 300, loss : 0.0432, accuracy : 98.58\n",
      "Epoch : 194, training loss : 0.0433, training accuracy : 98.58, validation loss : 0.2879, validation accuracy : 91.78\n",
      "\n",
      "Epoch: 195\n",
      "iteration :  50, loss : 0.0408, accuracy : 98.64\n",
      "iteration : 100, loss : 0.0403, accuracy : 98.72\n",
      "iteration : 150, loss : 0.0393, accuracy : 98.74\n",
      "iteration : 200, loss : 0.0393, accuracy : 98.73\n",
      "iteration : 250, loss : 0.0400, accuracy : 98.69\n",
      "iteration : 300, loss : 0.0406, accuracy : 98.67\n",
      "Epoch : 195, training loss : 0.0404, training accuracy : 98.67, validation loss : 0.2989, validation accuracy : 91.44\n",
      "\n",
      "Epoch: 196\n",
      "iteration :  50, loss : 0.0449, accuracy : 98.62\n",
      "iteration : 100, loss : 0.0393, accuracy : 98.72\n",
      "iteration : 150, loss : 0.0379, accuracy : 98.80\n",
      "iteration : 200, loss : 0.0384, accuracy : 98.79\n",
      "iteration : 250, loss : 0.0396, accuracy : 98.75\n",
      "iteration : 300, loss : 0.0391, accuracy : 98.77\n",
      "Epoch : 196, training loss : 0.0394, training accuracy : 98.76, validation loss : 0.2966, validation accuracy : 91.66\n",
      "\n",
      "Epoch: 197\n",
      "iteration :  50, loss : 0.0443, accuracy : 98.64\n",
      "iteration : 100, loss : 0.0449, accuracy : 98.65\n",
      "iteration : 150, loss : 0.0437, accuracy : 98.71\n",
      "iteration : 200, loss : 0.0425, accuracy : 98.70\n",
      "iteration : 250, loss : 0.0420, accuracy : 98.70\n",
      "iteration : 300, loss : 0.0411, accuracy : 98.71\n",
      "Epoch : 197, training loss : 0.0410, training accuracy : 98.71, validation loss : 0.2889, validation accuracy : 91.88\n",
      "\n",
      "Epoch: 198\n",
      "iteration :  50, loss : 0.0409, accuracy : 98.69\n",
      "iteration : 100, loss : 0.0410, accuracy : 98.72\n",
      "iteration : 150, loss : 0.0426, accuracy : 98.65\n",
      "iteration : 200, loss : 0.0424, accuracy : 98.63\n",
      "iteration : 250, loss : 0.0411, accuracy : 98.67\n",
      "iteration : 300, loss : 0.0405, accuracy : 98.71\n",
      "Epoch : 198, training loss : 0.0404, training accuracy : 98.71, validation loss : 0.2890, validation accuracy : 91.59\n",
      "\n",
      "Epoch: 199\n",
      "iteration :  50, loss : 0.0351, accuracy : 98.95\n",
      "iteration : 100, loss : 0.0359, accuracy : 98.84\n",
      "iteration : 150, loss : 0.0360, accuracy : 98.81\n",
      "iteration : 200, loss : 0.0376, accuracy : 98.77\n",
      "iteration : 250, loss : 0.0385, accuracy : 98.76\n",
      "iteration : 300, loss : 0.0387, accuracy : 98.74\n",
      "Epoch : 199, training loss : 0.0388, training accuracy : 98.73, validation loss : 0.2935, validation accuracy : 91.68\n",
      "\n",
      "Epoch: 200\n",
      "iteration :  50, loss : 0.0374, accuracy : 98.77\n",
      "iteration : 100, loss : 0.0355, accuracy : 98.80\n",
      "iteration : 150, loss : 0.0391, accuracy : 98.65\n",
      "iteration : 200, loss : 0.0389, accuracy : 98.68\n",
      "iteration : 250, loss : 0.0388, accuracy : 98.69\n",
      "iteration : 300, loss : 0.0391, accuracy : 98.68\n",
      "Epoch : 200, training loss : 0.0389, training accuracy : 98.69, validation loss : 0.2980, validation accuracy : 91.33\n",
      "\n",
      "Epoch: 201\n",
      "iteration :  50, loss : 0.0443, accuracy : 98.58\n",
      "iteration : 100, loss : 0.0390, accuracy : 98.71\n",
      "iteration : 150, loss : 0.0387, accuracy : 98.71\n",
      "iteration : 200, loss : 0.0381, accuracy : 98.73\n",
      "iteration : 250, loss : 0.0383, accuracy : 98.72\n",
      "iteration : 300, loss : 0.0384, accuracy : 98.73\n",
      "Epoch : 201, training loss : 0.0384, training accuracy : 98.73, validation loss : 0.2721, validation accuracy : 92.12\n",
      "\n",
      "Epoch: 202\n",
      "iteration :  50, loss : 0.0476, accuracy : 98.45\n",
      "iteration : 100, loss : 0.0456, accuracy : 98.51\n",
      "iteration : 150, loss : 0.0415, accuracy : 98.65\n",
      "iteration : 200, loss : 0.0401, accuracy : 98.71\n",
      "iteration : 250, loss : 0.0399, accuracy : 98.72\n",
      "iteration : 300, loss : 0.0400, accuracy : 98.68\n",
      "Epoch : 202, training loss : 0.0398, training accuracy : 98.67, validation loss : 0.2898, validation accuracy : 91.64\n",
      "\n",
      "Epoch: 203\n",
      "iteration :  50, loss : 0.0343, accuracy : 98.75\n",
      "iteration : 100, loss : 0.0360, accuracy : 98.80\n",
      "iteration : 150, loss : 0.0365, accuracy : 98.79\n",
      "iteration : 200, loss : 0.0366, accuracy : 98.77\n",
      "iteration : 250, loss : 0.0358, accuracy : 98.80\n",
      "iteration : 300, loss : 0.0361, accuracy : 98.81\n",
      "Epoch : 203, training loss : 0.0363, training accuracy : 98.81, validation loss : 0.2918, validation accuracy : 91.84\n",
      "\n",
      "Epoch: 204\n",
      "iteration :  50, loss : 0.0395, accuracy : 98.72\n",
      "iteration : 100, loss : 0.0394, accuracy : 98.75\n",
      "iteration : 150, loss : 0.0393, accuracy : 98.76\n",
      "iteration : 200, loss : 0.0385, accuracy : 98.77\n",
      "iteration : 250, loss : 0.0382, accuracy : 98.79\n",
      "iteration : 300, loss : 0.0391, accuracy : 98.76\n",
      "Epoch : 204, training loss : 0.0392, training accuracy : 98.76, validation loss : 0.2910, validation accuracy : 91.84\n",
      "\n",
      "Epoch: 205\n",
      "iteration :  50, loss : 0.0390, accuracy : 98.72\n",
      "iteration : 100, loss : 0.0362, accuracy : 98.77\n",
      "iteration : 150, loss : 0.0370, accuracy : 98.74\n",
      "iteration : 200, loss : 0.0364, accuracy : 98.78\n",
      "iteration : 250, loss : 0.0354, accuracy : 98.82\n",
      "iteration : 300, loss : 0.0351, accuracy : 98.84\n",
      "Epoch : 205, training loss : 0.0353, training accuracy : 98.84, validation loss : 0.2788, validation accuracy : 92.14\n",
      "\n",
      "Epoch: 206\n",
      "iteration :  50, loss : 0.0328, accuracy : 98.95\n",
      "iteration : 100, loss : 0.0341, accuracy : 98.88\n",
      "iteration : 150, loss : 0.0362, accuracy : 98.78\n",
      "iteration : 200, loss : 0.0370, accuracy : 98.78\n",
      "iteration : 250, loss : 0.0362, accuracy : 98.82\n",
      "iteration : 300, loss : 0.0361, accuracy : 98.82\n",
      "Epoch : 206, training loss : 0.0358, training accuracy : 98.83, validation loss : 0.2671, validation accuracy : 92.01\n",
      "\n",
      "Epoch: 207\n",
      "iteration :  50, loss : 0.0387, accuracy : 98.81\n",
      "iteration : 100, loss : 0.0361, accuracy : 98.87\n",
      "iteration : 150, loss : 0.0346, accuracy : 98.92\n",
      "iteration : 200, loss : 0.0344, accuracy : 98.93\n",
      "iteration : 250, loss : 0.0350, accuracy : 98.89\n",
      "iteration : 300, loss : 0.0368, accuracy : 98.81\n",
      "Epoch : 207, training loss : 0.0372, training accuracy : 98.81, validation loss : 0.2770, validation accuracy : 91.88\n",
      "\n",
      "Epoch: 208\n",
      "iteration :  50, loss : 0.0391, accuracy : 98.67\n",
      "iteration : 100, loss : 0.0340, accuracy : 98.91\n",
      "iteration : 150, loss : 0.0347, accuracy : 98.86\n",
      "iteration : 200, loss : 0.0359, accuracy : 98.82\n",
      "iteration : 250, loss : 0.0359, accuracy : 98.83\n",
      "iteration : 300, loss : 0.0358, accuracy : 98.83\n",
      "Epoch : 208, training loss : 0.0356, training accuracy : 98.83, validation loss : 0.2767, validation accuracy : 92.33\n",
      "\n",
      "Epoch: 209\n",
      "iteration :  50, loss : 0.0344, accuracy : 98.81\n",
      "iteration : 100, loss : 0.0340, accuracy : 98.90\n",
      "iteration : 150, loss : 0.0328, accuracy : 98.92\n",
      "iteration : 200, loss : 0.0325, accuracy : 98.95\n",
      "iteration : 250, loss : 0.0319, accuracy : 98.97\n",
      "iteration : 300, loss : 0.0324, accuracy : 98.97\n",
      "Epoch : 209, training loss : 0.0326, training accuracy : 98.97, validation loss : 0.2769, validation accuracy : 92.05\n",
      "\n",
      "Epoch: 210\n",
      "iteration :  50, loss : 0.0381, accuracy : 98.89\n",
      "iteration : 100, loss : 0.0360, accuracy : 98.94\n",
      "iteration : 150, loss : 0.0341, accuracy : 98.96\n",
      "iteration : 200, loss : 0.0333, accuracy : 98.98\n",
      "iteration : 250, loss : 0.0341, accuracy : 98.94\n",
      "iteration : 300, loss : 0.0337, accuracy : 98.95\n",
      "Epoch : 210, training loss : 0.0337, training accuracy : 98.94, validation loss : 0.2899, validation accuracy : 91.78\n",
      "\n",
      "Epoch: 211\n",
      "iteration :  50, loss : 0.0291, accuracy : 99.11\n",
      "iteration : 100, loss : 0.0296, accuracy : 99.13\n",
      "iteration : 150, loss : 0.0293, accuracy : 99.11\n",
      "iteration : 200, loss : 0.0294, accuracy : 99.11\n",
      "iteration : 250, loss : 0.0304, accuracy : 99.07\n",
      "iteration : 300, loss : 0.0302, accuracy : 99.07\n",
      "Epoch : 211, training loss : 0.0303, training accuracy : 99.08, validation loss : 0.2848, validation accuracy : 91.95\n",
      "\n",
      "Epoch: 212\n",
      "iteration :  50, loss : 0.0284, accuracy : 99.14\n",
      "iteration : 100, loss : 0.0306, accuracy : 99.04\n",
      "iteration : 150, loss : 0.0325, accuracy : 98.95\n",
      "iteration : 200, loss : 0.0323, accuracy : 98.97\n",
      "iteration : 250, loss : 0.0329, accuracy : 98.95\n",
      "iteration : 300, loss : 0.0323, accuracy : 98.95\n",
      "Epoch : 212, training loss : 0.0325, training accuracy : 98.94, validation loss : 0.2867, validation accuracy : 91.91\n",
      "\n",
      "Epoch: 213\n",
      "iteration :  50, loss : 0.0242, accuracy : 99.30\n",
      "iteration : 100, loss : 0.0277, accuracy : 99.09\n",
      "iteration : 150, loss : 0.0298, accuracy : 99.07\n",
      "iteration : 200, loss : 0.0317, accuracy : 99.01\n",
      "iteration : 250, loss : 0.0326, accuracy : 98.97\n",
      "iteration : 300, loss : 0.0330, accuracy : 98.96\n",
      "Epoch : 213, training loss : 0.0331, training accuracy : 98.96, validation loss : 0.2687, validation accuracy : 92.19\n",
      "\n",
      "Epoch: 214\n",
      "iteration :  50, loss : 0.0298, accuracy : 99.02\n",
      "iteration : 100, loss : 0.0300, accuracy : 98.98\n",
      "iteration : 150, loss : 0.0311, accuracy : 98.96\n",
      "iteration : 200, loss : 0.0318, accuracy : 98.95\n",
      "iteration : 250, loss : 0.0309, accuracy : 98.97\n",
      "iteration : 300, loss : 0.0309, accuracy : 99.00\n",
      "Epoch : 214, training loss : 0.0311, training accuracy : 99.00, validation loss : 0.2794, validation accuracy : 91.78\n",
      "\n",
      "Epoch: 215\n",
      "iteration :  50, loss : 0.0288, accuracy : 99.12\n",
      "iteration : 100, loss : 0.0286, accuracy : 99.16\n",
      "iteration : 150, loss : 0.0290, accuracy : 99.09\n",
      "iteration : 200, loss : 0.0295, accuracy : 99.06\n",
      "iteration : 250, loss : 0.0302, accuracy : 99.06\n",
      "iteration : 300, loss : 0.0305, accuracy : 99.05\n",
      "Epoch : 215, training loss : 0.0304, training accuracy : 99.06, validation loss : 0.2670, validation accuracy : 92.71\n",
      "\n",
      "Epoch: 216\n",
      "iteration :  50, loss : 0.0352, accuracy : 98.81\n",
      "iteration : 100, loss : 0.0320, accuracy : 98.93\n",
      "iteration : 150, loss : 0.0307, accuracy : 99.04\n",
      "iteration : 200, loss : 0.0301, accuracy : 99.03\n",
      "iteration : 250, loss : 0.0295, accuracy : 99.05\n",
      "iteration : 300, loss : 0.0294, accuracy : 99.06\n",
      "Epoch : 216, training loss : 0.0293, training accuracy : 99.06, validation loss : 0.2877, validation accuracy : 92.06\n",
      "\n",
      "Epoch: 217\n",
      "iteration :  50, loss : 0.0334, accuracy : 98.92\n",
      "iteration : 100, loss : 0.0290, accuracy : 99.05\n",
      "iteration : 150, loss : 0.0278, accuracy : 99.08\n",
      "iteration : 200, loss : 0.0291, accuracy : 99.05\n",
      "iteration : 250, loss : 0.0293, accuracy : 99.03\n",
      "iteration : 300, loss : 0.0296, accuracy : 99.03\n",
      "Epoch : 217, training loss : 0.0293, training accuracy : 99.05, validation loss : 0.2718, validation accuracy : 92.16\n",
      "\n",
      "Epoch: 218\n",
      "iteration :  50, loss : 0.0251, accuracy : 99.12\n",
      "iteration : 100, loss : 0.0264, accuracy : 99.12\n",
      "iteration : 150, loss : 0.0280, accuracy : 99.09\n",
      "iteration : 200, loss : 0.0289, accuracy : 99.08\n",
      "iteration : 250, loss : 0.0286, accuracy : 99.10\n",
      "iteration : 300, loss : 0.0289, accuracy : 99.09\n",
      "Epoch : 218, training loss : 0.0293, training accuracy : 99.08, validation loss : 0.2804, validation accuracy : 92.09\n",
      "\n",
      "Epoch: 219\n",
      "iteration :  50, loss : 0.0315, accuracy : 98.97\n",
      "iteration : 100, loss : 0.0299, accuracy : 99.02\n",
      "iteration : 150, loss : 0.0283, accuracy : 99.08\n",
      "iteration : 200, loss : 0.0282, accuracy : 99.07\n",
      "iteration : 250, loss : 0.0282, accuracy : 99.07\n",
      "iteration : 300, loss : 0.0293, accuracy : 99.02\n",
      "Epoch : 219, training loss : 0.0296, training accuracy : 99.03, validation loss : 0.2902, validation accuracy : 91.90\n",
      "\n",
      "Epoch: 220\n",
      "iteration :  50, loss : 0.0287, accuracy : 99.11\n",
      "iteration : 100, loss : 0.0277, accuracy : 99.09\n",
      "iteration : 150, loss : 0.0266, accuracy : 99.16\n",
      "iteration : 200, loss : 0.0262, accuracy : 99.19\n",
      "iteration : 250, loss : 0.0275, accuracy : 99.15\n",
      "iteration : 300, loss : 0.0288, accuracy : 99.10\n",
      "Epoch : 220, training loss : 0.0290, training accuracy : 99.10, validation loss : 0.2816, validation accuracy : 91.96\n",
      "\n",
      "Epoch: 221\n",
      "iteration :  50, loss : 0.0273, accuracy : 99.25\n",
      "iteration : 100, loss : 0.0252, accuracy : 99.28\n",
      "iteration : 150, loss : 0.0270, accuracy : 99.19\n",
      "iteration : 200, loss : 0.0277, accuracy : 99.19\n",
      "iteration : 250, loss : 0.0284, accuracy : 99.16\n",
      "iteration : 300, loss : 0.0283, accuracy : 99.15\n",
      "Epoch : 221, training loss : 0.0282, training accuracy : 99.16, validation loss : 0.2847, validation accuracy : 91.82\n",
      "\n",
      "Epoch: 222\n",
      "iteration :  50, loss : 0.0274, accuracy : 99.11\n",
      "iteration : 100, loss : 0.0274, accuracy : 99.16\n",
      "iteration : 150, loss : 0.0279, accuracy : 99.15\n",
      "iteration : 200, loss : 0.0279, accuracy : 99.15\n",
      "iteration : 250, loss : 0.0270, accuracy : 99.18\n",
      "iteration : 300, loss : 0.0266, accuracy : 99.19\n",
      "Epoch : 222, training loss : 0.0264, training accuracy : 99.19, validation loss : 0.2760, validation accuracy : 92.00\n",
      "\n",
      "Epoch: 223\n",
      "iteration :  50, loss : 0.0249, accuracy : 99.27\n",
      "iteration : 100, loss : 0.0252, accuracy : 99.28\n",
      "iteration : 150, loss : 0.0258, accuracy : 99.23\n",
      "iteration : 200, loss : 0.0260, accuracy : 99.23\n",
      "iteration : 250, loss : 0.0255, accuracy : 99.24\n",
      "iteration : 300, loss : 0.0252, accuracy : 99.24\n",
      "Epoch : 223, training loss : 0.0252, training accuracy : 99.23, validation loss : 0.2737, validation accuracy : 92.31\n",
      "\n",
      "Epoch: 224\n",
      "iteration :  50, loss : 0.0290, accuracy : 99.03\n",
      "iteration : 100, loss : 0.0280, accuracy : 99.10\n",
      "iteration : 150, loss : 0.0258, accuracy : 99.18\n",
      "iteration : 200, loss : 0.0275, accuracy : 99.15\n",
      "iteration : 250, loss : 0.0275, accuracy : 99.13\n",
      "iteration : 300, loss : 0.0276, accuracy : 99.13\n",
      "Epoch : 224, training loss : 0.0273, training accuracy : 99.13, validation loss : 0.2614, validation accuracy : 92.42\n",
      "\n",
      "Epoch: 225\n",
      "iteration :  50, loss : 0.0229, accuracy : 99.28\n",
      "iteration : 100, loss : 0.0243, accuracy : 99.21\n",
      "iteration : 150, loss : 0.0247, accuracy : 99.19\n",
      "iteration : 200, loss : 0.0251, accuracy : 99.20\n",
      "iteration : 250, loss : 0.0254, accuracy : 99.17\n",
      "iteration : 300, loss : 0.0256, accuracy : 99.17\n",
      "Epoch : 225, training loss : 0.0259, training accuracy : 99.16, validation loss : 0.2620, validation accuracy : 92.44\n",
      "\n",
      "Epoch: 226\n",
      "iteration :  50, loss : 0.0291, accuracy : 99.02\n",
      "iteration : 100, loss : 0.0291, accuracy : 99.07\n",
      "iteration : 150, loss : 0.0284, accuracy : 99.09\n",
      "iteration : 200, loss : 0.0281, accuracy : 99.12\n",
      "iteration : 250, loss : 0.0273, accuracy : 99.15\n",
      "iteration : 300, loss : 0.0262, accuracy : 99.20\n",
      "Epoch : 226, training loss : 0.0261, training accuracy : 99.19, validation loss : 0.2782, validation accuracy : 92.36\n",
      "\n",
      "Epoch: 227\n",
      "iteration :  50, loss : 0.0273, accuracy : 99.03\n",
      "iteration : 100, loss : 0.0271, accuracy : 99.12\n",
      "iteration : 150, loss : 0.0257, accuracy : 99.15\n",
      "iteration : 200, loss : 0.0266, accuracy : 99.10\n",
      "iteration : 250, loss : 0.0261, accuracy : 99.14\n",
      "iteration : 300, loss : 0.0259, accuracy : 99.15\n",
      "Epoch : 227, training loss : 0.0257, training accuracy : 99.16, validation loss : 0.2688, validation accuracy : 92.43\n",
      "\n",
      "Epoch: 228\n",
      "iteration :  50, loss : 0.0249, accuracy : 99.22\n",
      "iteration : 100, loss : 0.0231, accuracy : 99.32\n",
      "iteration : 150, loss : 0.0234, accuracy : 99.30\n",
      "iteration : 200, loss : 0.0219, accuracy : 99.34\n",
      "iteration : 250, loss : 0.0224, accuracy : 99.30\n",
      "iteration : 300, loss : 0.0224, accuracy : 99.31\n",
      "Epoch : 228, training loss : 0.0231, training accuracy : 99.28, validation loss : 0.2644, validation accuracy : 92.29\n",
      "\n",
      "Epoch: 229\n",
      "iteration :  50, loss : 0.0239, accuracy : 99.22\n",
      "iteration : 100, loss : 0.0243, accuracy : 99.19\n",
      "iteration : 150, loss : 0.0253, accuracy : 99.18\n",
      "iteration : 200, loss : 0.0249, accuracy : 99.18\n",
      "iteration : 250, loss : 0.0247, accuracy : 99.21\n",
      "iteration : 300, loss : 0.0243, accuracy : 99.23\n",
      "Epoch : 229, training loss : 0.0245, training accuracy : 99.23, validation loss : 0.2679, validation accuracy : 92.39\n",
      "\n",
      "Epoch: 230\n",
      "iteration :  50, loss : 0.0288, accuracy : 98.92\n",
      "iteration : 100, loss : 0.0273, accuracy : 99.06\n",
      "iteration : 150, loss : 0.0256, accuracy : 99.12\n",
      "iteration : 200, loss : 0.0268, accuracy : 99.11\n",
      "iteration : 250, loss : 0.0266, accuracy : 99.11\n",
      "iteration : 300, loss : 0.0260, accuracy : 99.14\n",
      "Epoch : 230, training loss : 0.0256, training accuracy : 99.16, validation loss : 0.2837, validation accuracy : 92.47\n",
      "\n",
      "Epoch: 231\n",
      "iteration :  50, loss : 0.0226, accuracy : 99.28\n",
      "iteration : 100, loss : 0.0217, accuracy : 99.30\n",
      "iteration : 150, loss : 0.0237, accuracy : 99.19\n",
      "iteration : 200, loss : 0.0243, accuracy : 99.18\n",
      "iteration : 250, loss : 0.0238, accuracy : 99.21\n",
      "iteration : 300, loss : 0.0240, accuracy : 99.20\n",
      "Epoch : 231, training loss : 0.0238, training accuracy : 99.21, validation loss : 0.2725, validation accuracy : 92.60\n",
      "\n",
      "Epoch: 232\n",
      "iteration :  50, loss : 0.0225, accuracy : 99.28\n",
      "iteration : 100, loss : 0.0265, accuracy : 99.18\n",
      "iteration : 150, loss : 0.0261, accuracy : 99.19\n",
      "iteration : 200, loss : 0.0249, accuracy : 99.23\n",
      "iteration : 250, loss : 0.0249, accuracy : 99.22\n",
      "iteration : 300, loss : 0.0245, accuracy : 99.22\n",
      "Epoch : 232, training loss : 0.0247, training accuracy : 99.22, validation loss : 0.2604, validation accuracy : 92.71\n",
      "\n",
      "Epoch: 233\n",
      "iteration :  50, loss : 0.0223, accuracy : 99.41\n",
      "iteration : 100, loss : 0.0242, accuracy : 99.26\n",
      "iteration : 150, loss : 0.0255, accuracy : 99.23\n",
      "iteration : 200, loss : 0.0244, accuracy : 99.24\n",
      "iteration : 250, loss : 0.0243, accuracy : 99.26\n",
      "iteration : 300, loss : 0.0245, accuracy : 99.26\n",
      "Epoch : 233, training loss : 0.0247, training accuracy : 99.25, validation loss : 0.2679, validation accuracy : 92.50\n",
      "\n",
      "Epoch: 234\n",
      "iteration :  50, loss : 0.0208, accuracy : 99.42\n",
      "iteration : 100, loss : 0.0209, accuracy : 99.36\n",
      "iteration : 150, loss : 0.0216, accuracy : 99.38\n",
      "iteration : 200, loss : 0.0208, accuracy : 99.39\n",
      "iteration : 250, loss : 0.0210, accuracy : 99.38\n",
      "iteration : 300, loss : 0.0207, accuracy : 99.37\n",
      "Epoch : 234, training loss : 0.0207, training accuracy : 99.38, validation loss : 0.2591, validation accuracy : 92.58\n",
      "\n",
      "Epoch: 235\n",
      "iteration :  50, loss : 0.0200, accuracy : 99.44\n",
      "iteration : 100, loss : 0.0206, accuracy : 99.43\n",
      "iteration : 150, loss : 0.0213, accuracy : 99.42\n",
      "iteration : 200, loss : 0.0229, accuracy : 99.36\n",
      "iteration : 250, loss : 0.0225, accuracy : 99.34\n",
      "iteration : 300, loss : 0.0234, accuracy : 99.28\n",
      "Epoch : 235, training loss : 0.0232, training accuracy : 99.28, validation loss : 0.2662, validation accuracy : 92.39\n",
      "\n",
      "Epoch: 236\n",
      "iteration :  50, loss : 0.0202, accuracy : 99.33\n",
      "iteration : 100, loss : 0.0204, accuracy : 99.31\n",
      "iteration : 150, loss : 0.0197, accuracy : 99.33\n",
      "iteration : 200, loss : 0.0213, accuracy : 99.31\n",
      "iteration : 250, loss : 0.0218, accuracy : 99.30\n",
      "iteration : 300, loss : 0.0214, accuracy : 99.30\n",
      "Epoch : 236, training loss : 0.0213, training accuracy : 99.31, validation loss : 0.2735, validation accuracy : 92.26\n",
      "\n",
      "Epoch: 237\n",
      "iteration :  50, loss : 0.0274, accuracy : 99.11\n",
      "iteration : 100, loss : 0.0240, accuracy : 99.24\n",
      "iteration : 150, loss : 0.0244, accuracy : 99.20\n",
      "iteration : 200, loss : 0.0235, accuracy : 99.21\n",
      "iteration : 250, loss : 0.0233, accuracy : 99.24\n",
      "iteration : 300, loss : 0.0223, accuracy : 99.28\n",
      "Epoch : 237, training loss : 0.0224, training accuracy : 99.28, validation loss : 0.2751, validation accuracy : 92.40\n",
      "\n",
      "Epoch: 238\n",
      "iteration :  50, loss : 0.0257, accuracy : 99.16\n",
      "iteration : 100, loss : 0.0236, accuracy : 99.21\n",
      "iteration : 150, loss : 0.0237, accuracy : 99.22\n",
      "iteration : 200, loss : 0.0224, accuracy : 99.27\n",
      "iteration : 250, loss : 0.0215, accuracy : 99.32\n",
      "iteration : 300, loss : 0.0214, accuracy : 99.32\n",
      "Epoch : 238, training loss : 0.0217, training accuracy : 99.32, validation loss : 0.2552, validation accuracy : 92.73\n",
      "\n",
      "Epoch: 239\n",
      "iteration :  50, loss : 0.0253, accuracy : 99.19\n",
      "iteration : 100, loss : 0.0211, accuracy : 99.33\n",
      "iteration : 150, loss : 0.0215, accuracy : 99.34\n",
      "iteration : 200, loss : 0.0221, accuracy : 99.30\n",
      "iteration : 250, loss : 0.0220, accuracy : 99.30\n",
      "iteration : 300, loss : 0.0216, accuracy : 99.31\n",
      "Epoch : 239, training loss : 0.0215, training accuracy : 99.31, validation loss : 0.2480, validation accuracy : 92.76\n",
      "\n",
      "Epoch: 240\n",
      "iteration :  50, loss : 0.0170, accuracy : 99.47\n",
      "iteration : 100, loss : 0.0193, accuracy : 99.39\n",
      "iteration : 150, loss : 0.0215, accuracy : 99.30\n",
      "iteration : 200, loss : 0.0221, accuracy : 99.29\n",
      "iteration : 250, loss : 0.0219, accuracy : 99.32\n",
      "iteration : 300, loss : 0.0228, accuracy : 99.27\n",
      "Epoch : 240, training loss : 0.0226, training accuracy : 99.28, validation loss : 0.2553, validation accuracy : 92.65\n",
      "\n",
      "Epoch: 241\n",
      "iteration :  50, loss : 0.0243, accuracy : 99.17\n",
      "iteration : 100, loss : 0.0215, accuracy : 99.29\n",
      "iteration : 150, loss : 0.0205, accuracy : 99.34\n",
      "iteration : 200, loss : 0.0207, accuracy : 99.36\n",
      "iteration : 250, loss : 0.0202, accuracy : 99.39\n",
      "iteration : 300, loss : 0.0204, accuracy : 99.38\n",
      "Epoch : 241, training loss : 0.0207, training accuracy : 99.38, validation loss : 0.2662, validation accuracy : 92.68\n",
      "\n",
      "Epoch: 242\n",
      "iteration :  50, loss : 0.0221, accuracy : 99.39\n",
      "iteration : 100, loss : 0.0193, accuracy : 99.43\n",
      "iteration : 150, loss : 0.0203, accuracy : 99.38\n",
      "iteration : 200, loss : 0.0200, accuracy : 99.38\n",
      "iteration : 250, loss : 0.0196, accuracy : 99.40\n",
      "iteration : 300, loss : 0.0196, accuracy : 99.40\n",
      "Epoch : 242, training loss : 0.0196, training accuracy : 99.40, validation loss : 0.2498, validation accuracy : 92.97\n",
      "\n",
      "Epoch: 243\n",
      "iteration :  50, loss : 0.0204, accuracy : 99.31\n",
      "iteration : 100, loss : 0.0226, accuracy : 99.26\n",
      "iteration : 150, loss : 0.0218, accuracy : 99.29\n",
      "iteration : 200, loss : 0.0210, accuracy : 99.32\n",
      "iteration : 250, loss : 0.0198, accuracy : 99.37\n",
      "iteration : 300, loss : 0.0199, accuracy : 99.35\n",
      "Epoch : 243, training loss : 0.0198, training accuracy : 99.36, validation loss : 0.2481, validation accuracy : 92.86\n",
      "\n",
      "Epoch: 244\n",
      "iteration :  50, loss : 0.0194, accuracy : 99.36\n",
      "iteration : 100, loss : 0.0177, accuracy : 99.44\n",
      "iteration : 150, loss : 0.0191, accuracy : 99.41\n",
      "iteration : 200, loss : 0.0187, accuracy : 99.41\n",
      "iteration : 250, loss : 0.0184, accuracy : 99.44\n",
      "iteration : 300, loss : 0.0189, accuracy : 99.45\n",
      "Epoch : 244, training loss : 0.0186, training accuracy : 99.46, validation loss : 0.2687, validation accuracy : 92.52\n",
      "\n",
      "Epoch: 245\n",
      "iteration :  50, loss : 0.0160, accuracy : 99.45\n",
      "iteration : 100, loss : 0.0196, accuracy : 99.33\n",
      "iteration : 150, loss : 0.0189, accuracy : 99.38\n",
      "iteration : 200, loss : 0.0192, accuracy : 99.39\n",
      "iteration : 250, loss : 0.0191, accuracy : 99.40\n",
      "iteration : 300, loss : 0.0185, accuracy : 99.43\n",
      "Epoch : 245, training loss : 0.0189, training accuracy : 99.42, validation loss : 0.2452, validation accuracy : 92.95\n",
      "\n",
      "Epoch: 246\n",
      "iteration :  50, loss : 0.0178, accuracy : 99.48\n",
      "iteration : 100, loss : 0.0194, accuracy : 99.34\n",
      "iteration : 150, loss : 0.0185, accuracy : 99.36\n",
      "iteration : 200, loss : 0.0189, accuracy : 99.37\n",
      "iteration : 250, loss : 0.0189, accuracy : 99.38\n",
      "iteration : 300, loss : 0.0196, accuracy : 99.36\n",
      "Epoch : 246, training loss : 0.0196, training accuracy : 99.37, validation loss : 0.2578, validation accuracy : 92.69\n",
      "\n",
      "Epoch: 247\n",
      "iteration :  50, loss : 0.0191, accuracy : 99.36\n",
      "iteration : 100, loss : 0.0193, accuracy : 99.40\n",
      "iteration : 150, loss : 0.0199, accuracy : 99.35\n",
      "iteration : 200, loss : 0.0192, accuracy : 99.37\n",
      "iteration : 250, loss : 0.0196, accuracy : 99.34\n",
      "iteration : 300, loss : 0.0191, accuracy : 99.37\n",
      "Epoch : 247, training loss : 0.0191, training accuracy : 99.38, validation loss : 0.2617, validation accuracy : 92.53\n",
      "\n",
      "Epoch: 248\n",
      "iteration :  50, loss : 0.0192, accuracy : 99.42\n",
      "iteration : 100, loss : 0.0201, accuracy : 99.33\n",
      "iteration : 150, loss : 0.0196, accuracy : 99.38\n",
      "iteration : 200, loss : 0.0197, accuracy : 99.38\n",
      "iteration : 250, loss : 0.0200, accuracy : 99.36\n",
      "iteration : 300, loss : 0.0197, accuracy : 99.38\n",
      "Epoch : 248, training loss : 0.0197, training accuracy : 99.38, validation loss : 0.2565, validation accuracy : 92.53\n",
      "\n",
      "Epoch: 249\n",
      "iteration :  50, loss : 0.0179, accuracy : 99.48\n",
      "iteration : 100, loss : 0.0181, accuracy : 99.45\n",
      "iteration : 150, loss : 0.0181, accuracy : 99.45\n",
      "iteration : 200, loss : 0.0182, accuracy : 99.44\n",
      "iteration : 250, loss : 0.0182, accuracy : 99.45\n",
      "iteration : 300, loss : 0.0190, accuracy : 99.41\n",
      "Epoch : 249, training loss : 0.0188, training accuracy : 99.41, validation loss : 0.2446, validation accuracy : 92.79\n",
      "\n",
      "Epoch: 250\n",
      "iteration :  50, loss : 0.0224, accuracy : 99.27\n",
      "iteration : 100, loss : 0.0194, accuracy : 99.43\n",
      "iteration : 150, loss : 0.0182, accuracy : 99.43\n",
      "iteration : 200, loss : 0.0178, accuracy : 99.44\n",
      "iteration : 250, loss : 0.0184, accuracy : 99.42\n",
      "iteration : 300, loss : 0.0181, accuracy : 99.42\n",
      "Epoch : 250, training loss : 0.0185, training accuracy : 99.41, validation loss : 0.2589, validation accuracy : 92.82\n",
      "\n",
      "Epoch: 251\n",
      "iteration :  50, loss : 0.0186, accuracy : 99.41\n",
      "iteration : 100, loss : 0.0186, accuracy : 99.40\n",
      "iteration : 150, loss : 0.0195, accuracy : 99.35\n",
      "iteration : 200, loss : 0.0192, accuracy : 99.36\n",
      "iteration : 250, loss : 0.0181, accuracy : 99.42\n",
      "iteration : 300, loss : 0.0181, accuracy : 99.43\n",
      "Epoch : 251, training loss : 0.0181, training accuracy : 99.42, validation loss : 0.2533, validation accuracy : 92.70\n",
      "\n",
      "Epoch: 252\n",
      "iteration :  50, loss : 0.0159, accuracy : 99.47\n",
      "iteration : 100, loss : 0.0154, accuracy : 99.52\n",
      "iteration : 150, loss : 0.0158, accuracy : 99.53\n",
      "iteration : 200, loss : 0.0169, accuracy : 99.48\n",
      "iteration : 250, loss : 0.0166, accuracy : 99.48\n",
      "iteration : 300, loss : 0.0165, accuracy : 99.48\n",
      "Epoch : 252, training loss : 0.0164, training accuracy : 99.49, validation loss : 0.2497, validation accuracy : 92.97\n",
      "\n",
      "Epoch: 253\n",
      "iteration :  50, loss : 0.0233, accuracy : 99.25\n",
      "iteration : 100, loss : 0.0200, accuracy : 99.39\n",
      "iteration : 150, loss : 0.0192, accuracy : 99.39\n",
      "iteration : 200, loss : 0.0191, accuracy : 99.39\n",
      "iteration : 250, loss : 0.0190, accuracy : 99.39\n",
      "iteration : 300, loss : 0.0189, accuracy : 99.39\n",
      "Epoch : 253, training loss : 0.0189, training accuracy : 99.39, validation loss : 0.2485, validation accuracy : 92.75\n",
      "\n",
      "Epoch: 254\n",
      "iteration :  50, loss : 0.0183, accuracy : 99.41\n",
      "iteration : 100, loss : 0.0197, accuracy : 99.38\n",
      "iteration : 150, loss : 0.0197, accuracy : 99.40\n",
      "iteration : 200, loss : 0.0193, accuracy : 99.41\n",
      "iteration : 250, loss : 0.0193, accuracy : 99.40\n",
      "iteration : 300, loss : 0.0195, accuracy : 99.39\n",
      "Epoch : 254, training loss : 0.0193, training accuracy : 99.40, validation loss : 0.2407, validation accuracy : 93.24\n",
      "\n",
      "Epoch: 255\n",
      "iteration :  50, loss : 0.0165, accuracy : 99.61\n",
      "iteration : 100, loss : 0.0177, accuracy : 99.47\n",
      "iteration : 150, loss : 0.0179, accuracy : 99.46\n",
      "iteration : 200, loss : 0.0170, accuracy : 99.49\n",
      "iteration : 250, loss : 0.0163, accuracy : 99.51\n",
      "iteration : 300, loss : 0.0163, accuracy : 99.52\n",
      "Epoch : 255, training loss : 0.0163, training accuracy : 99.51, validation loss : 0.2455, validation accuracy : 92.98\n",
      "\n",
      "Epoch: 256\n",
      "iteration :  50, loss : 0.0139, accuracy : 99.52\n",
      "iteration : 100, loss : 0.0149, accuracy : 99.51\n",
      "iteration : 150, loss : 0.0158, accuracy : 99.49\n",
      "iteration : 200, loss : 0.0155, accuracy : 99.52\n",
      "iteration : 250, loss : 0.0154, accuracy : 99.51\n",
      "iteration : 300, loss : 0.0157, accuracy : 99.49\n",
      "Epoch : 256, training loss : 0.0159, training accuracy : 99.49, validation loss : 0.2452, validation accuracy : 93.04\n",
      "\n",
      "Epoch: 257\n",
      "iteration :  50, loss : 0.0185, accuracy : 99.52\n",
      "iteration : 100, loss : 0.0185, accuracy : 99.48\n",
      "iteration : 150, loss : 0.0180, accuracy : 99.49\n",
      "iteration : 200, loss : 0.0183, accuracy : 99.49\n",
      "iteration : 250, loss : 0.0181, accuracy : 99.50\n",
      "iteration : 300, loss : 0.0176, accuracy : 99.51\n",
      "Epoch : 257, training loss : 0.0175, training accuracy : 99.51, validation loss : 0.2368, validation accuracy : 93.09\n",
      "\n",
      "Epoch: 258\n",
      "iteration :  50, loss : 0.0138, accuracy : 99.66\n",
      "iteration : 100, loss : 0.0134, accuracy : 99.62\n",
      "iteration : 150, loss : 0.0155, accuracy : 99.54\n",
      "iteration : 200, loss : 0.0157, accuracy : 99.51\n",
      "iteration : 250, loss : 0.0155, accuracy : 99.53\n",
      "iteration : 300, loss : 0.0156, accuracy : 99.54\n",
      "Epoch : 258, training loss : 0.0155, training accuracy : 99.55, validation loss : 0.2452, validation accuracy : 93.19\n",
      "\n",
      "Epoch: 259\n",
      "iteration :  50, loss : 0.0183, accuracy : 99.50\n",
      "iteration : 100, loss : 0.0164, accuracy : 99.55\n",
      "iteration : 150, loss : 0.0166, accuracy : 99.55\n",
      "iteration : 200, loss : 0.0168, accuracy : 99.52\n",
      "iteration : 250, loss : 0.0168, accuracy : 99.51\n",
      "iteration : 300, loss : 0.0168, accuracy : 99.48\n",
      "Epoch : 259, training loss : 0.0168, training accuracy : 99.49, validation loss : 0.2326, validation accuracy : 93.11\n",
      "\n",
      "Epoch: 260\n",
      "iteration :  50, loss : 0.0164, accuracy : 99.53\n",
      "iteration : 100, loss : 0.0169, accuracy : 99.52\n",
      "iteration : 150, loss : 0.0171, accuracy : 99.52\n",
      "iteration : 200, loss : 0.0168, accuracy : 99.53\n",
      "iteration : 250, loss : 0.0162, accuracy : 99.54\n",
      "iteration : 300, loss : 0.0157, accuracy : 99.55\n",
      "Epoch : 260, training loss : 0.0158, training accuracy : 99.55, validation loss : 0.2505, validation accuracy : 92.99\n",
      "\n",
      "Epoch: 261\n",
      "iteration :  50, loss : 0.0138, accuracy : 99.55\n",
      "iteration : 100, loss : 0.0142, accuracy : 99.57\n",
      "iteration : 150, loss : 0.0141, accuracy : 99.58\n",
      "iteration : 200, loss : 0.0142, accuracy : 99.56\n",
      "iteration : 250, loss : 0.0140, accuracy : 99.56\n",
      "iteration : 300, loss : 0.0138, accuracy : 99.58\n",
      "Epoch : 261, training loss : 0.0143, training accuracy : 99.56, validation loss : 0.2447, validation accuracy : 93.12\n",
      "\n",
      "Epoch: 262\n",
      "iteration :  50, loss : 0.0162, accuracy : 99.47\n",
      "iteration : 100, loss : 0.0175, accuracy : 99.42\n",
      "iteration : 150, loss : 0.0183, accuracy : 99.41\n",
      "iteration : 200, loss : 0.0174, accuracy : 99.43\n",
      "iteration : 250, loss : 0.0165, accuracy : 99.48\n",
      "iteration : 300, loss : 0.0165, accuracy : 99.48\n",
      "Epoch : 262, training loss : 0.0163, training accuracy : 99.48, validation loss : 0.2514, validation accuracy : 92.87\n",
      "\n",
      "Epoch: 263\n",
      "iteration :  50, loss : 0.0171, accuracy : 99.48\n",
      "iteration : 100, loss : 0.0160, accuracy : 99.47\n",
      "iteration : 150, loss : 0.0162, accuracy : 99.48\n",
      "iteration : 200, loss : 0.0162, accuracy : 99.49\n",
      "iteration : 250, loss : 0.0160, accuracy : 99.50\n",
      "iteration : 300, loss : 0.0160, accuracy : 99.51\n",
      "Epoch : 263, training loss : 0.0158, training accuracy : 99.52, validation loss : 0.2414, validation accuracy : 93.35\n",
      "\n",
      "Epoch: 264\n",
      "iteration :  50, loss : 0.0192, accuracy : 99.47\n",
      "iteration : 100, loss : 0.0190, accuracy : 99.47\n",
      "iteration : 150, loss : 0.0184, accuracy : 99.51\n",
      "iteration : 200, loss : 0.0173, accuracy : 99.54\n",
      "iteration : 250, loss : 0.0170, accuracy : 99.52\n",
      "iteration : 300, loss : 0.0168, accuracy : 99.51\n",
      "Epoch : 264, training loss : 0.0167, training accuracy : 99.51, validation loss : 0.2565, validation accuracy : 92.74\n",
      "\n",
      "Epoch: 265\n",
      "iteration :  50, loss : 0.0131, accuracy : 99.61\n",
      "iteration : 100, loss : 0.0125, accuracy : 99.60\n",
      "iteration : 150, loss : 0.0139, accuracy : 99.57\n",
      "iteration : 200, loss : 0.0138, accuracy : 99.58\n",
      "iteration : 250, loss : 0.0138, accuracy : 99.58\n",
      "iteration : 300, loss : 0.0141, accuracy : 99.57\n",
      "Epoch : 265, training loss : 0.0139, training accuracy : 99.58, validation loss : 0.2383, validation accuracy : 93.16\n",
      "\n",
      "Epoch: 266\n",
      "iteration :  50, loss : 0.0126, accuracy : 99.70\n",
      "iteration : 100, loss : 0.0138, accuracy : 99.61\n",
      "iteration : 150, loss : 0.0137, accuracy : 99.63\n",
      "iteration : 200, loss : 0.0134, accuracy : 99.63\n",
      "iteration : 250, loss : 0.0136, accuracy : 99.63\n",
      "iteration : 300, loss : 0.0139, accuracy : 99.60\n",
      "Epoch : 266, training loss : 0.0141, training accuracy : 99.58, validation loss : 0.2472, validation accuracy : 93.05\n",
      "\n",
      "Epoch: 267\n",
      "iteration :  50, loss : 0.0173, accuracy : 99.38\n",
      "iteration : 100, loss : 0.0155, accuracy : 99.50\n",
      "iteration : 150, loss : 0.0154, accuracy : 99.50\n",
      "iteration : 200, loss : 0.0150, accuracy : 99.52\n",
      "iteration : 250, loss : 0.0148, accuracy : 99.52\n",
      "iteration : 300, loss : 0.0146, accuracy : 99.54\n",
      "Epoch : 267, training loss : 0.0146, training accuracy : 99.54, validation loss : 0.2516, validation accuracy : 92.91\n",
      "\n",
      "Epoch: 268\n",
      "iteration :  50, loss : 0.0137, accuracy : 99.64\n",
      "iteration : 100, loss : 0.0139, accuracy : 99.60\n",
      "iteration : 150, loss : 0.0139, accuracy : 99.58\n",
      "iteration : 200, loss : 0.0140, accuracy : 99.59\n",
      "iteration : 250, loss : 0.0134, accuracy : 99.61\n",
      "iteration : 300, loss : 0.0141, accuracy : 99.58\n",
      "Epoch : 268, training loss : 0.0141, training accuracy : 99.57, validation loss : 0.2473, validation accuracy : 93.25\n",
      "\n",
      "Epoch: 269\n",
      "iteration :  50, loss : 0.0113, accuracy : 99.69\n",
      "iteration : 100, loss : 0.0122, accuracy : 99.66\n",
      "iteration : 150, loss : 0.0123, accuracy : 99.67\n",
      "iteration : 200, loss : 0.0134, accuracy : 99.61\n",
      "iteration : 250, loss : 0.0137, accuracy : 99.58\n",
      "iteration : 300, loss : 0.0143, accuracy : 99.55\n",
      "Epoch : 269, training loss : 0.0146, training accuracy : 99.53, validation loss : 0.2328, validation accuracy : 93.27\n",
      "\n",
      "Epoch: 270\n",
      "iteration :  50, loss : 0.0164, accuracy : 99.48\n",
      "iteration : 100, loss : 0.0146, accuracy : 99.57\n",
      "iteration : 150, loss : 0.0158, accuracy : 99.56\n",
      "iteration : 200, loss : 0.0155, accuracy : 99.55\n",
      "iteration : 250, loss : 0.0149, accuracy : 99.56\n",
      "iteration : 300, loss : 0.0151, accuracy : 99.54\n",
      "Epoch : 270, training loss : 0.0152, training accuracy : 99.54, validation loss : 0.2418, validation accuracy : 93.29\n",
      "\n",
      "Epoch: 271\n",
      "iteration :  50, loss : 0.0107, accuracy : 99.72\n",
      "iteration : 100, loss : 0.0132, accuracy : 99.65\n",
      "iteration : 150, loss : 0.0142, accuracy : 99.59\n",
      "iteration : 200, loss : 0.0139, accuracy : 99.60\n",
      "iteration : 250, loss : 0.0145, accuracy : 99.57\n",
      "iteration : 300, loss : 0.0145, accuracy : 99.58\n",
      "Epoch : 271, training loss : 0.0143, training accuracy : 99.59, validation loss : 0.2450, validation accuracy : 93.01\n",
      "\n",
      "Epoch: 272\n",
      "iteration :  50, loss : 0.0142, accuracy : 99.66\n",
      "iteration : 100, loss : 0.0150, accuracy : 99.58\n",
      "iteration : 150, loss : 0.0146, accuracy : 99.62\n",
      "iteration : 200, loss : 0.0147, accuracy : 99.61\n",
      "iteration : 250, loss : 0.0150, accuracy : 99.58\n",
      "iteration : 300, loss : 0.0149, accuracy : 99.58\n",
      "Epoch : 272, training loss : 0.0150, training accuracy : 99.57, validation loss : 0.2465, validation accuracy : 93.09\n",
      "\n",
      "Epoch: 273\n",
      "iteration :  50, loss : 0.0121, accuracy : 99.64\n",
      "iteration : 100, loss : 0.0124, accuracy : 99.64\n",
      "iteration : 150, loss : 0.0122, accuracy : 99.67\n",
      "iteration : 200, loss : 0.0135, accuracy : 99.62\n",
      "iteration : 250, loss : 0.0129, accuracy : 99.63\n",
      "iteration : 300, loss : 0.0126, accuracy : 99.64\n",
      "Epoch : 273, training loss : 0.0129, training accuracy : 99.64, validation loss : 0.2632, validation accuracy : 93.15\n",
      "\n",
      "Epoch: 274\n",
      "iteration :  50, loss : 0.0127, accuracy : 99.67\n",
      "iteration : 100, loss : 0.0146, accuracy : 99.60\n",
      "iteration : 150, loss : 0.0135, accuracy : 99.64\n",
      "iteration : 200, loss : 0.0134, accuracy : 99.63\n",
      "iteration : 250, loss : 0.0141, accuracy : 99.60\n",
      "iteration : 300, loss : 0.0138, accuracy : 99.61\n",
      "Epoch : 274, training loss : 0.0136, training accuracy : 99.61, validation loss : 0.2473, validation accuracy : 93.29\n",
      "\n",
      "Epoch: 275\n",
      "iteration :  50, loss : 0.0144, accuracy : 99.55\n",
      "iteration : 100, loss : 0.0165, accuracy : 99.44\n",
      "iteration : 150, loss : 0.0153, accuracy : 99.47\n",
      "iteration : 200, loss : 0.0154, accuracy : 99.50\n",
      "iteration : 250, loss : 0.0153, accuracy : 99.49\n",
      "iteration : 300, loss : 0.0158, accuracy : 99.46\n",
      "Epoch : 275, training loss : 0.0157, training accuracy : 99.46, validation loss : 0.2368, validation accuracy : 93.29\n",
      "\n",
      "Epoch: 276\n",
      "iteration :  50, loss : 0.0154, accuracy : 99.58\n",
      "iteration : 100, loss : 0.0138, accuracy : 99.67\n",
      "iteration : 150, loss : 0.0144, accuracy : 99.62\n",
      "iteration : 200, loss : 0.0141, accuracy : 99.62\n",
      "iteration : 250, loss : 0.0140, accuracy : 99.61\n",
      "iteration : 300, loss : 0.0138, accuracy : 99.60\n",
      "Epoch : 276, training loss : 0.0138, training accuracy : 99.61, validation loss : 0.2361, validation accuracy : 93.18\n",
      "\n",
      "Epoch: 277\n",
      "iteration :  50, loss : 0.0141, accuracy : 99.53\n",
      "iteration : 100, loss : 0.0150, accuracy : 99.53\n",
      "iteration : 150, loss : 0.0149, accuracy : 99.52\n",
      "iteration : 200, loss : 0.0148, accuracy : 99.52\n",
      "iteration : 250, loss : 0.0144, accuracy : 99.54\n",
      "iteration : 300, loss : 0.0145, accuracy : 99.53\n",
      "Epoch : 277, training loss : 0.0142, training accuracy : 99.55, validation loss : 0.2414, validation accuracy : 93.09\n",
      "\n",
      "Epoch: 278\n",
      "iteration :  50, loss : 0.0120, accuracy : 99.61\n",
      "iteration : 100, loss : 0.0150, accuracy : 99.55\n",
      "iteration : 150, loss : 0.0145, accuracy : 99.59\n",
      "iteration : 200, loss : 0.0138, accuracy : 99.62\n",
      "iteration : 250, loss : 0.0130, accuracy : 99.63\n",
      "iteration : 300, loss : 0.0130, accuracy : 99.63\n",
      "Epoch : 278, training loss : 0.0131, training accuracy : 99.63, validation loss : 0.2459, validation accuracy : 93.18\n",
      "\n",
      "Epoch: 279\n",
      "iteration :  50, loss : 0.0146, accuracy : 99.56\n",
      "iteration : 100, loss : 0.0137, accuracy : 99.54\n",
      "iteration : 150, loss : 0.0136, accuracy : 99.54\n",
      "iteration : 200, loss : 0.0137, accuracy : 99.55\n",
      "iteration : 250, loss : 0.0138, accuracy : 99.56\n",
      "iteration : 300, loss : 0.0137, accuracy : 99.57\n",
      "Epoch : 279, training loss : 0.0136, training accuracy : 99.58, validation loss : 0.2565, validation accuracy : 92.83\n",
      "\n",
      "Epoch: 280\n",
      "iteration :  50, loss : 0.0116, accuracy : 99.73\n",
      "iteration : 100, loss : 0.0124, accuracy : 99.70\n",
      "iteration : 150, loss : 0.0141, accuracy : 99.63\n",
      "iteration : 200, loss : 0.0134, accuracy : 99.62\n",
      "iteration : 250, loss : 0.0130, accuracy : 99.64\n",
      "iteration : 300, loss : 0.0129, accuracy : 99.64\n",
      "Epoch : 280, training loss : 0.0129, training accuracy : 99.64, validation loss : 0.2324, validation accuracy : 93.42\n",
      "\n",
      "Epoch: 281\n",
      "iteration :  50, loss : 0.0141, accuracy : 99.55\n",
      "iteration : 100, loss : 0.0131, accuracy : 99.58\n",
      "iteration : 150, loss : 0.0132, accuracy : 99.57\n",
      "iteration : 200, loss : 0.0132, accuracy : 99.56\n",
      "iteration : 250, loss : 0.0136, accuracy : 99.56\n",
      "iteration : 300, loss : 0.0138, accuracy : 99.55\n",
      "Epoch : 281, training loss : 0.0139, training accuracy : 99.55, validation loss : 0.2299, validation accuracy : 93.15\n",
      "\n",
      "Epoch: 282\n",
      "iteration :  50, loss : 0.0131, accuracy : 99.64\n",
      "iteration : 100, loss : 0.0125, accuracy : 99.66\n",
      "iteration : 150, loss : 0.0123, accuracy : 99.68\n",
      "iteration : 200, loss : 0.0132, accuracy : 99.64\n",
      "iteration : 250, loss : 0.0125, accuracy : 99.65\n",
      "iteration : 300, loss : 0.0131, accuracy : 99.63\n",
      "Epoch : 282, training loss : 0.0131, training accuracy : 99.63, validation loss : 0.2417, validation accuracy : 93.42\n",
      "\n",
      "Epoch: 283\n",
      "iteration :  50, loss : 0.0135, accuracy : 99.56\n",
      "iteration : 100, loss : 0.0138, accuracy : 99.57\n",
      "iteration : 150, loss : 0.0138, accuracy : 99.56\n",
      "iteration : 200, loss : 0.0134, accuracy : 99.57\n",
      "iteration : 250, loss : 0.0137, accuracy : 99.55\n",
      "iteration : 300, loss : 0.0139, accuracy : 99.55\n",
      "Epoch : 283, training loss : 0.0139, training accuracy : 99.55, validation loss : 0.2238, validation accuracy : 93.42\n",
      "\n",
      "Epoch: 284\n",
      "iteration :  50, loss : 0.0161, accuracy : 99.42\n",
      "iteration : 100, loss : 0.0146, accuracy : 99.53\n",
      "iteration : 150, loss : 0.0150, accuracy : 99.53\n",
      "iteration : 200, loss : 0.0150, accuracy : 99.55\n",
      "iteration : 250, loss : 0.0146, accuracy : 99.56\n",
      "iteration : 300, loss : 0.0146, accuracy : 99.57\n",
      "Epoch : 284, training loss : 0.0146, training accuracy : 99.56, validation loss : 0.2312, validation accuracy : 93.24\n",
      "\n",
      "Epoch: 285\n",
      "iteration :  50, loss : 0.0114, accuracy : 99.66\n",
      "iteration : 100, loss : 0.0123, accuracy : 99.62\n",
      "iteration : 150, loss : 0.0129, accuracy : 99.58\n",
      "iteration : 200, loss : 0.0129, accuracy : 99.61\n",
      "iteration : 250, loss : 0.0134, accuracy : 99.58\n",
      "iteration : 300, loss : 0.0136, accuracy : 99.58\n",
      "Epoch : 285, training loss : 0.0136, training accuracy : 99.58, validation loss : 0.2339, validation accuracy : 93.12\n",
      "\n",
      "Epoch: 286\n",
      "iteration :  50, loss : 0.0128, accuracy : 99.59\n",
      "iteration : 100, loss : 0.0136, accuracy : 99.57\n",
      "iteration : 150, loss : 0.0135, accuracy : 99.54\n",
      "iteration : 200, loss : 0.0132, accuracy : 99.56\n",
      "iteration : 250, loss : 0.0135, accuracy : 99.57\n",
      "iteration : 300, loss : 0.0140, accuracy : 99.56\n",
      "Epoch : 286, training loss : 0.0140, training accuracy : 99.56, validation loss : 0.2349, validation accuracy : 93.34\n",
      "\n",
      "Epoch: 287\n",
      "iteration :  50, loss : 0.0137, accuracy : 99.56\n",
      "iteration : 100, loss : 0.0116, accuracy : 99.63\n",
      "iteration : 150, loss : 0.0126, accuracy : 99.61\n",
      "iteration : 200, loss : 0.0131, accuracy : 99.61\n",
      "iteration : 250, loss : 0.0128, accuracy : 99.63\n",
      "iteration : 300, loss : 0.0127, accuracy : 99.62\n",
      "Epoch : 287, training loss : 0.0127, training accuracy : 99.62, validation loss : 0.2339, validation accuracy : 93.25\n",
      "\n",
      "Epoch: 288\n",
      "iteration :  50, loss : 0.0152, accuracy : 99.50\n",
      "iteration : 100, loss : 0.0153, accuracy : 99.50\n",
      "iteration : 150, loss : 0.0140, accuracy : 99.54\n",
      "iteration : 200, loss : 0.0136, accuracy : 99.57\n",
      "iteration : 250, loss : 0.0133, accuracy : 99.59\n",
      "iteration : 300, loss : 0.0135, accuracy : 99.59\n",
      "Epoch : 288, training loss : 0.0132, training accuracy : 99.60, validation loss : 0.2243, validation accuracy : 93.58\n",
      "\n",
      "Epoch: 289\n",
      "iteration :  50, loss : 0.0079, accuracy : 99.81\n",
      "iteration : 100, loss : 0.0092, accuracy : 99.76\n",
      "iteration : 150, loss : 0.0096, accuracy : 99.75\n",
      "iteration : 200, loss : 0.0105, accuracy : 99.69\n",
      "iteration : 250, loss : 0.0110, accuracy : 99.67\n",
      "iteration : 300, loss : 0.0115, accuracy : 99.67\n",
      "Epoch : 289, training loss : 0.0114, training accuracy : 99.68, validation loss : 0.2356, validation accuracy : 93.25\n",
      "\n",
      "Epoch: 290\n",
      "iteration :  50, loss : 0.0092, accuracy : 99.81\n",
      "iteration : 100, loss : 0.0109, accuracy : 99.70\n",
      "iteration : 150, loss : 0.0133, accuracy : 99.62\n",
      "iteration : 200, loss : 0.0126, accuracy : 99.66\n",
      "iteration : 250, loss : 0.0128, accuracy : 99.66\n",
      "iteration : 300, loss : 0.0127, accuracy : 99.65\n",
      "Epoch : 290, training loss : 0.0130, training accuracy : 99.64, validation loss : 0.2377, validation accuracy : 93.37\n",
      "\n",
      "Epoch: 291\n",
      "iteration :  50, loss : 0.0152, accuracy : 99.59\n",
      "iteration : 100, loss : 0.0153, accuracy : 99.52\n",
      "iteration : 150, loss : 0.0147, accuracy : 99.54\n",
      "iteration : 200, loss : 0.0143, accuracy : 99.54\n",
      "iteration : 250, loss : 0.0139, accuracy : 99.58\n",
      "iteration : 300, loss : 0.0137, accuracy : 99.58\n",
      "Epoch : 291, training loss : 0.0135, training accuracy : 99.58, validation loss : 0.2385, validation accuracy : 93.42\n",
      "\n",
      "Epoch: 292\n",
      "iteration :  50, loss : 0.0138, accuracy : 99.59\n",
      "iteration : 100, loss : 0.0123, accuracy : 99.64\n",
      "iteration : 150, loss : 0.0126, accuracy : 99.61\n",
      "iteration : 200, loss : 0.0120, accuracy : 99.64\n",
      "iteration : 250, loss : 0.0116, accuracy : 99.65\n",
      "iteration : 300, loss : 0.0120, accuracy : 99.65\n",
      "Epoch : 292, training loss : 0.0118, training accuracy : 99.66, validation loss : 0.2386, validation accuracy : 93.28\n",
      "\n",
      "Epoch: 293\n",
      "iteration :  50, loss : 0.0124, accuracy : 99.61\n",
      "iteration : 100, loss : 0.0138, accuracy : 99.60\n",
      "iteration : 150, loss : 0.0131, accuracy : 99.64\n",
      "iteration : 200, loss : 0.0140, accuracy : 99.59\n",
      "iteration : 250, loss : 0.0138, accuracy : 99.58\n",
      "iteration : 300, loss : 0.0136, accuracy : 99.59\n",
      "Epoch : 293, training loss : 0.0135, training accuracy : 99.59, validation loss : 0.2415, validation accuracy : 93.30\n",
      "\n",
      "Epoch: 294\n",
      "iteration :  50, loss : 0.0123, accuracy : 99.67\n",
      "iteration : 100, loss : 0.0133, accuracy : 99.60\n",
      "iteration : 150, loss : 0.0132, accuracy : 99.60\n",
      "iteration : 200, loss : 0.0131, accuracy : 99.59\n",
      "iteration : 250, loss : 0.0125, accuracy : 99.62\n",
      "iteration : 300, loss : 0.0123, accuracy : 99.63\n",
      "Epoch : 294, training loss : 0.0123, training accuracy : 99.62, validation loss : 0.2422, validation accuracy : 93.16\n",
      "\n",
      "Epoch: 295\n",
      "iteration :  50, loss : 0.0124, accuracy : 99.61\n",
      "iteration : 100, loss : 0.0119, accuracy : 99.64\n",
      "iteration : 150, loss : 0.0130, accuracy : 99.59\n",
      "iteration : 200, loss : 0.0127, accuracy : 99.62\n",
      "iteration : 250, loss : 0.0131, accuracy : 99.60\n",
      "iteration : 300, loss : 0.0126, accuracy : 99.62\n",
      "Epoch : 295, training loss : 0.0125, training accuracy : 99.61, validation loss : 0.2363, validation accuracy : 93.18\n",
      "\n",
      "Epoch: 296\n",
      "iteration :  50, loss : 0.0145, accuracy : 99.56\n",
      "iteration : 100, loss : 0.0131, accuracy : 99.61\n",
      "iteration : 150, loss : 0.0129, accuracy : 99.61\n",
      "iteration : 200, loss : 0.0134, accuracy : 99.59\n",
      "iteration : 250, loss : 0.0134, accuracy : 99.60\n",
      "iteration : 300, loss : 0.0134, accuracy : 99.60\n",
      "Epoch : 296, training loss : 0.0133, training accuracy : 99.61, validation loss : 0.2473, validation accuracy : 93.21\n",
      "\n",
      "Epoch: 297\n",
      "iteration :  50, loss : 0.0120, accuracy : 99.67\n",
      "iteration : 100, loss : 0.0117, accuracy : 99.70\n",
      "iteration : 150, loss : 0.0118, accuracy : 99.69\n",
      "iteration : 200, loss : 0.0123, accuracy : 99.65\n",
      "iteration : 250, loss : 0.0121, accuracy : 99.65\n",
      "iteration : 300, loss : 0.0123, accuracy : 99.65\n",
      "Epoch : 297, training loss : 0.0122, training accuracy : 99.65, validation loss : 0.2376, validation accuracy : 93.23\n",
      "\n",
      "Epoch: 298\n",
      "iteration :  50, loss : 0.0124, accuracy : 99.61\n",
      "iteration : 100, loss : 0.0138, accuracy : 99.55\n",
      "iteration : 150, loss : 0.0134, accuracy : 99.59\n",
      "iteration : 200, loss : 0.0133, accuracy : 99.60\n",
      "iteration : 250, loss : 0.0141, accuracy : 99.58\n",
      "iteration : 300, loss : 0.0141, accuracy : 99.58\n",
      "Epoch : 298, training loss : 0.0138, training accuracy : 99.59, validation loss : 0.2440, validation accuracy : 93.11\n",
      "\n",
      "Epoch: 299\n",
      "iteration :  50, loss : 0.0132, accuracy : 99.64\n",
      "iteration : 100, loss : 0.0131, accuracy : 99.59\n",
      "iteration : 150, loss : 0.0145, accuracy : 99.53\n",
      "iteration : 200, loss : 0.0144, accuracy : 99.53\n",
      "iteration : 250, loss : 0.0139, accuracy : 99.57\n",
      "iteration : 300, loss : 0.0140, accuracy : 99.56\n",
      "Epoch : 299, training loss : 0.0140, training accuracy : 99.56, validation loss : 0.2231, validation accuracy : 93.65\n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "# main body\n",
    "config = {\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4\n",
    "}\n",
    "\n",
    "train_loss_ls = []\n",
    "train_acc_ls = []\n",
    "valid_loss_ls = []\n",
    "valid_acc_ls = []\n",
    "\n",
    "net = ResNet18().to('cuda')\n",
    "#net.load_state_dict(state['net'])\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
    "                      momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
    "\n",
    "for epoch in range(0, 300):\n",
    "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler, optimizer)\n",
    "    valid_loss, valid_acc = test(epoch, net, criterion, validloader)\n",
    "    \n",
    "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, validation loss \" + \\\n",
    "      \": %0.4f, validation accuracy : %2.2f\") % (epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "\n",
    "    train_loss_ls.append(train_loss)\n",
    "    valid_loss_ls.append(valid_loss)\n",
    "    train_acc_ls.append(train_acc)\n",
    "    valid_acc_ls.append(valid_acc) \n",
    "\n",
    "#Finally, get the test result\n",
    "test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
    "\n",
    "file_name = '6-rep=0.5'\n",
    "if not os.path.isfile(f'./result/{file_name}.pth'):\n",
    "  save_result(file_name, train_loss_ls, valid_loss_ls,  test_loss, train_acc_ls, valid_acc_ls,test_acc)  \n",
    "else:\n",
    "  raise ValueError('File already exist!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#temp = torch.load('./result/lr001.pth')\n",
    "\n",
    "plt.plot(range(len(train_loss_ls)), train_loss_ls, 'b')\n",
    "plt.plot(range(len(valid_loss_ls)), valid_loss_ls, 'r')\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Logistic Regression: Loss vs Number of epochs\")\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABC/0lEQVR4nO2dd5xVxfXAv2eXhYWlV6lSBUQRdK2ADcQualDURNEYjdGo2PUXE0tMbIktscSCEjtBUZPYCYoFpQmI9CqdFXYXlC0se35/nPv2vV3eVnbf2913vp/P+9x75947c+bed+fMnJk5I6qK4ziO4wAkxVsAx3Ecp/bgSsFxHMcpwpWC4ziOU4QrBcdxHKcIVwqO4zhOEa4UHMdxnCJcKVQTIvKUiPy+Cvd1E5EfRSS5JuSqrYjIeyIyNt5yOPFFRC4Wkc/jmP5vRGRz8A22iZccEfLE9XlAgioFEVktIiOqM05VvUJV/1jZtFX1e1Vtqqq7K5Ne8OfZHfyZt4vIPBE5rSqyxwNVPVlVJ8QqPRG5U0RURA6PVZp1ERF5IXhOh0WE9RaRejehSURSgIeAkcE3uDXeMtUGElIp1COmq2pToCXwBPCaiLSs7kTqeitGRAS4CNgWbGOZdoNYpldNbAPuibcQlaUKz7oDkAp8VwPi1FlcKUQgIo1E5BER2RD8HhGRRhHnbxaRjcG5XwU1qt7BuRdE5J5gv62I/EdEskRkm4h8JiJJIvIi0A34d1DDv1lEugfxNAjubS0izwdpZIrIW+XJraqFwItAGtAnIi9/EZHvg+bxUyLSuBJ5eVJE3hWRn4DjRKSTiLwhIhkiskpEromI6zARmRW0WDaLyENBeKqIvCQiW4NnMVNEOgTnPhGRXwX7SSJyu4isEZEtIvJPEWkRnAs9n7FBXn4Qkd9V8tUOAzoC1wDniUjDCNkbi8hfg7SzReTz0HMSkaEi8mUg+1oRubik7MFxsSZ/IO9VIrIMWBaEPRrEsV1EZovIsIjrk0Xk/0RkhYjsCM53FZHHReSvkRkRkXdE5LqSGQze119KhL0tItcH+7eIyPog/iUiMryM5zUBGCgix0Q7KSVau2KtsJeC/dD7uiTIb6aIXCEih4rI/OBZ/n3PKOXvwfNfHCmbiLQQkeeC/+p6EblHgkpK8Ny/EJGHRWQrcGcUWaN+0yKyH7AkuCxLRP5XSl6PiPgPzBORYyPOfSIi94rIjOC9vi0irSPOnyEi3wX3fiIi/SPOdRWRN8W+p60ln4nYt5sp9q2dHBF+sYisDN7jKhH5eTS59wpVTbgfsBoYESX8buAroD3QDvgS+GNw7iRgEzAAaAK8BCjQOzj/AnBPsH8v8BSQEvyGARItbaB7EE+D4Pi/wOtAq+DeY0rJw8XA58F+MnAVkA+0D8IeBt4BWgPNgH8D91YiL9nAEKzi0ASYDfwBaAj0BFYCJwbXTwcuDPabAkcE+78O0m0SyHgI0Dw49wnwq2D/l8DyIN6mwJvAiyWezzNAY+AgIA/oH5wfCmSV876fAyYGz3Mr8LOIc48HsnQOZDwKaATsC+wAzg/uawMMKil7yXcRHCvwUfDsGwdhvwjiaADcEDz/1ODcTcC3QF9Agjy2AQ4DNgBJwXVtgZ1Ahyh5PBpYS/h/1grIAToF8a4FOkU8016lPKsXsFbCNYT/X70BLe37wQrjl0q8r6ewWvhIIBd4C/uuOgNbCP7XwbMrAK4LnvMY7L/XOjg/GfgHVuFpD8wAfl3i3quD59q4kt90SNYGpTyLztj/5RTsOzghOG4X8T9YDxwQyPdGxHPYD/gpuCcFuBn7jzfE/mfzsG80LXhOQyPytAu4LLjuN8F/QIJrtwN9g2s7AgOqvXyMZWFcW34l/9QR4SuAUyKOTwRWB/vjCQrVyA+F6ErhbuDt0Lmy0o78YwYvuRBoVYE8hD6IrOBPlAOcG5yT4A/ZK+L6I4FVlcjLPyPOHw58XyL924Dng/1pwF1A2xLX/DL4CAdGkf8TwkphCnBlxLm+QZ4aRDyfLhHnZwDnVfBdNwk+pDOD438Abwf7ScFzOyjKfbcBk0uJs0j2iHdRUikcX45cmaF0sRrrqFKuWwScEOz/Fni3lOsE+B44Oji+DPhfxPvdAowAUsqR6wVMKTQK4juZqimFzhHntwJjIo7fAMZFPLsNBMos4v1eiJl38ogo7DElPTXi3u/LyU9Z33RI1tKUwi0ElZOIsA+AsRH/g/sizu2PVcySgd8DEyPOJWEK5FjsW8yIlm6Qp+Ul/r8K7IMphSzgZ0RRgNX1c/NRcToBayKO1wRhoXNrI85F7pfkQaxW8GHQ1Lu1gul3BbapamYFr/9KVVtitcJ3sBYJWI2oCTA7aLpmAe8H4VCxvESG7Qt0CsUVxPd/2EcLcClWM1osZiIKdXi/iH1ErwVN9wfEOvdKEu25N4iIH6xmHWIn1qKoCGdhyvPd4Phl4GQRaYfVvFOxgqMkXUsJryjFnqmI3CgiiwITSRbQIki/vLQmYK0Mgu2L0S5SK0FewwpNgAuwvKKqy4FxWOG9RUReE5FOUaKJjC8P+GPwqwqbI/ZzohxHvr/1gfwhQt/dvlgte2PE/+4fWK0/RFnfIZT9TZfHvsA5Jf73Q7HKW7T01wTyti2ZrpqJdy3W+ugKrFHVglLS3RRx385gt6mq/oS1pK7Ansl/RaRfBfNSYVwpFGcD9kcI0S0IA9gIdIk417W0SFR1h6reoKo9gTOA6yPspFrafdifprVUsrNYVX/EmpkXishg4Afswxugqi2DXwu1TumK5iVSzrVYK6NlxK+Zqp4SpL9MVc/HPtb7gUkikqaqu1T1LlXdHzPLnEb0jt5oz72A4gVJVRmLFUDfi8gm4F/Yh3sB9pxygV5R7ltbSjhYK6xJxPE+Ua4pen5B/8HNwLlYK7AlZiKRCqT1EjBKRA4C+mNmmNJ4FRgtIvtirbs3ioRRfUVVh2LPWbH3VB7PY4MYzi4RXpH8V4bOIiIRx6Hvbi3WUmgb8b9rrqoDIq4t63uCsr/p8liLtRQi//dpqnpfxDWR3043rIX7Q8l0g/x1xVoLa4FuUoVBCKr6gaqegCmmxZhZtVpJZKWQItYRGvo1wD6q20WknYi0xWzoLwXXTwQuEZH+ItIEax5GRUROExvGJ9jHvxszC4EVdD2j3aeqG4H3gCdEpJWIpIjI0RXJjKpuA54F/hDUSp4BHhaR9oFMnUXkxMrmJWAGsEOss7KxWMfoASJyaBD3L0SkXZBuVnBPoYgcJyIHBh2D27EPpjBK/K8C14lIDxFpCvwZeL2MmlSFEJHOwHBMGQ0KfgdhBeJFgbzjgYfEOtKTReRIscEFLwMjRORcEWkgIm1EZFAQ9VzgbBFpItY5f2k5ojTDlFwG0EBE/gA0jzj/LPBHEekjxkAJxsyr6jpgJtZCeENVc0pLRFW/wQqkZ4EPVDUreA59ReT4IF+5WIUh2nsoGV8BcAdmRolkLtZhnyIi6cDo8uIqh/bANUF852DK793ge/gQ+KuINBcbkNBLSukAL4WyvunyeAk4XURODP4bqSJyrIhEVqh+ISL7B9/R3cAkteHlE4FTRWR40Dq+AVNwX2Lf00bgPhFJC+IdUp4wItJBREaJSFoQ149U4D1WlkRWCu9iH0fodydmS50FzMc6/uYEYajqe8BjwFTMNPRVEE9elLj7AB9jL2068ISqTg3O3Yv9SbNE5MYo916IFZ6LMTvwuErk6RHgFBEZiH3Iy4GvRGR7IE/fKuSF4E8eKlhXES54WgSXnAR8JyI/Ao9i9v4crAY5CVMIi4BPiW7+GB+ETwviz8U6D8tFRIYF6UbjQmCuqn6oqptCvyDvA0XkAOBG7F3PxIZi3o917H6PdTDeEITPxRQKWAdhPqbgJxCYacrgA8x8txQzKeRS3OzwEFaIfIg9q+ewTvUQE4ADKcV0VIJXsL6DVyLCGgH3Ye9tE1YI31aBuMAK1Y0lwn6PtWwysb6kV0reVEm+xr6ZH4A/AaM1PGfgIqxzdmGQ3iSKm2/Ko9RvujxUdS0wCjOVZmDv7CaKl5svYv0wmzBT5DXBvUswc9/fgnydDpyuqvnB93Q61lfzPbAOMwuVRxJwPdYK2QYcg1kIqpXQSAWnkogNL1sANNrbGm28qU95qY8ErcWXgH3VP9hag4h8gnWwPxtvWaqTRG4pVBoROUtsjHMrrEb577paiNanvNRnAtPDtcCzrhCcWOBKoXL8GjPprMD6Caq96RZD6lNe6iVBCy4LM5c8EldhnITBzUeO4zhOEd5ScBzHcYqoi866imjbtq1279493mI4juPUKWbPnv2DqraLdq5OK4Xu3bsza9aseIvhOI5TpxCRNaWdc/OR4ziOU4QrBcdxHKcIVwqO4zhOEa4UHMdxnCJqTCmIyHixVbQWRIS1FpGPRGRZsG0VhIuIPCYiy8VWZzq4puRyHMdxSqcmWwovYI7SIrkVmKKqfbCFVULrDJyMOcTqA1wOPFmDcjmO4zilUGNKQVWnYZ78IhmFeXwk2J4ZEf5PNb4CWopIZTwhOo7jONVArOcpdAh8pIO5mg2trNWZ4q6E1wVhJV32IiKXY60JunXrVnOSOo6TcOzcCQ0bwu7dsHUrtGoFjQMn5pmZsHYtpKVB06ZQWAht28LmzXbt9u2wYQP06QO9e8OSJdCkCaxYAS1a2P29elkcAweCKuTmQk6ObTMy4NtvoVs36NTJ7u/WDfLyIDkZGjSAjRuhQwdLo0cPSIm2juFeErfJa6qqIlJpx0uq+jTwNEB6ero7bnKceoYqbNkC7dtDaD22wkJYtgw++AAGD7ZCslkzK5DBCukvvrBCs0UL+Ooru2affSweVSvwd+6EHTtg8WIrcLt0gY8/tgI9JweysqwgLyiA/Hxo1Mji27kTfixt1Y4oNG5s8dUkDz8M48ZVf7yxVgqbRaSjqm4MzENbgvD1FF/WrksQ5jhOHWD3bisEZ8ywQvfoo+GHH6yW/P33VsDm50NqqtW0Z8yAefNg9WqrCR9+uNW2s7Ot1rxqldXSe/Wy+7dutTTKIzXV7u/Txwr3GTOsBg6WblqahffsCXPnwiefwCGHwNChdm/nzqaQGja0mviyZaYwmjWDNm2gb9+wAgGr/bdpY4onNRW6d4f33jOlM2KEXRe6Jy0Nli+Hffe1VkDDhnZP48a2bdYMDjwQ1q+3PO+3n+2HWiX5+abkNm82uQ4/vGbeZY16SRWR7sB/VPWA4PhBYKuq3ie2mH1rVb1ZRE4FfoutdHU48JiqHlZe/Onp6epuLhynahQWQlKJXsUff4RPPzWzRPPmsGuXFUa7dkHHjlYD37bNCumNG60mLwL/+5+ZTipK06ZW4+/Z02rlc+da/C1amJIYOhRWrrRf9+5W6HbsaAXtsmUmX3a2FZJJSVbwH3GEmV127LD4I1sZITkdQ0Rmq2p6tHM11lIQkVeBY4G2IrIOW+v1PmCiiFyKLUt4bnD5u5hCWA7sBC6pKbkcp65TsjAP2aZDtm+wgvX7781Esm0bLF0Ks2eb+WXRIivwly2zWm5urm1btYL58+3e8mjUyEwvqvY78ED47W8trFUrqynvsw907Wq1/dRUqxnv2GG15v32M/NOVejXr+zzzZoVPy6p+JyyqdPrKXhLwakvrFwJc+bAz35mNdqZM2HyZLjlFquVv/KK1ZiffNJMImeeaeHdu8OHH5qZ4bDD4KefrFa/cqXVwEuy//5w8MFmO+/Z08wfaWkWV1YWDBgAp51mBXZurtXIU1Ksk3P1ajN9DBxoYampsXxCTnVSVkvBlYLj7CVZWVZARiskf/wRJkyA774zW3bItjx6tNXUFy82+/fdd1vNvmdPq3lnZlq83bpZTXf1aouvRw9IT4ePPrJa+PLlcNJJFsdnn5mZJSXFjkP2+Pbt7fiAA8y84jhxMR85Tn1i5054/HEr0A8/3Grn2dlW8N52m9nCjzgCWrY0W/mXX9pomGXLrJBv3Tps4tmxAx55xOJNTrYO1MaN4YYbzLYOsGYNPP00PPCAmXv++18zixx1VNXNLo5TEbyl4CQMCxfC+PHwy1+aiWb7drOBv/UWvPmm1abz8qxGP2+eFfoiVjP/5BMbERNJSoqZajp3tlbCjh32y8kxJXHccXDQQXDCCTBkSPi+LVvMPNSypSmY1autwO/QgT1QtfiaNKm55+IkHm4+cuoFO3eGR5x88omZZHbtgunT4Q9/sAJ8zhwrkFetsoL6iSdsIlHLlmbGKW1YY9++NqmoRQsr2AcMMFNLbi5MnWqmlwcftLBvvrGO1f32s7R79bJwVTMXZWaajA0bxvLpOE7FcaXg1FpCI2dmzrTCtlWr8IzNpUttTPbkyWaK2bnTCvUhQ2wIZIiGDW3YZDQGDLD71qyByy6Dq66ylsG++5oCWLrUavTpUT8PI9rQTcepy3ifghNTdu2Cv/7VCt4jj4R33rGa9XHH2YSmuXOtYE5NtSGSixdHHykTokULOPtss+d/9pkphHvvhXPPtZZDhw7wxhtmYjn00HBNfcYMGDPGlMauXWGXAAceWLn8uEJwEglXCk6F2bnTbO6hGZghtm2D66+Hk0+2mZoTJ5ppJ5LkZBs+uWKFHaemWkGdlma1944d4cILLY2FC21i1IABVrh36RJOb/t2i2Pw4OLxX331nvL27RverwkfMY5TH3HzkROVrVttQtOcOTZu/Ygj4N13TSmIWCHcv791yL7zTvHZrEOGwJVXmtllxw7rTH3oIZt1OnKkdb4eeqilkZRkysJxnNjh5iOnVAoKrLM0J8dMPnPm2GiZV1+1GaqTJ9u5yZNtPPxJJ5n557HH7P7mzWHYMBvV88QTcOyxcN11e6bz0kt7hrVoUZM5cxynKnhLIcH44APzrnjvvfD11+a865137FzHjjY56ssvzSXwDz+Yvf7DD20b6bXyyy+tJbHffu5TxnHqGt5SSHDmzbOauohNmtq1y5QDWNjdd1vhfuqp1l8wZYqZi557zmbedu26Z5xHHRXTLDiOEyNcKdQjVGHaNHj/fbPj33uvddhu3WoduGCF/MUXm3K4/XYbf9+mTfF4Ro60bTQzkOM49RtXCvWAvDy49FKz++fnh4d37ref+bUvLIT777dWQNu2du7UU+Mnr+M4tRdXCnUM1bAN/403zLfOunXw8stwzjnmY2foUFtN6pFHbCav4zhORXGlUIcYN84WOXnjDfjTn8yNcogrrzSHbSF+8YuYi+c4TqzYts1qgDWAz9Wsxaiam4dHHzWF8OijNmKoSxdTCDfeCCeeaEM777gj3tI6jlMqqta5p2ozO0t6V1y0CC65xHyd79xpMzRLGxm6caMNE/zHP2pEVG8p1FJycuCUU8zxG5jJqF07ePZZUwwjR8Ixx1j/wbZtNlzUcZxK8tJL5o9l2DDYtMnGWqel2ceVn29T8W+6yT7A2283vyr9+tk47RdftI9vv/3gvvtsBueLL9p6pq1a2UpIAwbYwhhffQW//72thDR9uqX985/bSI+XXjLXuRkZ5iM9M9M+7P33tw971CizA//wg03pnzfPCojhw2vkkfg8hVrEsmW2zOHjj9tw0G3b7H90wgn2vy0stK3jOFHIzrYm88CBcMEFNpty5EgreMHGYm/ebMPt7r/fptPfcceePljACuG8PFvsOSPDwjp2tFr6UUfBrFnWRO/UyQrppk3NRS6Yd8WNG205vBCNGll6OTmWZl6edfrl5prPlt274de/hqeesoWou3a1CUQ//AALFuyZ12uuMdNBFXEvqbWcjAz7vxxwgP1HsrLg+OPhiius89hx6iyTJ1vN+IEHqnb/ypU2vf6bb+Dyy61wHTLE1gddvBguushMLkOGmOOtkNOtHj3MRNOsmRWeHTvC2LFWI09NtQ8NbGm74cOtBjZsmMWTmWlppqTY7M527czN7o4dthJSyPvip5+aXf/11+26ww6zuPr2tRrc119bTW/CBPO/PnOmXd+jh6WdmWlmoyOOKN3roqrFkZJiQweTky2u448vvih3JXGlUIuZONGGk4rYfy4lxVqMS5ZYK9Zxaj2zZ1sNeOjQPc8dfLAV6F9/bYVmSTIy4C9/sYJu925bnGLrViugP/jAzC5gtfLsbNsfNMg+mgcesML9xBPNztqsmS16sXAh3HyzeWjMzAzH0bs3XHutXXvSSVazHzLEHHGVxs6dVjB/+qldf+65dqxacfe5OTmm3AYMqNj1McCVQi2ksNBMlQ89BIccYhWcvn3NvNiwoZkpHafGWb68uBva0rjrLjOxjB1r9u5rrrEC94ILrLa+fbutH3r++WY6uf12q9l8843d37Gj1ZJHjjS3uCkpVusfPtxq/EcfbTWjFSvs3PLl5jnxggvgvPNMKbz1limOu++2mnv37lZrP/jg4rKGZnGmp1uN/n//M0UzcqSP0Q4oSymgqnX2d8ghh2hdJC9P9cILrbrx29+q5uerLl6sum5dvCVz6jU7d9q2sFD1X/9Sfe891eRk1SOOUN22TfWRR1Rnz1adN081PV31+utVFyywn4hqaqrq+eer9uplf96mTW2bmqp6zDG236pVqB4d/v32t6r77ad64omWXig8Lc3unTKluJy7d6tu2lR6PnbtUp0717ZOlQBmaSnlqrcUYkxmJvzsZ9Za/uMf4Xe/c4dyTgV4/31bc/R//7NOzUi2bbMa+bBhZu547jmzvzdrFr5m7lzrIP35z230zI03WniLFnZPy5ZmymnRwkwyKSnhjtNGjaxjNC3NOmtHjLCRNRdcAK+8YotUjxhhk2XWrLE/eL9+8Pnn1gR+9dWwHPPmWS1+2zZrFdx6q42ycWKKtxRqCStWqPbrp5qSovrPf8ZbGqdWs2qV6qOPqv70k2pBgf1xQPWtt+x8To7V7E87zf5QoNq9u+pBB9n+7bdby+CGG1SPPdZq6o0bh2vpRx2lOmCA6iuvWIuhcWPVM85Q7dxZ9ZRTVDdvVn31VdU771RNSrKWwPz5qkuXxvGhONUFZbQU4l6w782vLimFl19Wbd3aWteffBJvaZwqkZNjpo3KsGWL6vr1pZ/fvl112TLVNWtU337blMCSJaqjR9vn2b+/6p//HC7Mr7jC7rvpprAiuOEG1ZdesoK7USNTIC1amNkGVAcPNrPNP/+pOmOG6jPPqG7dWlyOrCwzKxUU7Cnj9Omq339fuXw7tZpapxSAa4EFwHfAuCCsNfARsCzYtiovntquFAoLrbJ3+eXhyplXtOooO3eqdupkBbSq6osvqr7zjr3krCzVH38sfn1urupXX1nNu2FDK6BXrLBzzz6resEFqu++q3rkkWZbP+QQ+5P06WMFeHKy6kkn2b2gOny46umn237z5tY6GDt2Tzl371b95pvwfeedZ+F5eTX1ZJw6SK1SCsABgUJogs2o/hjoDTwA3Bpccytwf3lx1XalMHFiuIJ33nnWoezEgWnTrHN0zpxwWFaWmU6ys+24oED1nntUR4xQnTnTwjIyrNBXVX39dXuRhx9u94Ve7EUXqXbrZh2m7durjhypunBh2IzTvLkpgIYNVXv3Vv3gAzPHhArtyF9amm27dLHt0qWmfI44QnXjRqvpg3XYDhyounZt6XleskT1ttvMDOQ4JahtSuEc4LmI498DNwNLgI5BWEdgSXlx1Wal8J//qLZtq3rwwao7dsRbmjpOaSabuXNVx40z+/utt0YvJPPywvb4446zQn76dNWWLbXIPLNkieprr9mxiBX0P/yg2qyZ6pgxds8pp9j5pCQLHzpU9eqrw/dceqnV3Js3t2vAWhXLlpkc48dr0eicfv1UMzNVH3zQ5P7lL1XbtbN8fPaZFeTRbIyFhWaOcpy9pLYphf7AUqBN0FqYDvwNyIq4RiKPS9x/OTALmNWtW7cae2hVJT9f9Y477MkecIDqokXxliiOFBaqXnut6nPPhcNWrCi7hhvi3XdV/+//rODt1s2GRZYkVFCfcIJtmzRR/e9/7Vx2ttnubrvNzp19tm0nTzZt3auX6vPPq7ZpY/b3rl2tsL7oIivYn302XIMfMsS2w4aFw+bPNxPRsGGqt9wSlmnNGtXjj1f91a+Ky5qZGe4Qnjix+Lm8vD1t/I5Tg9QqpWDycCkwG5gGPAk8UlIJAJnlxVPbWgo7d4atBhddZGVGQlBQoDpqlNWGIwmZOxo1Ul250sIGDLAx8KNHqx54oOoTT1gNOcSUKaoffWQaNVQAN2hghXfI/PPuu1YYi4Sv6drVmmUNGpji+d3vwudGj7ZCOSnJOmbBxuOrqq5erXrqqVZgT5xocYfu22cf1T/9yeQ/+2xTNE2a2CidqnDuuZb/aJ25jhNDap1SKCYA/Bm4sj6YjyZMsCc6fnzYFF3nyctTveqq4gV3Sd56K1z433OPFcqvvGLDHAcPtoL0/PNtdl6kDT00RLJzZ9Ogd98dPte7t21HjLBWQrduZnpZulT16KPD9++zj+3feae1QESs5t62raV9xhnhWYHp6XZtt257vqDQcV6e3Qc2w1DV7H+h87Nnm2mpKuTm7tkh7ThxoNYpBaB9sO0GLAZaAg+W6Gh+oLx4apNS2LDBypy+feuRQlBV/fRT+5vccYd1oIZmkebkqD71lNnMhw+3kTldu4YL+eRkq81v3GjDJ5OSihf6qak2a/XNN+34rruKj6MPjbUPsWKFtRZCNf0//MEe+mWX2fG0aXbdiBFhm/7HHxfPy803W/jll5ed56ws1euus2nmjlMPqY1K4TNgITAPGB6EtQGmBENSPwZalxdPbVEKU6aEB5P84x/xlqYcCgrMTHLvvdGHQ23YoLr//qYMMjJU//IXy1jPnlbY/v73NjqmSZNw4Q52XV6emX5EVHv0CI/sWbcubE9PSzMF8MADdq6wMGyzFzE7f0gpvPRScdm+/NIme0DYHPX112YeCg25fOMNMyHdf/+e2vnjj+3et9+utsfpOHWRWqcUqutXG5TCokU2kGX//cNm6lrFjBmqZ51lo1nOOMNGyIQK3UmT9rw+NPQy9GvfvvhxqBY+dqzZyfr3txE0kQXwRx+FC+0QTz1lHbqXXrpnmhkZNgHrj38080qoryDaA121SvX998vOc05O9PDQyKN61ZRznMpTllJw30d7wU8/mSPHHTvMM3D37nETJSzQhAnmjyYjw1wa33yzrd0Z8l8DcNlltoDH0KEwaVLxOO680zxiQnjhkJDb4iZNzE9Oerr5hq8sof9aec6e+vc3z5k7duzp58dxnL2mLN9HvkbzXvDGG+bT68UXa1AhXHqprd1aGq+8AqNH29KBo0aZW+JNm2DpUnNENnWqXZeXB//3f/Z75BHzC//vf5vf7jZtbCGRM84wZ2V9+tj1991n915xhfmif+45c7N8001Vy4tIxbz/DRlifsRdIThOzPGWwl5wwgmmFFasqCFPpzk5VjuHcC27JEceaYuIHH64NVeefRZ++UtrwmzZYssP3nCDFfS/+pWt3ATmvfKCC8zPfc+eplSee87OnXUWvPmmtRKuucbWBA2tFhULduywVs8++8QuTcdJIMpqKTSItTD1hY8+gilTzJtxpRXCqaeaRhk3ruzrPvwwvF9YuOdKTxkZpgjAttdfby0LsNr8RRfZ/pgxe64uNWCAuTGOZPlyW2EqtKZt06YwfnyFs1VtNGtW3O2z4zgxw5VCJfnpJ7j4YqtIH3igWWsqxe7dVtjn55euFPLzzcTz1FPhsB9+sHU6P/jAFg2/+GI7rwoPP2wtgj/+MXz9hRdaX8Bnn+25MlVpXHxxcaXgOE7C4eajSvL3v8PVV9saJb//PTRvXskINm6ETp1sYZJ16+w4OdkKfID1661vYPZsMw316mVrdH76qZlThg61FkKI7t3NflXR9WLLIj8fnnnGWhvlLc/oOE6dxddoriZUrRLdpEnVBt8AdmNoAfOsLFubdv58GzV0wgnWyZqRYcdnnw0zZlh/Qbt2YWVw//22OtbIkSaQd8g6jlMJvE+hmpg2DRYu3Esz+7p14f1Fi0whALzwAkycaJ3DU6eG+wA6d7ZtpEK4+ea9EMBxHKd0XClUgieesKVsx4ypws1HHw2/+EV4rgDA9Onh/TVrrMPivPOKdwp36GCmocJC6zsor3PacRxnL3ClUEE2brTO5auvDo8SrTDZ2dbhm5xspqOGDW3I0kcf2fnu3U0p7N5tfQiRNGhgfQkbNuw5gshxHKea8clrFeS556CgwOZxVZpVq2z71VfWKdylC/TrZ53HAEcdZQoBbM5ASTp3NoUyeHCVZHccx6korhQqQEEB/OMf1g+8335B4OrV4QllhYXmGiLUXzB9uk3AIuJasM7ht94ypdC/v7mMABtlFCKaUjjwQLum0k0Ux3GcyuFKoQI8+aSV91deGQQsWmQzfB96yI5XrDCfQa+/bhPCjjoKbr89HEGopQDWIujSBfbfPxxWnlJ44gl4//3qyo7jOE6puFIoh1WrbLDPKafY9AHAJopBeHLZhg223bgRHn3U9rduDUeyerXN0L3tNlMYP/uZtRQAWrc2Pz9gkx5at95TiEaNIC2tOrPlOI4TFe9oLodXXjGrz5NPRriz2L7dtsuX23bjRtsuXWozjsFu+vRTm1+wapW1LP7853DECxfatlMnm2fQujV07VpDTpQcx3EqhiuFcpg8GY44Arp1iwjMzAzvZ2SEWwqffWazgsEUxLHHhq87/fTiEffubSOLQvMQ0tNh332rW3zHcZxK4UqhDL7/3rxN3H9/iRORSmHy5LBSyMqy7QEHwIIFtt+hg5mb2rUrHkfDhnDyyaZxAP7zH28lOI4Td1wplEFoKYJTTy1xIlT4H3WUdTgcdFDx88OGhZXC++/DkiXhwj+Sd94J76ekVIfIjuM4e4UrhTKYMcP6h0N9wkVkZpoH0hdesDGq06aFz7Vta3MQQvTuDYMGxUBax3GcvcdHH5XBjBk2iXgPB6SZmebvok+f8NDSBoF+7dXLOo/BZiK7szrHceoQrhRKITfXphyEHJoWIzMTWrWy/eOOs22oOdG7d1gp9OlT43I6juNUJ64USmHuXNi1qwJK4ZBDbBua3dyrly1xCa4UHMepc7hSKIX//tfMRkOHRjmZlWXmI4DTTjMF8fDDtubxWWdZS6FlSxtm6jiOU4fwjuYoqMK//mXTDEqOJAWKtxTatYNt22x/xIjwNatXe3+C4zh1Dm8pRGHhQhtFOnp0EJCXZwvg5ObacaRSKI0WLcyzqeM4Th0iLkpBRK4Tke9EZIGIvCoiqSLSQ0S+FpHlIvK6iDSMh2wA775r2yJfR+PG2co6t9xiCiInp3yl4DiOUweJuVIQkc7ANUC6qh4AJAPnAfcDD6tqbyATuDTWsoX4+GMYMCAYRPTxx+b4rnt3eOwxmDLFLgr1KTiO49Qj4mU+agA0FpEGQBNgI3A8MCk4PwE4Mx6C5ebaXLQRI7DOhTvvNFfX06ebG4qXX7YLvaXgOE49JOZKQVXXA38BvseUQTYwG8hS1YLgsnVA52j3i8jlIjJLRGZlhBazr0a+/NIUw4gRwKxZ8MUXZjbaZx9zZzFxol0YuR6C4zhOPSEe5qNWwCigB9AJSANOquj9qvq0qqaranq7qEOD9o7PPoPG5HD8iqfh228t8OSTbXvccbYM24EH7unvyHEcpx4QD/PRCGCVqmao6i7gTWAI0DIwJwF0AdbHQTa++AKu6vZvmoz7Nbz6qpmMuna1k6HZyxdf7B5NHcepl8RDKXwPHCEiTUREgOHAQmAqEBoEOhZ4O9aCFRRY18Fh3TZZwOef2+zkhsFAqJNPhscfh1//OtaiOY7jxIR49Cl8jXUozwG+DWR4GrgFuF5ElgNtgOdiLdu338KPP0L/NsFym7m5xVfXadDAFmr2pTEdx6mnxGVGs6reAdxRInglEM3TUMyYM8e2XRtuDgf6amiO4yQQPqM5gpUrbRJys52uFBzHSUxcKUSwcqXpgKQtrhQcx0lMXClEsGoV9OiBrakcWlknsk/BcRynnuNKIYKVK6FnD4UtW+CUU6Bv3/B6CY7jOAmAK4WAH3+EjAzo13mHjTo65hhYvDi8YI7jOE4C4EohYNUq2/ZtGfQndOgQP2Ecx3HiRLlKQUROF5F6rzxWrrRtzzRXCo7jJC4VKezHAMtE5AER6VfTAsWLUEuhiwTeNVwpOI6TgJSrFFT1F8BgYAXwgohMDzyVNqtx6WLIypXQrBk0nTPNZiz37x9vkRzHcWJOhcxCqrodc03xGtAROAuYIyJX16BsMWXlSujZE+Tjj6yTuWHcFn5zHMeJGxXpUzhDRCYDnwApwGGqejJwEHBDzYoXOzKWZ3PDrvtg2TI44YR4i+M4jhMXKuL76GfYMpnTIgNVdaeIxG3JzOpEFQ5a8SYXFtxmAaH1ExzHcRKMiiiFO7EV0gAQkcZAB1VdrapTakqwWLJpEzQv2GoHc+fapDXHcZwEpCJ9Cv8CCiOOdwdh9YZVq6AlWWhSkq2q5jiOk6BURCk0UNX80EGwX696YVeuNKVQ2Kxl2OeR4zhOAlKREjBDRM4IHYjIKOCHmhMp9qxfD63IRFq3jLcojuM4caUifQpXAC+LyN8BAdYCF9WoVDFmwwYY1CCLpFYt4y2K4zhOXClXKajqCmxN5abB8Y81LlWM2bAB2qVkQatW8RbFcRwnrlRoOU4RORUYAKSKCACqencNyhVTNmyA1klZ0NI9ojqOk9hUZPLaU5j/o6sx89E5QL1ajmzDBmhRmAktW8ZbFMdxnLhSkY7mo1T1IiBTVe8CjgT2q1mxYoeqKYW0XVluPnIcJ+GpiFLIDbY7RaQTsAvzf1Qv2LYNyM+jYUGOtxQcx0l4KtKn8G8RaQk8CMwBFHimJoWKJRs2QAuy7cCVguM4CU6ZSiFYXGeKqmYBb4jIf4BUVc2OhXCxYMMGm6MAuFJwHCfhKdN8pKqFwOMRx3n1SSGAKYWWZNmB9yk4jpPgVKRPYYqI/ExCY1H3EhHpKyJzI37bRWSciLQWkY9EZFmwjUkJvW1bhFLwloLjOAlORZTCrzEHeHlBAb5DRLZXNUFVXaKqg1R1EHAIsBOYDNyKmar6AFOC4xonOxtau/nIcRwHqNhynM1UNUlVG6pq8+C4eTWlPxxYoaprgFHAhCB8AnBmNaVRJtnZ0Cl1mx20bh2LJB3HcWot5Y4+EpGjo4WXXHSnipwHvBrsd1DV0LoNm4AOpchzOXA5QLdu3fZagOxsGNgwwwbetmmz1/E5juPUZSoyJPWmiP1U4DBgNnD83iQsIg2BM4DbSp5TVRURjXafqj4NPA2Qnp4e9ZrKsH07dGyQYZ3MKSl7G53jOE6dpiIO8U6PPBaRrsAj1ZD2ycAcVd0cHG8WkY6qulFEOgJbqiGNcsnOhvayBdq1j0VyjuM4tZqqrCizDuhfDWmfT9h0BPAOMDbYHwu8XQ1plEt2NrTVDGjXLhbJOY7j1Goq0qfwN2wWM5gSGYTNbK4yIpIGnICNbApxHzBRRC4F1gDn7k0aFSU7G1rvzoB29cadk+M4TpWpSJ/CrIj9AuBVVf1ibxJV1Z+ANiXCtmKjkWLK9u3QIj8D2g2JddKO4zi1jooohUlArqruBhCRZBFpoqo7a1a02LA9q5Cmu36A9t6n4DiOU6EZzUDjiOPGwMc1I05sycuDtPxtJGmh9yk4juNQMaWQGrkEZ7DfpOZEih3Z2dCODDtwpeA4jlMhpfCTiBwcOhCRQ4CcmhMpdmzfDu1DI1/dfOQ4jlOhPoVxwL9EZAO2HOc+2PKcdR5vKTiO4xSnIpPXZopIP6BvELREVXfVrFixITsbOhDMnfOWguM4TvnmIxG5CkhT1QWqugBoKiJX1rxoNU92NnTjewpTGrpScBzHoWJ9CpcFK68BoKqZwGU1JlEM2b7dlMLujl0hqSqTux3HceoXFSkJkyMX2BGRZKBhzYkUO7Zvh31Zg+67b7xFcRzHqRVURCm8D7wuIsNFZDjmr+i9mhUrNvz0kymFpH333gW34zhOfaAio49uwdYvuCI4no+NQKrz5G7PpyMbkZ7eUnAcx4GKrbxWCHwNrMbWUjgeWFSzYsWGlM3rSEIRbyk4juMAZbQURGQ/zL31+cAPwOsAqnpcbESreRpnfG873qfgOI4DlG0+Wgx8BpymqssBROS6mEgVI5puXWM71bCsp+M4Tn2gLPPR2cBGYKqIPBN0MksZ19c5UrcHLi72qRddJI7jOHtNqUpBVd9S1fOAfsBUzN1FexF5UkRGxki+GiVlZza7SYKmTeMtiuM4Tq2gIh3NP6nqK8FazV2Ab7ARSXWeRrnZ7GzQHKReNYAcx3GqTKWm8apqpqo+raoxXyGtJmiUm83OlBbxFsNxHKfWkNC+HRrnZ5PT0JWC4zhOiMRWCru2k9fIlYLjOE6IhFYKaQXZ5Dd2peA4jhMioZVC08JsdjVpHm8xHMdxag0JqxQKC6G5ZlOQ5i0Fx3GcEAmrFHJ2Ki3IprCpKwXHcZwQCasUdm7NIYUCCpu5UnAcxwkRF6UgIi1FZJKILBaRRSJypIi0FpGPRGRZsG1VkzLkbs62nZauFBzHcULEq6XwKPC+qvYDDsJccd8KTFHVPsCU4LjGyNtiSkFauFJwHMcJEXOlICItgKOB5wBUNT9YA3oUMCG4bAJwZk3KkZ9hSiG5tSsFx3GcEPFoKfQAMoDnReQbEXlWRNKADqq6MbhmE9Ah2s0icrmIzBKRWRkZGVUWomCrKwXHcZySxEMpNAAOBp5U1cHAT5QwFamqAhrt5sD3Urqqprdr167KQoSUQkpbVwqO4zgh4qEU1gHrVPXr4HgSpiQ2i0hHgGC7pSaFKMwypdCwnSsFx3GcEDFXCqq6CVgrIn2DoOHAQuAdYGwQNhZ4u0blyN4BQGq7ZjWZjOM4Tp2irOU4a5KrgZdFpCGwErgEU1ATReRSYA1wbk0KUJCTD0Bqy9SaTMZxHKdOEReloKpzgfQop2K2TkNhrimFxi0axipJx3GcWk/CzmiW/HwKEVJSk+MtiuM4Tq0hYZUCu/LJpyEN4mVAcxzHqYUkrFKQfFcKjuM4JUlcpVBgSiEpYZ+A4zjOniRskSi7dpGPdzI7juNEkrBKIakgn13iSsFxHCeShFUK4krBcRxnDxJWKXhLwXEcZ08SWinslpR4i+E4jlOrSFilkFyQz64kbyk4juNEkrBKIWl3PgWuFBzHcYqRsEoh2ZWC4zjOHiSsUkjavcuVguM4TgkSVik02J3PblcKjuM4xUhYpZBcmE9BsisFx3GcSBJWKTTYnc9uVwqO4zjFSFilkFyYT2GSz1NwHMeJJGGVQkphPrsbeEvBcRwnkoRVCg00n0I3HzmO4xQjYZVCcuEuCr2l4DiOU4yEVQop6h3NjuM4JUlMpaBKiuajKa4UHMdxIklMpbB7N0mom48cx3FKkJhKIT8fgEJvKTiO4xQjoZWCNvB5Co7jOJE0iEeiIrIa2AHsBgpUNV1EWgOvA92B1cC5qppZIwIESgFvKTiO4xQjni2F41R1kKqmB8e3AlNUtQ8wJTiuGdx85DiOE5XaZD4aBUwI9icAZ9ZYSrt22daVguM4TjHipRQU+FBEZovI5UFYB1XdGOxvAjrUWOqhPgVXCo7jOMWIS58CMFRV14tIe+AjEVkceVJVVUQ02o2BErkcoFu3blVLPdSn0NCVguPUJnbt2sW6devIzc2Ntyj1gtTUVLp06UJKSsUH1cRFKajq+mC7RUQmA4cBm0Wko6puFJGOwJZS7n0aeBogPT09quIoF1cKjlMrWbduHc2aNaN79+6ISLzFqdOoKlu3bmXdunX06NGjwvfF3HwkImki0iy0D4wEFgDvAGODy8YCb9eUDLtzTClII1cKjlObyM3NpU2bNq4QqgERoU2bNpVudcWjpdABmBy89AbAK6r6vojMBCaKyKXAGuDcmhJgd04+yQCVaFI5jhMbXCFUH1V5ljFXCqq6EjgoSvhWYHgsZPCWguM4TnRq05DUmBFSCt6n4DhOJFlZWTzxxBOVvu+UU04hKyur+gWKA4mpFHJtnkJSqisFx3HClKYUCgoKyrzv3XffpWXLljUkVWyJ15DUuFKY6+Yjx6ntjBsHc+dWb5yDBsEjj5R+/tZbb2XFihUMGjSIlJQUUlNTadWqFYsXL2bp0qWceeaZrF27ltzcXK699louv9ymWXXv3p1Zs2bx448/cvLJJzN06FC+/PJLOnfuzNtvv03jxo2rNyM1SGK2FLxPwXGcKNx333306tWLuXPn8uCDDzJnzhweffRRli5dCsD48eOZPXs2s2bN4rHHHmPr1q17xLFs2TKuuuoqvvvuO1q2bMkbb7wR62zsFQnZUtA8UwrJjV0pOE5tpawafaw47LDDio3xf+yxx5g8eTIAa9euZdmyZbRp06bYPT169GDQoEEAHHLIIaxevTpW4lYLCakUdofMRw19SKrjOKWTlpZWtP/JJ5/w8ccfM336dJo0acKxxx4bdQ5Ao0aNivaTk5PJycmJiazVRUKajzTXWwqO4+xJs2bN2LFjR9Rz2dnZtGrViiZNmrB48WK++uqrGEsXGxKypVAYmI989JHjOJG0adOGIUOGcMABB9C4cWM6dAj75TzppJN46qmn6N+/P3379uWII46Io6Q1R0IqhR19D+UBbmJAk9R4i+I4Ti3jlVdeiRreqFEj3nvvvajnQv0Gbdu2ZcGCBUXhN954Y7XLV9MkpPkoc+Ax3MIDJKd6n4LjOE4kCakUQvNQ3PWR4zhOcRJSKYQWXmuQkMYzx3Gc0klIpeAtBcdxnOgkpFLwloLjOE50ElIpeEvBcRwnOgmpFLyl4DhOddC0aVMANmzYwOjRo6Nec+yxxzJr1qwy43nkkUfYuXNn0XE8XXEnpFLwloLjONVJp06dmDRpUpXvL6kU4umKOyHryt5ScJw6QBx8Z99666107dqVq666CoA777yTBg0aMHXqVDIzM9m1axf33HMPo0aNKnbf6tWrOe2001iwYAE5OTlccsklzJs3j379+hXzffSb3/yGmTNnkpOTw+jRo7nrrrt47LHH2LBhA8cddxxt27Zl6tSpRa6427Zty0MPPcT48eMB+NWvfsW4ceNYvXp1jbno9paC4zhOwJgxY5g4cWLR8cSJExk7diyTJ09mzpw5TJ06lRtuuAFVLTWOJ598kiZNmrBo0SLuuusuZs+eXXTuT3/6E7NmzWL+/Pl8+umnzJ8/n2uuuYZOnToxdepUpk6dWiyu2bNn8/zzz/P111/z1Vdf8cwzz/DNN98ANeeiOyHryt5ScJw6QBx8Zw8ePJgtW7awYcMGMjIyaNWqFfvssw/XXXcd06ZNIykpifXr17N582b22WefqHFMmzaNa665BoCBAwcycODAonMTJ07k6aefpqCggI0bN7Jw4cJi50vy+eefc9ZZZxV5az377LP57LPPOOOMM2rMRXdCFoshpeAtBcdxSnLOOecwadIkNm3axJgxY3j55ZfJyMhg9uzZpKSk0L1796gus8tj1apV/OUvf2HmzJm0atWKiy++uErxhKgpF90JbT7yloLjOCUZM2YMr732GpMmTeKcc84hOzub9u3bk5KSwtSpU1mzZk2Z9x999NFFTvUWLFjA/PnzAdi+fTtpaWm0aNGCzZs3F3OuV5rL7mHDhvHWW2+xc+dOfvrpJyZPnsywYcOqMbd7kpDForcUHMcpjQEDBrBjxw46d+5Mx44d+fnPf87pp5/OgQceSHp6Ov369Svz/t/85jdccskl9O/fn/79+3PIIYcAcNBBBzF48GD69etH165dGTJkSNE9l19+OSeddFJR30KIgw8+mIsvvpjDDjsMsI7mwYMH1+hqblJWh0ltJz09Xcsb/xuNt9+Gl16yX0QLzHGcOLNo0SL69+8fbzHqFdGeqYjMVtX0aNcnZEth1Cj7OY7jOMWJW5+CiCSLyDci8p/guIeIfC0iy0XkdRHxZdEcx3FiTDw7mq8FFkUc3w88rKq9gUzg0rhI5ThOXKnLJu3aRlWeZVyUgoh0AU4Fng2OBTgeCM0TnwCcGQ/ZHMeJH6mpqWzdutUVQzWgqmzdupXU1MotOxyvPoVHgJuBZsFxGyBLVYPBoqwDOsdBLsdx4kiXLl1Yt24dGRkZ8RalXpCamkqXLl0qdU/MlYKInAZsUdXZInJsFe6/HLgcoFu3btUrnOM4cSUlJYUePXrEW4yEJh7moyHAGSKyGngNMxs9CrQUkZCS6gKsj3azqj6tqumqmt6uXbtYyOs4jpMwxFwpqOptqtpFVbsD5wH/U9WfA1OBkEPyscDbsZbNcRwn0alNbi5uAa4XkeVYH8NzcZbHcRwn4ajTM5pFJAMo2xFJ6bQFfqhGceKJ56V24nmpnXheYF9VjWp/r9NKYW8QkVmlTfOua3heaieel9qJ56VsapP5yHEcx4kzrhQcx3GcIhJZKTwdbwGqEc9L7cTzUjvxvJRBwvYpOI7jOHuSyC0Fx3EcpwSuFBzHcZwiElIpiMhJIrIkWLvh1njLU1lEZLWIfCsic0VkVhDWWkQ+EpFlwbZVvOWMhoiMF5EtIrIgIiyq7GI8Fryn+SJycPwk35NS8nKniKwP3s1cETkl4txtQV6WiMiJ8ZF6T0Skq4hMFZGFIvKdiFwbhNe591JGXurie0kVkRkiMi/Iy11BeNS1Z0SkUXC8PDjfvUoJq2pC/YBkYAXQE2gIzAP2j7dclczDaqBtibAHgFuD/VuB++MtZymyHw0cDCwoT3bgFOA9QIAjgK/jLX8F8nIncGOUa/cP/muNgB7BfzA53nkIZOsIHBzsNwOWBvLWufdSRl7q4nsRoGmwnwJ8HTzvicB5QfhTwG+C/SuBp4L984DXq5JuIrYUDgOWq+pKVc3HnPLVh8U5R2HrUEAtXo9CVacB20oElyb7KOCfanyFOU3sGBNBK0ApeSmNUcBrqpqnqquA5dh/Me6o6kZVnRPs78AWv+pMHXwvZeSlNGrze1FV/TE4TAl+Sulrz0S+r0nA8GCtmkqRiEqhM7A24rgurt2gwIciMjtwJQ7QQVU3BvubgA7xEa1KlCZ7XX1Xvw3MKuMjzHh1Ii+ByWEwViut0++lRF6gDr4XsWWL5wJbgI+wlkyWRl97pigvwflszI9cpUhEpVAfGKqqBwMnA1eJyNGRJ9Xaj3VyrHFdlj3gSaAXMAjYCPw1rtJUAhFpCrwBjFPV7ZHn6tp7iZKXOvleVHW3qg7ClhM4DOhX02kmolJYD3SNOC517YbaiqquD7ZbgMnYn2VzqAkfbLfET8JKU5rsde5dqerm4EMuBJ4hbIqo1XkRkRSsEH1ZVd8Mguvke4mWl7r6XkKoaha2vMCRlL72TFFegvMtgK2VTSsRlcJMoE/Qg98Q65B5J84yVRgRSRORZqF9YCSwAMvD2OCyurYeRWmyvwNcFIx2OQLIjjBn1EpK2NbPwt4NWF7OC0aI9AD6ADNiLV80Arvzc8AiVX0o4lSdey+l5aWOvpd2ItIy2G8MnID1kZS29kzk+xqNrVVT+dZdvHvY4/HDRk8sxexzv4u3PJWUvSc2WmIe8F1Ifsx2OAVYBnwMtI63rKXI/yrWfN+F2UMvLU12bPTF48F7+hZIj7f8FcjLi4Gs84OPtGPE9b8L8rIEODne8kfINRQzDc0H5ga/U+rieykjL3XxvQwEvglkXgD8IQjviSmu5cC/gEZBeGpwvDw437Mq6bqbC8dxHKeIRDQfOY7jOKXgSsFxHMcpwpWC4ziOU4QrBcdxHKcIVwqO4zhOEa4UnDqBiKiI/DXi+EYRubOa4n5BREaXf+Vep3OOiCwSkak1nVaJdC8Wkb/HMk2n7uJKwakr5AFni0jbeAsSScTM0opwKXCZqh5XU/I4zt7iSsGpKxRg69FeV/JEyZq+iPwYbI8VkU9F5G0RWSki94nIzwMf9d+KSK+IaEaIyCwRWSoipwX3J4vIgyIyM3Ck9uuIeD8TkXeAhVHkOT+If4GI3B+E/QGbWPWciDwY5Z6bItIJ+c3vLiKLReTloIUxSUSaBOeGi8g3QTrjRaRREH6oiHwp5oN/Rmj2O9BJRN4XWxvhgYj8vRDI+a2I7PFsncSjMrUcx4k3jwPzQ4VaBTkI6I+5uF4JPKuqh4ktvnI1MC64rjvmD6cXMFVEegMXYS4cDg0K3S9E5MPg+oOBA9TcLRchIp2A+4FDgEzMm+2Zqnq3iByP+fSfVeKekZh7hcOw2cLvBE4Ovwf6Apeq6hciMh64MjAFvQAMV9WlIvJP4Dci8gTwOjBGVWeKSHMgJ0hmEOYxNA9YIiJ/A9oDnVX1gECOlpV4rk49xVsKTp1BzdvlP4FrKnHbTDUf+3mYK4NQof4tpghCTFTVQlVdhimPfphfqYvEXBd/jbl96BNcP6OkQgg4FPhEVTPU3Be/jC3GUxYjg983wJwg7VA6a1X1i2D/Jay10RdYpapLg/AJQRp9gY2qOhPseWnYxfIUVc1W1VysdbNvkM+eIvI3ETkJKOYZ1UlMvKXg1DUewQrO5yPCCggqOCKShK2oFyIvYr8w4riQ4v//kv5eFKu1X62qH0SeEJFjgZ+qInwpCHCvqv6jRDrdS5GrKkQ+h91AA1XNFJGDgBOBK4BzgV9WMX6nnuAtBadOoarbsOUIL40IXo2ZawDOwFaoqizniEhS0M/QE3OO9gFmlkkBEJH9As+0ZTEDOEZE2opIMnA+8Gk593wA/FJsDQBEpLOItA/OdRORI4P9C4DPA9m6ByYugAuDNJYAHUXk0CCeZmV1hAed9kmq+gZwO2YScxIcbyk4dZG/Ar+NOH4GeFtE5gHvU7Va/PdYgd4cuEJVc0XkWczENCdwyZxBOcucqupGEbkVc28swH9VtUw35qr6oYj0B6ZbMvwI/AKr0S/BFlIaj5l9ngxkuwT4V1Doz8TW5s0XkTHA3wJXyznAiDKS7gw8H7SuAG4rS04nMXAvqY5TSwnMR/8JdQQ7Tixw85HjOI5ThLcUHMdxnCK8peA4juMU4UrBcRzHKcKVguM4jlOEKwXHcRynCFcKjuM4ThH/D+ehlsZZr1TYAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_acc_ls)), train_acc_ls, 'b')\n",
    "plt.plot(range(len(valid_acc_ls)), valid_acc_ls, 'r')\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Logistic Regression: Accuracy vs Number of epochs\")\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22308227622622176, 93.65)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loss_ls[-1],valid_acc_ls[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19186406899243594, 95.05)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "690c9ea092c8a6fc9517542155c4d05fadb9e10c4733225e6f103cd30826cc12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

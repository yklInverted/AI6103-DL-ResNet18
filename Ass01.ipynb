{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#torch.manual_seed(0)\n",
    "\n",
    "# these are commonly used data augmentations\n",
    "# random cropping and random horizontal flip\n",
    "# lastay, we normalize each channel into zero mean and unit standard deviation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.45), ratio=(0.33, 3), value=(0.4914, 0.4822, 0.4465), inplace=False),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "  \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "tempset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "trainset, valset = data.random_split(tempset, [40000, 10000], generator= torch.Generator().manual_seed(0))\n",
    "\n",
    "valset.dataset = copy(tempset)\n",
    "valset.dataset.transform = transform_test\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    valset, batch_size=128, shuffle=True, num_workers=2)\n",
    "    \n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=False, transform=transform_test)\n",
    "\n",
    "# we can use a larger batch size during test, because we do not save \n",
    "# intermediate variables for gradient computation, which leaves more memory\n",
    "testaoader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21507a40130>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(valset[4][0].permute((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deer'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_indices, val_indices = train_test_split(list(range(len(tempset.targets))), test_size=0.2, random_state = 0, stratify=tempset.targets)\n",
    "#trainset = torch.utils.data.Subset(tempset, train_indices)\n",
    "#valset = torch.utils.data.Subset(tempset, val_indices)\n",
    "classes[valset[278][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_temp = np.zeros(len(classes))\n",
    "count_train = np.zeros(len(classes))\n",
    "\n",
    "for im, label in trainset:\n",
    "    count_temp[label] += 1\n",
    "    count_train[label] += 1\n",
    "\n",
    "for im, label in valset:\n",
    "    count_temp[label] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.bar(classes, count_temp, color = 'r', label = 'lala')\n",
    "plt.bar(classes, count_train, color = 'b', label = 'haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch, net, criterion, trainloader, scheduler, optimizer):\n",
    "    device = 'cuda'\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if (batch_idx+1) % 50 == 0:\n",
    "          print(\"iteration : %3d, loss : %0.4f, loss : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
    "\n",
    "    scheduler.step()\n",
    "    return train_loss/(batch_idx+1), 100.*correct/total\n",
    "\n",
    "def test(epoch, net, criterion, testaoader):\n",
    "    device = 'cuda'\n",
    "    net.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testaoader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return valid_loss/(batch_idx+1), 100.*correct/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(net, acc, epoch):\n",
    "    # Save checkpoint.\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/ckpt.pth')\n",
    "\n",
    "def save_result(name, train_loss, valid_loss, test_loss, train_acc, valid_acc, test_acc):\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'ta': train_loss,\n",
    "        'va': valid_loss,\n",
    "        'fl': test_loss,\n",
    "        'ta': train_acc,  \n",
    "        'va': valid_acc,  \n",
    "        'fa': test_acc,\n",
    "    }\n",
    "\n",
    "    if not os.path.isdir('result'):\n",
    "        os.mkdir('result')\n",
    "    torch.save(state, f'./result/{name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining resnet models\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottaeneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottaeneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # This is the \"stem\"\n",
    "        # For CIFAR (32x32 images), it does not perform downsampling\n",
    "        # It should downsample for ImageNet\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # four stages with three downsampling\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottaeneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottaeneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottaeneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test_resnet18():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "iteration :  50, loss : 2.0433, accuracy : 25.06\n",
      "iteration : 100, loss : 1.9029, accuracy : 30.36\n",
      "iteration : 150, loss : 1.8037, accuracy : 33.70\n",
      "iteration : 200, loss : 1.7451, accuracy : 35.62\n",
      "iteration : 250, loss : 1.6903, accuracy : 37.51\n",
      "iteration : 300, loss : 1.6468, accuracy : 39.26\n",
      "Epoch :   0, training loss : 1.6357, training accuracy : 39.66, validation loss : 1.3036, validation accuracy : 53.98\n",
      "\n",
      "Epoch: 1\n",
      "iteration :  50, loss : 1.3085, accuracy : 52.55\n",
      "iteration : 100, loss : 1.3013, accuracy : 52.57\n",
      "iteration : 150, loss : 1.2893, accuracy : 53.08\n",
      "iteration : 200, loss : 1.2586, accuracy : 54.33\n",
      "iteration : 250, loss : 1.2328, accuracy : 55.43\n",
      "iteration : 300, loss : 1.2098, accuracy : 56.24\n",
      "Epoch :   1, training loss : 1.2049, training accuracy : 56.43, validation loss : 1.1302, validation accuracy : 61.84\n",
      "\n",
      "Epoch: 2\n",
      "iteration :  50, loss : 1.0688, accuracy : 61.59\n",
      "iteration : 100, loss : 1.0709, accuracy : 61.31\n",
      "iteration : 150, loss : 1.0480, accuracy : 62.38\n",
      "iteration : 200, loss : 1.0328, accuracy : 63.11\n",
      "iteration : 250, loss : 1.0238, accuracy : 63.39\n",
      "iteration : 300, loss : 1.0047, accuracy : 63.93\n",
      "Epoch :   2, training loss : 1.0027, training accuracy : 64.08, validation loss : 0.9970, validation accuracy : 66.13\n",
      "\n",
      "Epoch: 3\n",
      "iteration :  50, loss : 0.9046, accuracy : 68.16\n",
      "iteration : 100, loss : 0.8947, accuracy : 68.43\n",
      "iteration : 150, loss : 0.8931, accuracy : 68.45\n",
      "iteration : 200, loss : 0.8779, accuracy : 68.89\n",
      "iteration : 250, loss : 0.8710, accuracy : 69.11\n",
      "iteration : 300, loss : 0.8671, accuracy : 69.32\n",
      "Epoch :   3, training loss : 0.8636, training accuracy : 69.44, validation loss : 0.8616, validation accuracy : 71.58\n",
      "\n",
      "Epoch: 4\n",
      "iteration :  50, loss : 0.7844, accuracy : 72.23\n",
      "iteration : 100, loss : 0.7701, accuracy : 72.61\n",
      "iteration : 150, loss : 0.7755, accuracy : 72.63\n",
      "iteration : 200, loss : 0.7792, accuracy : 72.61\n",
      "iteration : 250, loss : 0.7726, accuracy : 72.76\n",
      "iteration : 300, loss : 0.7678, accuracy : 72.92\n",
      "Epoch :   4, training loss : 0.7683, training accuracy : 72.90, validation loss : 0.7197, validation accuracy : 76.05\n",
      "\n",
      "Epoch: 5\n",
      "iteration :  50, loss : 0.7164, accuracy : 75.38\n",
      "iteration : 100, loss : 0.7214, accuracy : 75.12\n",
      "iteration : 150, loss : 0.7196, accuracy : 75.03\n",
      "iteration : 200, loss : 0.7170, accuracy : 75.16\n",
      "iteration : 250, loss : 0.7138, accuracy : 75.21\n",
      "iteration : 300, loss : 0.7124, accuracy : 75.22\n",
      "Epoch :   5, training loss : 0.7123, training accuracy : 75.21, validation loss : 0.6151, validation accuracy : 79.03\n",
      "\n",
      "Epoch: 6\n",
      "iteration :  50, loss : 0.6705, accuracy : 76.53\n",
      "iteration : 100, loss : 0.6587, accuracy : 76.84\n",
      "iteration : 150, loss : 0.6613, accuracy : 76.84\n",
      "iteration : 200, loss : 0.6625, accuracy : 76.73\n",
      "iteration : 250, loss : 0.6565, accuracy : 77.08\n",
      "iteration : 300, loss : 0.6558, accuracy : 77.02\n",
      "Epoch :   6, training loss : 0.6553, training accuracy : 77.07, validation loss : 0.5914, validation accuracy : 79.81\n",
      "\n",
      "Epoch: 7\n",
      "iteration :  50, loss : 0.6119, accuracy : 79.11\n",
      "iteration : 100, loss : 0.6198, accuracy : 78.53\n",
      "iteration : 150, loss : 0.6223, accuracy : 78.27\n",
      "iteration : 200, loss : 0.6212, accuracy : 78.39\n",
      "iteration : 250, loss : 0.6168, accuracy : 78.58\n",
      "iteration : 300, loss : 0.6144, accuracy : 78.62\n",
      "Epoch :   7, training loss : 0.6146, training accuracy : 78.60, validation loss : 0.6256, validation accuracy : 79.45\n",
      "\n",
      "Epoch: 8\n",
      "iteration :  50, loss : 0.5544, accuracy : 80.67\n",
      "iteration : 100, loss : 0.5752, accuracy : 79.72\n",
      "iteration : 150, loss : 0.5727, accuracy : 79.96\n",
      "iteration : 200, loss : 0.5753, accuracy : 79.87\n",
      "iteration : 250, loss : 0.5772, accuracy : 79.82\n",
      "iteration : 300, loss : 0.5751, accuracy : 79.89\n",
      "Epoch :   8, training loss : 0.5753, training accuracy : 79.90, validation loss : 0.5353, validation accuracy : 82.09\n",
      "\n",
      "Epoch: 9\n",
      "iteration :  50, loss : 0.5420, accuracy : 81.02\n",
      "iteration : 100, loss : 0.5597, accuracy : 80.27\n",
      "iteration : 150, loss : 0.5538, accuracy : 80.69\n",
      "iteration : 200, loss : 0.5540, accuracy : 80.85\n",
      "iteration : 250, loss : 0.5562, accuracy : 80.79\n",
      "iteration : 300, loss : 0.5552, accuracy : 80.79\n",
      "Epoch :   9, training loss : 0.5536, training accuracy : 80.85, validation loss : 0.5115, validation accuracy : 82.78\n",
      "\n",
      "Epoch: 10\n",
      "iteration :  50, loss : 0.5116, accuracy : 82.58\n",
      "iteration : 100, loss : 0.5161, accuracy : 82.13\n",
      "iteration : 150, loss : 0.5220, accuracy : 81.92\n",
      "iteration : 200, loss : 0.5256, accuracy : 81.66\n",
      "iteration : 250, loss : 0.5242, accuracy : 81.64\n",
      "iteration : 300, loss : 0.5260, accuracy : 81.53\n",
      "Epoch :  10, training loss : 0.5243, training accuracy : 81.61, validation loss : 0.5279, validation accuracy : 82.77\n",
      "\n",
      "Epoch: 11\n",
      "iteration :  50, loss : 0.5212, accuracy : 81.66\n",
      "iteration : 100, loss : 0.5081, accuracy : 82.23\n",
      "iteration : 150, loss : 0.4999, accuracy : 82.54\n",
      "iteration : 200, loss : 0.4994, accuracy : 82.59\n",
      "iteration : 250, loss : 0.4974, accuracy : 82.64\n",
      "iteration : 300, loss : 0.4966, accuracy : 82.73\n",
      "Epoch :  11, training loss : 0.4977, training accuracy : 82.69, validation loss : 0.5092, validation accuracy : 82.88\n",
      "\n",
      "Epoch: 12\n",
      "iteration :  50, loss : 0.4872, accuracy : 83.05\n",
      "iteration : 100, loss : 0.4776, accuracy : 83.23\n",
      "iteration : 150, loss : 0.4820, accuracy : 83.09\n",
      "iteration : 200, loss : 0.4781, accuracy : 83.17\n",
      "iteration : 250, loss : 0.4816, accuracy : 83.08\n",
      "iteration : 300, loss : 0.4806, accuracy : 83.05\n",
      "Epoch :  12, training loss : 0.4793, training accuracy : 83.10, validation loss : 0.4596, validation accuracy : 84.33\n",
      "\n",
      "Epoch: 13\n",
      "iteration :  50, loss : 0.4498, accuracy : 84.36\n",
      "iteration : 100, loss : 0.4474, accuracy : 84.37\n",
      "iteration : 150, loss : 0.4554, accuracy : 84.07\n",
      "iteration : 200, loss : 0.4551, accuracy : 84.11\n",
      "iteration : 250, loss : 0.4530, accuracy : 84.25\n",
      "iteration : 300, loss : 0.4557, accuracy : 84.12\n",
      "Epoch :  13, training loss : 0.4551, training accuracy : 84.10, validation loss : 0.4858, validation accuracy : 84.27\n",
      "\n",
      "Epoch: 14\n",
      "iteration :  50, loss : 0.4391, accuracy : 84.89\n",
      "iteration : 100, loss : 0.4481, accuracy : 84.35\n",
      "iteration : 150, loss : 0.4421, accuracy : 84.59\n",
      "iteration : 200, loss : 0.4426, accuracy : 84.59\n",
      "iteration : 250, loss : 0.4412, accuracy : 84.57\n",
      "iteration : 300, loss : 0.4402, accuracy : 84.63\n",
      "Epoch :  14, training loss : 0.4414, training accuracy : 84.59, validation loss : 0.4563, validation accuracy : 84.98\n",
      "\n",
      "Epoch: 15\n",
      "iteration :  50, loss : 0.4249, accuracy : 85.11\n",
      "iteration : 100, loss : 0.4220, accuracy : 85.42\n",
      "iteration : 150, loss : 0.4222, accuracy : 85.33\n",
      "iteration : 200, loss : 0.4225, accuracy : 85.41\n",
      "iteration : 250, loss : 0.4240, accuracy : 85.26\n",
      "iteration : 300, loss : 0.4238, accuracy : 85.28\n",
      "Epoch :  15, training loss : 0.4246, training accuracy : 85.25, validation loss : 0.5086, validation accuracy : 84.32\n",
      "\n",
      "Epoch: 16\n",
      "iteration :  50, loss : 0.4025, accuracy : 86.17\n",
      "iteration : 100, loss : 0.4090, accuracy : 85.70\n",
      "iteration : 150, loss : 0.4137, accuracy : 85.45\n",
      "iteration : 200, loss : 0.4152, accuracy : 85.45\n",
      "iteration : 250, loss : 0.4127, accuracy : 85.59\n",
      "iteration : 300, loss : 0.4134, accuracy : 85.51\n",
      "Epoch :  16, training loss : 0.4127, training accuracy : 85.50, validation loss : 0.4004, validation accuracy : 86.76\n",
      "\n",
      "Epoch: 17\n",
      "iteration :  50, loss : 0.3980, accuracy : 86.09\n",
      "iteration : 100, loss : 0.4054, accuracy : 85.78\n",
      "iteration : 150, loss : 0.4043, accuracy : 85.94\n",
      "iteration : 200, loss : 0.3995, accuracy : 86.02\n",
      "iteration : 250, loss : 0.4006, accuracy : 86.05\n",
      "iteration : 300, loss : 0.3974, accuracy : 86.09\n",
      "Epoch :  17, training loss : 0.3965, training accuracy : 86.12, validation loss : 0.4452, validation accuracy : 85.89\n",
      "\n",
      "Epoch: 18\n",
      "iteration :  50, loss : 0.3875, accuracy : 86.03\n",
      "iteration : 100, loss : 0.3818, accuracy : 86.30\n",
      "iteration : 150, loss : 0.3768, accuracy : 86.56\n",
      "iteration : 200, loss : 0.3808, accuracy : 86.40\n",
      "iteration : 250, loss : 0.3852, accuracy : 86.36\n",
      "iteration : 300, loss : 0.3864, accuracy : 86.34\n",
      "Epoch :  18, training loss : 0.3867, training accuracy : 86.34, validation loss : 0.5985, validation accuracy : 81.46\n",
      "\n",
      "Epoch: 19\n",
      "iteration :  50, loss : 0.3600, accuracy : 86.98\n",
      "iteration : 100, loss : 0.3734, accuracy : 86.49\n",
      "iteration : 150, loss : 0.3747, accuracy : 86.55\n",
      "iteration : 200, loss : 0.3699, accuracy : 86.89\n",
      "iteration : 250, loss : 0.3703, accuracy : 86.92\n",
      "iteration : 300, loss : 0.3731, accuracy : 86.82\n",
      "Epoch :  19, training loss : 0.3746, training accuracy : 86.77, validation loss : 0.5146, validation accuracy : 83.71\n",
      "\n",
      "Epoch: 20\n",
      "iteration :  50, loss : 0.3635, accuracy : 87.53\n",
      "iteration : 100, loss : 0.3676, accuracy : 87.37\n",
      "iteration : 150, loss : 0.3641, accuracy : 87.40\n",
      "iteration : 200, loss : 0.3619, accuracy : 87.54\n",
      "iteration : 250, loss : 0.3651, accuracy : 87.40\n",
      "iteration : 300, loss : 0.3644, accuracy : 87.32\n",
      "Epoch :  20, training loss : 0.3655, training accuracy : 87.31, validation loss : 0.3733, validation accuracy : 87.98\n",
      "\n",
      "Epoch: 21\n",
      "iteration :  50, loss : 0.3469, accuracy : 87.58\n",
      "iteration : 100, loss : 0.3432, accuracy : 87.92\n",
      "iteration : 150, loss : 0.3420, accuracy : 88.09\n",
      "iteration : 200, loss : 0.3467, accuracy : 87.90\n",
      "iteration : 250, loss : 0.3528, accuracy : 87.74\n",
      "iteration : 300, loss : 0.3542, accuracy : 87.66\n",
      "Epoch :  21, training loss : 0.3536, training accuracy : 87.69, validation loss : 0.4246, validation accuracy : 86.08\n",
      "\n",
      "Epoch: 22\n",
      "iteration :  50, loss : 0.3301, accuracy : 88.66\n",
      "iteration : 100, loss : 0.3349, accuracy : 88.36\n",
      "iteration : 150, loss : 0.3336, accuracy : 88.39\n",
      "iteration : 200, loss : 0.3367, accuracy : 88.20\n",
      "iteration : 250, loss : 0.3402, accuracy : 88.06\n",
      "iteration : 300, loss : 0.3455, accuracy : 87.92\n",
      "Epoch :  22, training loss : 0.3462, training accuracy : 87.89, validation loss : 0.3925, validation accuracy : 87.41\n",
      "\n",
      "Epoch: 23\n",
      "iteration :  50, loss : 0.3434, accuracy : 88.14\n",
      "iteration : 100, loss : 0.3454, accuracy : 87.91\n",
      "iteration : 150, loss : 0.3366, accuracy : 88.15\n",
      "iteration : 200, loss : 0.3356, accuracy : 88.14\n",
      "iteration : 250, loss : 0.3349, accuracy : 88.17\n",
      "iteration : 300, loss : 0.3357, accuracy : 88.20\n",
      "Epoch :  23, training loss : 0.3374, training accuracy : 88.14, validation loss : 0.3678, validation accuracy : 87.96\n",
      "\n",
      "Epoch: 24\n",
      "iteration :  50, loss : 0.3261, accuracy : 88.84\n",
      "iteration : 100, loss : 0.3186, accuracy : 89.02\n",
      "iteration : 150, loss : 0.3189, accuracy : 88.90\n",
      "iteration : 200, loss : 0.3237, accuracy : 88.67\n",
      "iteration : 250, loss : 0.3286, accuracy : 88.38\n",
      "iteration : 300, loss : 0.3251, accuracy : 88.51\n",
      "Epoch :  24, training loss : 0.3243, training accuracy : 88.52, validation loss : 0.3839, validation accuracy : 87.71\n",
      "\n",
      "Epoch: 25\n",
      "iteration :  50, loss : 0.2899, accuracy : 89.73\n",
      "iteration : 100, loss : 0.3004, accuracy : 89.60\n",
      "iteration : 150, loss : 0.3061, accuracy : 89.26\n",
      "iteration : 200, loss : 0.3107, accuracy : 88.98\n",
      "iteration : 250, loss : 0.3138, accuracy : 88.94\n",
      "iteration : 300, loss : 0.3151, accuracy : 88.97\n",
      "Epoch :  25, training loss : 0.3160, training accuracy : 88.93, validation loss : 0.3686, validation accuracy : 88.26\n",
      "\n",
      "Epoch: 26\n",
      "iteration :  50, loss : 0.2954, accuracy : 89.77\n",
      "iteration : 100, loss : 0.3016, accuracy : 89.33\n",
      "iteration : 150, loss : 0.3005, accuracy : 89.35\n",
      "iteration : 200, loss : 0.3047, accuracy : 89.08\n",
      "iteration : 250, loss : 0.3079, accuracy : 89.00\n",
      "iteration : 300, loss : 0.3090, accuracy : 89.09\n",
      "Epoch :  26, training loss : 0.3081, training accuracy : 89.08, validation loss : 0.3712, validation accuracy : 88.22\n",
      "\n",
      "Epoch: 27\n",
      "iteration :  50, loss : 0.2887, accuracy : 89.83\n",
      "iteration : 100, loss : 0.2928, accuracy : 89.58\n",
      "iteration : 150, loss : 0.2962, accuracy : 89.39\n",
      "iteration : 200, loss : 0.2975, accuracy : 89.40\n",
      "iteration : 250, loss : 0.3035, accuracy : 89.26\n",
      "iteration : 300, loss : 0.3054, accuracy : 89.17\n",
      "Epoch :  27, training loss : 0.3053, training accuracy : 89.21, validation loss : 0.4073, validation accuracy : 87.04\n",
      "\n",
      "Epoch: 28\n",
      "iteration :  50, loss : 0.2864, accuracy : 89.75\n",
      "iteration : 100, loss : 0.2911, accuracy : 89.76\n",
      "iteration : 150, loss : 0.2937, accuracy : 89.84\n",
      "iteration : 200, loss : 0.2940, accuracy : 89.79\n",
      "iteration : 250, loss : 0.2966, accuracy : 89.62\n",
      "iteration : 300, loss : 0.2981, accuracy : 89.52\n",
      "Epoch :  28, training loss : 0.2981, training accuracy : 89.51, validation loss : 0.3706, validation accuracy : 88.83\n",
      "\n",
      "Epoch: 29\n",
      "iteration :  50, loss : 0.2756, accuracy : 90.47\n",
      "iteration : 100, loss : 0.2755, accuracy : 90.25\n",
      "iteration : 150, loss : 0.2832, accuracy : 90.07\n",
      "iteration : 200, loss : 0.2882, accuracy : 89.80\n",
      "iteration : 250, loss : 0.2859, accuracy : 89.92\n",
      "iteration : 300, loss : 0.2852, accuracy : 89.99\n",
      "Epoch :  29, training loss : 0.2848, training accuracy : 90.00, validation loss : 0.3836, validation accuracy : 87.78\n",
      "\n",
      "Epoch: 30\n",
      "iteration :  50, loss : 0.2777, accuracy : 90.34\n",
      "iteration : 100, loss : 0.2805, accuracy : 90.27\n",
      "iteration : 150, loss : 0.2839, accuracy : 90.12\n",
      "iteration : 200, loss : 0.2781, accuracy : 90.35\n",
      "iteration : 250, loss : 0.2790, accuracy : 90.29\n",
      "iteration : 300, loss : 0.2826, accuracy : 90.10\n",
      "Epoch :  30, training loss : 0.2832, training accuracy : 90.09, validation loss : 0.3590, validation accuracy : 88.85\n",
      "\n",
      "Epoch: 31\n",
      "iteration :  50, loss : 0.2682, accuracy : 90.53\n",
      "iteration : 100, loss : 0.2713, accuracy : 90.49\n",
      "iteration : 150, loss : 0.2713, accuracy : 90.55\n",
      "iteration : 200, loss : 0.2715, accuracy : 90.48\n",
      "iteration : 250, loss : 0.2747, accuracy : 90.35\n",
      "iteration : 300, loss : 0.2752, accuracy : 90.33\n",
      "Epoch :  31, training loss : 0.2747, training accuracy : 90.35, validation loss : 0.3330, validation accuracy : 89.35\n",
      "\n",
      "Epoch: 32\n",
      "iteration :  50, loss : 0.2602, accuracy : 90.89\n",
      "iteration : 100, loss : 0.2707, accuracy : 90.42\n",
      "iteration : 150, loss : 0.2722, accuracy : 90.42\n",
      "iteration : 200, loss : 0.2720, accuracy : 90.46\n",
      "iteration : 250, loss : 0.2718, accuracy : 90.44\n",
      "iteration : 300, loss : 0.2741, accuracy : 90.31\n",
      "Epoch :  32, training loss : 0.2747, training accuracy : 90.29, validation loss : 0.3604, validation accuracy : 89.30\n",
      "\n",
      "Epoch: 33\n",
      "iteration :  50, loss : 0.2715, accuracy : 89.94\n",
      "iteration : 100, loss : 0.2675, accuracy : 90.23\n",
      "iteration : 150, loss : 0.2625, accuracy : 90.48\n",
      "iteration : 200, loss : 0.2613, accuracy : 90.65\n",
      "iteration : 250, loss : 0.2618, accuracy : 90.67\n",
      "iteration : 300, loss : 0.2654, accuracy : 90.57\n",
      "Epoch :  33, training loss : 0.2662, training accuracy : 90.56, validation loss : 0.3667, validation accuracy : 88.79\n",
      "\n",
      "Epoch: 34\n",
      "iteration :  50, loss : 0.2458, accuracy : 91.56\n",
      "iteration : 100, loss : 0.2486, accuracy : 91.36\n",
      "iteration : 150, loss : 0.2573, accuracy : 91.05\n",
      "iteration : 200, loss : 0.2566, accuracy : 91.00\n",
      "iteration : 250, loss : 0.2573, accuracy : 91.00\n",
      "iteration : 300, loss : 0.2603, accuracy : 90.88\n",
      "Epoch :  34, training loss : 0.2594, training accuracy : 90.92, validation loss : 0.3424, validation accuracy : 89.83\n",
      "\n",
      "Epoch: 35\n",
      "iteration :  50, loss : 0.2512, accuracy : 90.77\n",
      "iteration : 100, loss : 0.2412, accuracy : 91.32\n",
      "iteration : 150, loss : 0.2435, accuracy : 91.21\n",
      "iteration : 200, loss : 0.2492, accuracy : 91.07\n",
      "iteration : 250, loss : 0.2537, accuracy : 90.94\n",
      "iteration : 300, loss : 0.2531, accuracy : 91.00\n",
      "Epoch :  35, training loss : 0.2543, training accuracy : 90.93, validation loss : 0.3804, validation accuracy : 88.46\n",
      "\n",
      "Epoch: 36\n",
      "iteration :  50, loss : 0.2590, accuracy : 90.56\n",
      "iteration : 100, loss : 0.2517, accuracy : 90.99\n",
      "iteration : 150, loss : 0.2512, accuracy : 91.15\n",
      "iteration : 200, loss : 0.2523, accuracy : 91.16\n",
      "iteration : 250, loss : 0.2532, accuracy : 91.15\n",
      "iteration : 300, loss : 0.2550, accuracy : 91.03\n",
      "Epoch :  36, training loss : 0.2544, training accuracy : 91.03, validation loss : 0.3254, validation accuracy : 89.80\n",
      "\n",
      "Epoch: 37\n",
      "iteration :  50, loss : 0.2204, accuracy : 92.20\n",
      "iteration : 100, loss : 0.2362, accuracy : 91.45\n",
      "iteration : 150, loss : 0.2363, accuracy : 91.65\n",
      "iteration : 200, loss : 0.2392, accuracy : 91.54\n",
      "iteration : 250, loss : 0.2414, accuracy : 91.46\n",
      "iteration : 300, loss : 0.2404, accuracy : 91.53\n",
      "Epoch :  37, training loss : 0.2412, training accuracy : 91.50, validation loss : 0.3435, validation accuracy : 89.48\n",
      "\n",
      "Epoch: 38\n",
      "iteration :  50, loss : 0.2452, accuracy : 91.39\n",
      "iteration : 100, loss : 0.2384, accuracy : 91.72\n",
      "iteration : 150, loss : 0.2326, accuracy : 91.98\n",
      "iteration : 200, loss : 0.2308, accuracy : 91.96\n",
      "iteration : 250, loss : 0.2365, accuracy : 91.74\n",
      "iteration : 300, loss : 0.2393, accuracy : 91.65\n",
      "Epoch :  38, training loss : 0.2418, training accuracy : 91.58, validation loss : 0.3799, validation accuracy : 89.08\n",
      "\n",
      "Epoch: 39\n",
      "iteration :  50, loss : 0.2305, accuracy : 92.17\n",
      "iteration : 100, loss : 0.2336, accuracy : 91.97\n",
      "iteration : 150, loss : 0.2380, accuracy : 91.71\n",
      "iteration : 200, loss : 0.2335, accuracy : 91.88\n",
      "iteration : 250, loss : 0.2349, accuracy : 91.83\n",
      "iteration : 300, loss : 0.2351, accuracy : 91.84\n",
      "Epoch :  39, training loss : 0.2363, training accuracy : 91.79, validation loss : 0.3473, validation accuracy : 89.33\n",
      "\n",
      "Epoch: 40\n",
      "iteration :  50, loss : 0.2483, accuracy : 91.55\n",
      "iteration : 100, loss : 0.2373, accuracy : 91.87\n",
      "iteration : 150, loss : 0.2408, accuracy : 91.59\n",
      "iteration : 200, loss : 0.2376, accuracy : 91.81\n",
      "iteration : 250, loss : 0.2380, accuracy : 91.72\n",
      "iteration : 300, loss : 0.2374, accuracy : 91.75\n",
      "Epoch :  40, training loss : 0.2374, training accuracy : 91.74, validation loss : 0.3372, validation accuracy : 89.51\n",
      "\n",
      "Epoch: 41\n",
      "iteration :  50, loss : 0.2194, accuracy : 92.48\n",
      "iteration : 100, loss : 0.2176, accuracy : 92.30\n",
      "iteration : 150, loss : 0.2178, accuracy : 92.28\n",
      "iteration : 200, loss : 0.2227, accuracy : 92.06\n",
      "iteration : 250, loss : 0.2252, accuracy : 92.02\n",
      "iteration : 300, loss : 0.2279, accuracy : 91.99\n",
      "Epoch :  41, training loss : 0.2277, training accuracy : 91.98, validation loss : 0.3838, validation accuracy : 88.50\n",
      "\n",
      "Epoch: 42\n",
      "iteration :  50, loss : 0.2236, accuracy : 92.27\n",
      "iteration : 100, loss : 0.2202, accuracy : 92.31\n",
      "iteration : 150, loss : 0.2233, accuracy : 92.20\n",
      "iteration : 200, loss : 0.2225, accuracy : 92.23\n",
      "iteration : 250, loss : 0.2237, accuracy : 92.12\n",
      "iteration : 300, loss : 0.2261, accuracy : 92.04\n",
      "Epoch :  42, training loss : 0.2257, training accuracy : 92.03, validation loss : 0.3489, validation accuracy : 89.66\n",
      "\n",
      "Epoch: 43\n",
      "iteration :  50, loss : 0.2266, accuracy : 91.84\n",
      "iteration : 100, loss : 0.2258, accuracy : 92.04\n",
      "iteration : 150, loss : 0.2259, accuracy : 92.09\n",
      "iteration : 200, loss : 0.2257, accuracy : 92.05\n",
      "iteration : 250, loss : 0.2253, accuracy : 92.05\n",
      "iteration : 300, loss : 0.2263, accuracy : 91.99\n",
      "Epoch :  43, training loss : 0.2262, training accuracy : 91.99, validation loss : 0.3459, validation accuracy : 89.93\n",
      "\n",
      "Epoch: 44\n",
      "iteration :  50, loss : 0.2107, accuracy : 92.55\n",
      "iteration : 100, loss : 0.2093, accuracy : 92.70\n",
      "iteration : 150, loss : 0.2115, accuracy : 92.62\n",
      "iteration : 200, loss : 0.2167, accuracy : 92.45\n",
      "iteration : 250, loss : 0.2200, accuracy : 92.22\n",
      "iteration : 300, loss : 0.2216, accuracy : 92.14\n",
      "Epoch :  44, training loss : 0.2222, training accuracy : 92.11, validation loss : 0.3070, validation accuracy : 90.18\n",
      "\n",
      "Epoch: 45\n",
      "iteration :  50, loss : 0.2006, accuracy : 93.56\n",
      "iteration : 100, loss : 0.2022, accuracy : 93.26\n",
      "iteration : 150, loss : 0.2034, accuracy : 93.08\n",
      "iteration : 200, loss : 0.2046, accuracy : 92.91\n",
      "iteration : 250, loss : 0.2073, accuracy : 92.80\n",
      "iteration : 300, loss : 0.2125, accuracy : 92.62\n",
      "Epoch :  45, training loss : 0.2132, training accuracy : 92.58, validation loss : 0.3784, validation accuracy : 88.68\n",
      "\n",
      "Epoch: 46\n",
      "iteration :  50, loss : 0.2033, accuracy : 93.16\n",
      "iteration : 100, loss : 0.2033, accuracy : 92.94\n",
      "iteration : 150, loss : 0.2089, accuracy : 92.67\n",
      "iteration : 200, loss : 0.2152, accuracy : 92.43\n",
      "iteration : 250, loss : 0.2136, accuracy : 92.47\n",
      "iteration : 300, loss : 0.2142, accuracy : 92.48\n",
      "Epoch :  46, training loss : 0.2155, training accuracy : 92.44, validation loss : 0.3561, validation accuracy : 89.52\n",
      "\n",
      "Epoch: 47\n",
      "iteration :  50, loss : 0.2009, accuracy : 93.14\n",
      "iteration : 100, loss : 0.2110, accuracy : 92.66\n",
      "iteration : 150, loss : 0.2112, accuracy : 92.65\n",
      "iteration : 200, loss : 0.2081, accuracy : 92.73\n",
      "iteration : 250, loss : 0.2115, accuracy : 92.51\n",
      "iteration : 300, loss : 0.2123, accuracy : 92.48\n",
      "Epoch :  47, training loss : 0.2146, training accuracy : 92.43, validation loss : 0.3572, validation accuracy : 89.49\n",
      "\n",
      "Epoch: 48\n",
      "iteration :  50, loss : 0.2073, accuracy : 92.53\n",
      "iteration : 100, loss : 0.2030, accuracy : 92.83\n",
      "iteration : 150, loss : 0.2021, accuracy : 92.94\n",
      "iteration : 200, loss : 0.2065, accuracy : 92.73\n",
      "iteration : 250, loss : 0.2107, accuracy : 92.57\n",
      "iteration : 300, loss : 0.2100, accuracy : 92.62\n",
      "Epoch :  48, training loss : 0.2099, training accuracy : 92.64, validation loss : 0.3019, validation accuracy : 90.53\n",
      "\n",
      "Epoch: 49\n",
      "iteration :  50, loss : 0.1998, accuracy : 92.84\n",
      "iteration : 100, loss : 0.1963, accuracy : 93.05\n",
      "iteration : 150, loss : 0.1972, accuracy : 92.99\n",
      "iteration : 200, loss : 0.2007, accuracy : 92.93\n",
      "iteration : 250, loss : 0.2020, accuracy : 92.86\n",
      "iteration : 300, loss : 0.2025, accuracy : 92.81\n",
      "Epoch :  49, training loss : 0.2039, training accuracy : 92.74, validation loss : 0.3247, validation accuracy : 90.03\n",
      "\n",
      "Epoch: 50\n",
      "iteration :  50, loss : 0.1963, accuracy : 93.39\n",
      "iteration : 100, loss : 0.2012, accuracy : 93.18\n",
      "iteration : 150, loss : 0.2031, accuracy : 93.04\n",
      "iteration : 200, loss : 0.2053, accuracy : 92.84\n",
      "iteration : 250, loss : 0.2022, accuracy : 92.97\n",
      "iteration : 300, loss : 0.2019, accuracy : 92.99\n",
      "Epoch :  50, training loss : 0.2026, training accuracy : 93.00, validation loss : 0.2779, validation accuracy : 91.36\n",
      "\n",
      "Epoch: 51\n",
      "iteration :  50, loss : 0.1929, accuracy : 92.97\n",
      "iteration : 100, loss : 0.1913, accuracy : 93.28\n",
      "iteration : 150, loss : 0.1978, accuracy : 93.02\n",
      "iteration : 200, loss : 0.1998, accuracy : 93.00\n",
      "iteration : 250, loss : 0.1987, accuracy : 93.06\n",
      "iteration : 300, loss : 0.1982, accuracy : 93.07\n",
      "Epoch :  51, training loss : 0.1990, training accuracy : 93.03, validation loss : 0.3949, validation accuracy : 88.93\n",
      "\n",
      "Epoch: 52\n",
      "iteration :  50, loss : 0.1866, accuracy : 93.48\n",
      "iteration : 100, loss : 0.1936, accuracy : 93.14\n",
      "iteration : 150, loss : 0.1952, accuracy : 93.12\n",
      "iteration : 200, loss : 0.1956, accuracy : 93.16\n",
      "iteration : 250, loss : 0.1947, accuracy : 93.16\n",
      "iteration : 300, loss : 0.1977, accuracy : 93.00\n",
      "Epoch :  52, training loss : 0.1995, training accuracy : 92.95, validation loss : 0.3813, validation accuracy : 89.25\n",
      "\n",
      "Epoch: 53\n",
      "iteration :  50, loss : 0.1816, accuracy : 93.47\n",
      "iteration : 100, loss : 0.1911, accuracy : 93.20\n",
      "iteration : 150, loss : 0.1916, accuracy : 93.22\n",
      "iteration : 200, loss : 0.1916, accuracy : 93.25\n",
      "iteration : 250, loss : 0.1918, accuracy : 93.25\n",
      "iteration : 300, loss : 0.1946, accuracy : 93.17\n",
      "Epoch :  53, training loss : 0.1952, training accuracy : 93.17, validation loss : 0.3798, validation accuracy : 88.99\n",
      "\n",
      "Epoch: 54\n",
      "iteration :  50, loss : 0.1806, accuracy : 93.70\n",
      "iteration : 100, loss : 0.1821, accuracy : 93.55\n",
      "iteration : 150, loss : 0.1858, accuracy : 93.46\n",
      "iteration : 200, loss : 0.1885, accuracy : 93.46\n",
      "iteration : 250, loss : 0.1882, accuracy : 93.49\n",
      "iteration : 300, loss : 0.1915, accuracy : 93.36\n",
      "Epoch :  54, training loss : 0.1926, training accuracy : 93.32, validation loss : 0.3152, validation accuracy : 90.80\n",
      "\n",
      "Epoch: 55\n",
      "iteration :  50, loss : 0.1951, accuracy : 93.27\n",
      "iteration : 100, loss : 0.1875, accuracy : 93.38\n",
      "iteration : 150, loss : 0.1857, accuracy : 93.45\n",
      "iteration : 200, loss : 0.1874, accuracy : 93.43\n",
      "iteration : 250, loss : 0.1885, accuracy : 93.32\n",
      "iteration : 300, loss : 0.1894, accuracy : 93.27\n",
      "Epoch :  55, training loss : 0.1898, training accuracy : 93.27, validation loss : 0.2951, validation accuracy : 91.28\n",
      "\n",
      "Epoch: 56\n",
      "iteration :  50, loss : 0.1798, accuracy : 93.66\n",
      "iteration : 100, loss : 0.1794, accuracy : 93.66\n",
      "iteration : 150, loss : 0.1837, accuracy : 93.44\n",
      "iteration : 200, loss : 0.1834, accuracy : 93.48\n",
      "iteration : 250, loss : 0.1879, accuracy : 93.40\n",
      "iteration : 300, loss : 0.1882, accuracy : 93.32\n",
      "Epoch :  56, training loss : 0.1888, training accuracy : 93.28, validation loss : 0.3876, validation accuracy : 89.41\n",
      "\n",
      "Epoch: 57\n",
      "iteration :  50, loss : 0.1924, accuracy : 93.12\n",
      "iteration : 100, loss : 0.1891, accuracy : 93.33\n",
      "iteration : 150, loss : 0.1878, accuracy : 93.38\n",
      "iteration : 200, loss : 0.1857, accuracy : 93.41\n",
      "iteration : 250, loss : 0.1893, accuracy : 93.36\n",
      "iteration : 300, loss : 0.1922, accuracy : 93.23\n",
      "Epoch :  57, training loss : 0.1918, training accuracy : 93.25, validation loss : 0.3144, validation accuracy : 90.34\n",
      "\n",
      "Epoch: 58\n",
      "iteration :  50, loss : 0.1688, accuracy : 93.98\n",
      "iteration : 100, loss : 0.1755, accuracy : 93.67\n",
      "iteration : 150, loss : 0.1737, accuracy : 93.90\n",
      "iteration : 200, loss : 0.1770, accuracy : 93.83\n",
      "iteration : 250, loss : 0.1793, accuracy : 93.76\n",
      "iteration : 300, loss : 0.1826, accuracy : 93.64\n",
      "Epoch :  58, training loss : 0.1840, training accuracy : 93.60, validation loss : 0.4075, validation accuracy : 88.94\n",
      "\n",
      "Epoch: 59\n",
      "iteration :  50, loss : 0.1723, accuracy : 94.14\n",
      "iteration : 100, loss : 0.1735, accuracy : 93.99\n",
      "iteration : 150, loss : 0.1777, accuracy : 93.81\n",
      "iteration : 200, loss : 0.1779, accuracy : 93.82\n",
      "iteration : 250, loss : 0.1786, accuracy : 93.82\n",
      "iteration : 300, loss : 0.1801, accuracy : 93.76\n",
      "Epoch :  59, training loss : 0.1799, training accuracy : 93.74, validation loss : 0.3024, validation accuracy : 90.78\n",
      "\n",
      "Epoch: 60\n",
      "iteration :  50, loss : 0.1715, accuracy : 93.88\n",
      "iteration : 100, loss : 0.1770, accuracy : 93.76\n",
      "iteration : 150, loss : 0.1779, accuracy : 93.77\n",
      "iteration : 200, loss : 0.1754, accuracy : 93.89\n",
      "iteration : 250, loss : 0.1764, accuracy : 93.85\n",
      "iteration : 300, loss : 0.1759, accuracy : 93.86\n",
      "Epoch :  60, training loss : 0.1760, training accuracy : 93.86, validation loss : 0.2949, validation accuracy : 91.16\n",
      "\n",
      "Epoch: 61\n",
      "iteration :  50, loss : 0.1589, accuracy : 94.36\n",
      "iteration : 100, loss : 0.1655, accuracy : 94.10\n",
      "iteration : 150, loss : 0.1753, accuracy : 93.87\n",
      "iteration : 200, loss : 0.1765, accuracy : 93.79\n",
      "iteration : 250, loss : 0.1780, accuracy : 93.72\n",
      "iteration : 300, loss : 0.1783, accuracy : 93.70\n",
      "Epoch :  61, training loss : 0.1804, training accuracy : 93.64, validation loss : 0.2969, validation accuracy : 90.75\n",
      "\n",
      "Epoch: 62\n",
      "iteration :  50, loss : 0.1731, accuracy : 93.86\n",
      "iteration : 100, loss : 0.1713, accuracy : 93.73\n",
      "iteration : 150, loss : 0.1722, accuracy : 93.74\n",
      "iteration : 200, loss : 0.1757, accuracy : 93.71\n",
      "iteration : 250, loss : 0.1782, accuracy : 93.72\n",
      "iteration : 300, loss : 0.1788, accuracy : 93.68\n",
      "Epoch :  62, training loss : 0.1787, training accuracy : 93.66, validation loss : 0.2860, validation accuracy : 91.21\n",
      "\n",
      "Epoch: 63\n",
      "iteration :  50, loss : 0.1670, accuracy : 94.11\n",
      "iteration : 100, loss : 0.1743, accuracy : 93.88\n",
      "iteration : 150, loss : 0.1776, accuracy : 93.79\n",
      "iteration : 200, loss : 0.1758, accuracy : 93.84\n",
      "iteration : 250, loss : 0.1749, accuracy : 93.84\n",
      "iteration : 300, loss : 0.1755, accuracy : 93.83\n",
      "Epoch :  63, training loss : 0.1748, training accuracy : 93.86, validation loss : 0.2974, validation accuracy : 91.00\n",
      "\n",
      "Epoch: 64\n",
      "iteration :  50, loss : 0.1608, accuracy : 94.23\n",
      "iteration : 100, loss : 0.1663, accuracy : 94.02\n",
      "iteration : 150, loss : 0.1699, accuracy : 94.03\n",
      "iteration : 200, loss : 0.1714, accuracy : 94.03\n",
      "iteration : 250, loss : 0.1699, accuracy : 94.11\n",
      "iteration : 300, loss : 0.1715, accuracy : 93.99\n",
      "Epoch :  64, training loss : 0.1718, training accuracy : 93.99, validation loss : 0.2624, validation accuracy : 91.83\n",
      "\n",
      "Epoch: 65\n",
      "iteration :  50, loss : 0.1668, accuracy : 94.44\n",
      "iteration : 100, loss : 0.1717, accuracy : 94.23\n",
      "iteration : 150, loss : 0.1712, accuracy : 94.18\n",
      "iteration : 200, loss : 0.1741, accuracy : 94.01\n",
      "iteration : 250, loss : 0.1735, accuracy : 94.03\n",
      "iteration : 300, loss : 0.1746, accuracy : 93.95\n",
      "Epoch :  65, training loss : 0.1757, training accuracy : 93.92, validation loss : 0.3132, validation accuracy : 91.38\n",
      "\n",
      "Epoch: 66\n",
      "iteration :  50, loss : 0.1696, accuracy : 93.75\n",
      "iteration : 100, loss : 0.1615, accuracy : 94.35\n",
      "iteration : 150, loss : 0.1626, accuracy : 94.27\n",
      "iteration : 200, loss : 0.1681, accuracy : 94.15\n",
      "iteration : 250, loss : 0.1722, accuracy : 94.02\n",
      "iteration : 300, loss : 0.1720, accuracy : 94.03\n",
      "Epoch :  66, training loss : 0.1728, training accuracy : 94.02, validation loss : 0.2801, validation accuracy : 92.02\n",
      "\n",
      "Epoch: 67\n",
      "iteration :  50, loss : 0.1612, accuracy : 94.44\n",
      "iteration : 100, loss : 0.1620, accuracy : 94.28\n",
      "iteration : 150, loss : 0.1653, accuracy : 94.20\n",
      "iteration : 200, loss : 0.1645, accuracy : 94.25\n",
      "iteration : 250, loss : 0.1685, accuracy : 94.14\n",
      "iteration : 300, loss : 0.1718, accuracy : 93.99\n",
      "Epoch :  67, training loss : 0.1717, training accuracy : 93.97, validation loss : 0.3198, validation accuracy : 90.76\n",
      "\n",
      "Epoch: 68\n",
      "iteration :  50, loss : 0.1735, accuracy : 94.22\n",
      "iteration : 100, loss : 0.1635, accuracy : 94.51\n",
      "iteration : 150, loss : 0.1677, accuracy : 94.35\n",
      "iteration : 200, loss : 0.1664, accuracy : 94.37\n",
      "iteration : 250, loss : 0.1665, accuracy : 94.33\n",
      "iteration : 300, loss : 0.1661, accuracy : 94.34\n",
      "Epoch :  68, training loss : 0.1665, training accuracy : 94.32, validation loss : 0.3125, validation accuracy : 90.94\n",
      "\n",
      "Epoch: 69\n",
      "iteration :  50, loss : 0.1634, accuracy : 94.58\n",
      "iteration : 100, loss : 0.1643, accuracy : 94.34\n",
      "iteration : 150, loss : 0.1631, accuracy : 94.30\n",
      "iteration : 200, loss : 0.1627, accuracy : 94.41\n",
      "iteration : 250, loss : 0.1637, accuracy : 94.35\n",
      "iteration : 300, loss : 0.1647, accuracy : 94.29\n",
      "Epoch :  69, training loss : 0.1640, training accuracy : 94.33, validation loss : 0.3104, validation accuracy : 90.81\n",
      "\n",
      "Epoch: 70\n",
      "iteration :  50, loss : 0.1430, accuracy : 95.19\n",
      "iteration : 100, loss : 0.1532, accuracy : 94.73\n",
      "iteration : 150, loss : 0.1572, accuracy : 94.64\n",
      "iteration : 200, loss : 0.1568, accuracy : 94.61\n",
      "iteration : 250, loss : 0.1585, accuracy : 94.54\n",
      "iteration : 300, loss : 0.1610, accuracy : 94.44\n",
      "Epoch :  70, training loss : 0.1622, training accuracy : 94.41, validation loss : 0.3494, validation accuracy : 90.26\n",
      "\n",
      "Epoch: 71\n",
      "iteration :  50, loss : 0.1478, accuracy : 94.55\n",
      "iteration : 100, loss : 0.1446, accuracy : 94.91\n",
      "iteration : 150, loss : 0.1544, accuracy : 94.57\n",
      "iteration : 200, loss : 0.1587, accuracy : 94.52\n",
      "iteration : 250, loss : 0.1588, accuracy : 94.51\n",
      "iteration : 300, loss : 0.1603, accuracy : 94.40\n",
      "Epoch :  71, training loss : 0.1610, training accuracy : 94.38, validation loss : 0.2675, validation accuracy : 92.14\n",
      "\n",
      "Epoch: 72\n",
      "iteration :  50, loss : 0.1586, accuracy : 94.44\n",
      "iteration : 100, loss : 0.1555, accuracy : 94.54\n",
      "iteration : 150, loss : 0.1564, accuracy : 94.55\n",
      "iteration : 200, loss : 0.1571, accuracy : 94.54\n",
      "iteration : 250, loss : 0.1580, accuracy : 94.46\n",
      "iteration : 300, loss : 0.1585, accuracy : 94.48\n",
      "Epoch :  72, training loss : 0.1591, training accuracy : 94.45, validation loss : 0.2723, validation accuracy : 91.65\n",
      "\n",
      "Epoch: 73\n",
      "iteration :  50, loss : 0.1587, accuracy : 94.25\n",
      "iteration : 100, loss : 0.1591, accuracy : 94.38\n",
      "iteration : 150, loss : 0.1600, accuracy : 94.36\n",
      "iteration : 200, loss : 0.1580, accuracy : 94.50\n",
      "iteration : 250, loss : 0.1567, accuracy : 94.57\n",
      "iteration : 300, loss : 0.1581, accuracy : 94.53\n",
      "Epoch :  73, training loss : 0.1586, training accuracy : 94.51, validation loss : 0.2921, validation accuracy : 91.30\n",
      "\n",
      "Epoch: 74\n",
      "iteration :  50, loss : 0.1429, accuracy : 95.02\n",
      "iteration : 100, loss : 0.1504, accuracy : 94.84\n",
      "iteration : 150, loss : 0.1498, accuracy : 94.87\n",
      "iteration : 200, loss : 0.1524, accuracy : 94.75\n",
      "iteration : 250, loss : 0.1552, accuracy : 94.64\n",
      "iteration : 300, loss : 0.1543, accuracy : 94.65\n",
      "Epoch :  74, training loss : 0.1548, training accuracy : 94.66, validation loss : 0.2933, validation accuracy : 91.24\n",
      "\n",
      "Epoch: 75\n",
      "iteration :  50, loss : 0.1483, accuracy : 95.12\n",
      "iteration : 100, loss : 0.1459, accuracy : 95.09\n",
      "iteration : 150, loss : 0.1509, accuracy : 94.76\n",
      "iteration : 200, loss : 0.1544, accuracy : 94.59\n",
      "iteration : 250, loss : 0.1559, accuracy : 94.56\n",
      "iteration : 300, loss : 0.1558, accuracy : 94.52\n",
      "Epoch :  75, training loss : 0.1560, training accuracy : 94.53, validation loss : 0.2970, validation accuracy : 91.25\n",
      "\n",
      "Epoch: 76\n",
      "iteration :  50, loss : 0.1644, accuracy : 94.47\n",
      "iteration : 100, loss : 0.1541, accuracy : 94.79\n",
      "iteration : 150, loss : 0.1552, accuracy : 94.69\n",
      "iteration : 200, loss : 0.1538, accuracy : 94.77\n",
      "iteration : 250, loss : 0.1531, accuracy : 94.79\n",
      "iteration : 300, loss : 0.1543, accuracy : 94.76\n",
      "Epoch :  76, training loss : 0.1541, training accuracy : 94.77, validation loss : 0.2795, validation accuracy : 91.58\n",
      "\n",
      "Epoch: 77\n",
      "iteration :  50, loss : 0.1399, accuracy : 95.33\n",
      "iteration : 100, loss : 0.1402, accuracy : 95.27\n",
      "iteration : 150, loss : 0.1456, accuracy : 95.03\n",
      "iteration : 200, loss : 0.1461, accuracy : 95.01\n",
      "iteration : 250, loss : 0.1495, accuracy : 94.86\n",
      "iteration : 300, loss : 0.1506, accuracy : 94.81\n",
      "Epoch :  77, training loss : 0.1516, training accuracy : 94.78, validation loss : 0.3060, validation accuracy : 91.15\n",
      "\n",
      "Epoch: 78\n",
      "iteration :  50, loss : 0.1444, accuracy : 94.95\n",
      "iteration : 100, loss : 0.1483, accuracy : 94.86\n",
      "iteration : 150, loss : 0.1477, accuracy : 94.83\n",
      "iteration : 200, loss : 0.1499, accuracy : 94.70\n",
      "iteration : 250, loss : 0.1514, accuracy : 94.61\n",
      "iteration : 300, loss : 0.1534, accuracy : 94.54\n",
      "Epoch :  78, training loss : 0.1533, training accuracy : 94.56, validation loss : 0.2841, validation accuracy : 91.36\n",
      "\n",
      "Epoch: 79\n",
      "iteration :  50, loss : 0.1553, accuracy : 94.86\n",
      "iteration : 100, loss : 0.1532, accuracy : 94.80\n",
      "iteration : 150, loss : 0.1601, accuracy : 94.62\n",
      "iteration : 200, loss : 0.1616, accuracy : 94.52\n",
      "iteration : 250, loss : 0.1621, accuracy : 94.41\n",
      "iteration : 300, loss : 0.1623, accuracy : 94.41\n",
      "Epoch :  79, training loss : 0.1632, training accuracy : 94.40, validation loss : 0.2691, validation accuracy : 91.87\n",
      "\n",
      "Epoch: 80\n",
      "iteration :  50, loss : 0.1473, accuracy : 94.88\n",
      "iteration : 100, loss : 0.1519, accuracy : 94.61\n",
      "iteration : 150, loss : 0.1485, accuracy : 94.83\n",
      "iteration : 200, loss : 0.1496, accuracy : 94.83\n",
      "iteration : 250, loss : 0.1495, accuracy : 94.84\n",
      "iteration : 300, loss : 0.1478, accuracy : 94.94\n",
      "Epoch :  80, training loss : 0.1480, training accuracy : 94.92, validation loss : 0.3045, validation accuracy : 91.51\n",
      "\n",
      "Epoch: 81\n",
      "iteration :  50, loss : 0.1523, accuracy : 94.84\n",
      "iteration : 100, loss : 0.1437, accuracy : 94.98\n",
      "iteration : 150, loss : 0.1425, accuracy : 95.03\n",
      "iteration : 200, loss : 0.1442, accuracy : 94.97\n",
      "iteration : 250, loss : 0.1454, accuracy : 94.94\n",
      "iteration : 300, loss : 0.1482, accuracy : 94.83\n",
      "Epoch :  81, training loss : 0.1485, training accuracy : 94.80, validation loss : 0.2802, validation accuracy : 92.03\n",
      "\n",
      "Epoch: 82\n",
      "iteration :  50, loss : 0.1323, accuracy : 95.52\n",
      "iteration : 100, loss : 0.1429, accuracy : 95.03\n",
      "iteration : 150, loss : 0.1494, accuracy : 94.71\n",
      "iteration : 200, loss : 0.1469, accuracy : 94.86\n",
      "iteration : 250, loss : 0.1457, accuracy : 94.91\n",
      "iteration : 300, loss : 0.1468, accuracy : 94.89\n",
      "Epoch :  82, training loss : 0.1460, training accuracy : 94.92, validation loss : 0.3083, validation accuracy : 91.15\n",
      "\n",
      "Epoch: 83\n",
      "iteration :  50, loss : 0.1405, accuracy : 95.22\n",
      "iteration : 100, loss : 0.1419, accuracy : 95.20\n",
      "iteration : 150, loss : 0.1381, accuracy : 95.30\n",
      "iteration : 200, loss : 0.1422, accuracy : 95.20\n",
      "iteration : 250, loss : 0.1449, accuracy : 95.04\n",
      "iteration : 300, loss : 0.1459, accuracy : 95.04\n",
      "Epoch :  83, training loss : 0.1456, training accuracy : 95.05, validation loss : 0.3307, validation accuracy : 90.74\n",
      "\n",
      "Epoch: 84\n",
      "iteration :  50, loss : 0.1444, accuracy : 95.03\n",
      "iteration : 100, loss : 0.1371, accuracy : 95.20\n",
      "iteration : 150, loss : 0.1360, accuracy : 95.32\n",
      "iteration : 200, loss : 0.1372, accuracy : 95.26\n",
      "iteration : 250, loss : 0.1402, accuracy : 95.19\n",
      "iteration : 300, loss : 0.1415, accuracy : 95.13\n",
      "Epoch :  84, training loss : 0.1420, training accuracy : 95.11, validation loss : 0.2775, validation accuracy : 92.11\n",
      "\n",
      "Epoch: 85\n",
      "iteration :  50, loss : 0.1394, accuracy : 95.41\n",
      "iteration : 100, loss : 0.1378, accuracy : 95.40\n",
      "iteration : 150, loss : 0.1391, accuracy : 95.32\n",
      "iteration : 200, loss : 0.1402, accuracy : 95.21\n",
      "iteration : 250, loss : 0.1400, accuracy : 95.18\n",
      "iteration : 300, loss : 0.1423, accuracy : 95.12\n",
      "Epoch :  85, training loss : 0.1425, training accuracy : 95.12, validation loss : 0.2727, validation accuracy : 92.22\n",
      "\n",
      "Epoch: 86\n",
      "iteration :  50, loss : 0.1543, accuracy : 94.75\n",
      "iteration : 100, loss : 0.1465, accuracy : 95.06\n",
      "iteration : 150, loss : 0.1419, accuracy : 95.23\n",
      "iteration : 200, loss : 0.1429, accuracy : 95.16\n",
      "iteration : 250, loss : 0.1446, accuracy : 95.08\n",
      "iteration : 300, loss : 0.1451, accuracy : 95.08\n",
      "Epoch :  86, training loss : 0.1460, training accuracy : 95.06, validation loss : 0.2704, validation accuracy : 91.96\n",
      "\n",
      "Epoch: 87\n",
      "iteration :  50, loss : 0.1411, accuracy : 95.14\n",
      "iteration : 100, loss : 0.1381, accuracy : 95.23\n",
      "iteration : 150, loss : 0.1413, accuracy : 95.23\n",
      "iteration : 200, loss : 0.1412, accuracy : 95.19\n",
      "iteration : 250, loss : 0.1400, accuracy : 95.24\n",
      "iteration : 300, loss : 0.1413, accuracy : 95.22\n",
      "Epoch :  87, training loss : 0.1420, training accuracy : 95.19, validation loss : 0.2748, validation accuracy : 91.72\n",
      "\n",
      "Epoch: 88\n",
      "iteration :  50, loss : 0.1287, accuracy : 95.53\n",
      "iteration : 100, loss : 0.1306, accuracy : 95.55\n",
      "iteration : 150, loss : 0.1329, accuracy : 95.45\n",
      "iteration : 200, loss : 0.1343, accuracy : 95.35\n",
      "iteration : 250, loss : 0.1376, accuracy : 95.24\n",
      "iteration : 300, loss : 0.1399, accuracy : 95.20\n",
      "Epoch :  88, training loss : 0.1411, training accuracy : 95.16, validation loss : 0.2858, validation accuracy : 92.10\n",
      "\n",
      "Epoch: 89\n",
      "iteration :  50, loss : 0.1361, accuracy : 95.55\n",
      "iteration : 100, loss : 0.1344, accuracy : 95.56\n",
      "iteration : 150, loss : 0.1387, accuracy : 95.44\n",
      "iteration : 200, loss : 0.1423, accuracy : 95.30\n",
      "iteration : 250, loss : 0.1436, accuracy : 95.20\n",
      "iteration : 300, loss : 0.1443, accuracy : 95.14\n",
      "Epoch :  89, training loss : 0.1449, training accuracy : 95.09, validation loss : 0.2857, validation accuracy : 91.83\n",
      "\n",
      "Epoch: 90\n",
      "iteration :  50, loss : 0.1354, accuracy : 95.45\n",
      "iteration : 100, loss : 0.1353, accuracy : 95.34\n",
      "iteration : 150, loss : 0.1315, accuracy : 95.49\n",
      "iteration : 200, loss : 0.1331, accuracy : 95.46\n",
      "iteration : 250, loss : 0.1376, accuracy : 95.29\n",
      "iteration : 300, loss : 0.1386, accuracy : 95.22\n",
      "Epoch :  90, training loss : 0.1386, training accuracy : 95.23, validation loss : 0.2612, validation accuracy : 92.23\n",
      "\n",
      "Epoch: 91\n",
      "iteration :  50, loss : 0.1426, accuracy : 95.14\n",
      "iteration : 100, loss : 0.1470, accuracy : 94.88\n",
      "iteration : 150, loss : 0.1414, accuracy : 95.11\n",
      "iteration : 200, loss : 0.1398, accuracy : 95.09\n",
      "iteration : 250, loss : 0.1399, accuracy : 95.06\n",
      "iteration : 300, loss : 0.1411, accuracy : 95.03\n",
      "Epoch :  91, training loss : 0.1401, training accuracy : 95.08, validation loss : 0.2453, validation accuracy : 92.43\n",
      "\n",
      "Epoch: 92\n",
      "iteration :  50, loss : 0.1296, accuracy : 95.20\n",
      "iteration : 100, loss : 0.1309, accuracy : 95.44\n",
      "iteration : 150, loss : 0.1311, accuracy : 95.54\n",
      "iteration : 200, loss : 0.1305, accuracy : 95.53\n",
      "iteration : 250, loss : 0.1315, accuracy : 95.48\n",
      "iteration : 300, loss : 0.1319, accuracy : 95.48\n",
      "Epoch :  92, training loss : 0.1323, training accuracy : 95.45, validation loss : 0.3031, validation accuracy : 91.40\n",
      "\n",
      "Epoch: 93\n",
      "iteration :  50, loss : 0.1331, accuracy : 95.41\n",
      "iteration : 100, loss : 0.1371, accuracy : 95.30\n",
      "iteration : 150, loss : 0.1407, accuracy : 95.18\n",
      "iteration : 200, loss : 0.1372, accuracy : 95.33\n",
      "iteration : 250, loss : 0.1389, accuracy : 95.26\n",
      "iteration : 300, loss : 0.1407, accuracy : 95.17\n",
      "Epoch :  93, training loss : 0.1413, training accuracy : 95.17, validation loss : 0.2586, validation accuracy : 92.16\n",
      "\n",
      "Epoch: 94\n",
      "iteration :  50, loss : 0.1317, accuracy : 95.33\n",
      "iteration : 100, loss : 0.1361, accuracy : 95.23\n",
      "iteration : 150, loss : 0.1332, accuracy : 95.39\n",
      "iteration : 200, loss : 0.1321, accuracy : 95.48\n",
      "iteration : 250, loss : 0.1339, accuracy : 95.44\n",
      "iteration : 300, loss : 0.1332, accuracy : 95.44\n",
      "Epoch :  94, training loss : 0.1334, training accuracy : 95.42, validation loss : 0.3105, validation accuracy : 91.23\n",
      "\n",
      "Epoch: 95\n",
      "iteration :  50, loss : 0.1310, accuracy : 95.27\n",
      "iteration : 100, loss : 0.1294, accuracy : 95.48\n",
      "iteration : 150, loss : 0.1326, accuracy : 95.42\n",
      "iteration : 200, loss : 0.1333, accuracy : 95.36\n",
      "iteration : 250, loss : 0.1359, accuracy : 95.29\n",
      "iteration : 300, loss : 0.1350, accuracy : 95.32\n",
      "Epoch :  95, training loss : 0.1345, training accuracy : 95.35, validation loss : 0.2698, validation accuracy : 92.39\n",
      "\n",
      "Epoch: 96\n",
      "iteration :  50, loss : 0.1409, accuracy : 95.16\n",
      "iteration : 100, loss : 0.1344, accuracy : 95.27\n",
      "iteration : 150, loss : 0.1352, accuracy : 95.28\n",
      "iteration : 200, loss : 0.1391, accuracy : 95.15\n",
      "iteration : 250, loss : 0.1362, accuracy : 95.28\n",
      "iteration : 300, loss : 0.1356, accuracy : 95.31\n",
      "Epoch :  96, training loss : 0.1360, training accuracy : 95.28, validation loss : 0.2842, validation accuracy : 91.77\n",
      "\n",
      "Epoch: 97\n",
      "iteration :  50, loss : 0.1407, accuracy : 95.09\n",
      "iteration : 100, loss : 0.1306, accuracy : 95.54\n",
      "iteration : 150, loss : 0.1334, accuracy : 95.43\n",
      "iteration : 200, loss : 0.1315, accuracy : 95.54\n",
      "iteration : 250, loss : 0.1333, accuracy : 95.49\n",
      "iteration : 300, loss : 0.1331, accuracy : 95.50\n",
      "Epoch :  97, training loss : 0.1334, training accuracy : 95.46, validation loss : 0.2637, validation accuracy : 92.35\n",
      "\n",
      "Epoch: 98\n",
      "iteration :  50, loss : 0.1367, accuracy : 95.09\n",
      "iteration : 100, loss : 0.1298, accuracy : 95.41\n",
      "iteration : 150, loss : 0.1305, accuracy : 95.41\n",
      "iteration : 200, loss : 0.1310, accuracy : 95.43\n",
      "iteration : 250, loss : 0.1297, accuracy : 95.44\n",
      "iteration : 300, loss : 0.1330, accuracy : 95.38\n",
      "Epoch :  98, training loss : 0.1340, training accuracy : 95.35, validation loss : 0.2704, validation accuracy : 92.15\n",
      "\n",
      "Epoch: 99\n",
      "iteration :  50, loss : 0.1359, accuracy : 95.27\n",
      "iteration : 100, loss : 0.1338, accuracy : 95.38\n",
      "iteration : 150, loss : 0.1326, accuracy : 95.38\n",
      "iteration : 200, loss : 0.1293, accuracy : 95.50\n",
      "iteration : 250, loss : 0.1308, accuracy : 95.43\n",
      "iteration : 300, loss : 0.1310, accuracy : 95.47\n",
      "Epoch :  99, training loss : 0.1310, training accuracy : 95.46, validation loss : 0.2558, validation accuracy : 92.51\n",
      "\n",
      "Epoch: 100\n",
      "iteration :  50, loss : 0.1252, accuracy : 95.78\n",
      "iteration : 100, loss : 0.1177, accuracy : 95.98\n",
      "iteration : 150, loss : 0.1197, accuracy : 95.77\n",
      "iteration : 200, loss : 0.1241, accuracy : 95.67\n",
      "iteration : 250, loss : 0.1251, accuracy : 95.62\n",
      "iteration : 300, loss : 0.1278, accuracy : 95.56\n",
      "Epoch : 100, training loss : 0.1286, training accuracy : 95.53, validation loss : 0.2694, validation accuracy : 92.31\n",
      "\n",
      "Epoch: 101\n",
      "iteration :  50, loss : 0.1376, accuracy : 95.23\n",
      "iteration : 100, loss : 0.1335, accuracy : 95.43\n",
      "iteration : 150, loss : 0.1313, accuracy : 95.53\n",
      "iteration : 200, loss : 0.1325, accuracy : 95.49\n",
      "iteration : 250, loss : 0.1325, accuracy : 95.43\n",
      "iteration : 300, loss : 0.1327, accuracy : 95.44\n",
      "Epoch : 101, training loss : 0.1332, training accuracy : 95.41, validation loss : 0.2538, validation accuracy : 92.32\n",
      "\n",
      "Epoch: 102\n",
      "iteration :  50, loss : 0.1300, accuracy : 95.50\n",
      "iteration : 100, loss : 0.1304, accuracy : 95.43\n",
      "iteration : 150, loss : 0.1312, accuracy : 95.43\n",
      "iteration : 200, loss : 0.1308, accuracy : 95.47\n",
      "iteration : 250, loss : 0.1299, accuracy : 95.53\n",
      "iteration : 300, loss : 0.1308, accuracy : 95.48\n",
      "Epoch : 102, training loss : 0.1313, training accuracy : 95.47, validation loss : 0.2732, validation accuracy : 92.29\n",
      "\n",
      "Epoch: 103\n",
      "iteration :  50, loss : 0.1343, accuracy : 95.25\n",
      "iteration : 100, loss : 0.1359, accuracy : 95.30\n",
      "iteration : 150, loss : 0.1305, accuracy : 95.46\n",
      "iteration : 200, loss : 0.1320, accuracy : 95.43\n",
      "iteration : 250, loss : 0.1312, accuracy : 95.45\n",
      "iteration : 300, loss : 0.1291, accuracy : 95.52\n",
      "Epoch : 103, training loss : 0.1290, training accuracy : 95.53, validation loss : 0.2845, validation accuracy : 91.56\n",
      "\n",
      "Epoch: 104\n",
      "iteration :  50, loss : 0.1172, accuracy : 96.09\n",
      "iteration : 100, loss : 0.1227, accuracy : 95.84\n",
      "iteration : 150, loss : 0.1194, accuracy : 96.01\n",
      "iteration : 200, loss : 0.1204, accuracy : 95.86\n",
      "iteration : 250, loss : 0.1242, accuracy : 95.74\n",
      "iteration : 300, loss : 0.1254, accuracy : 95.67\n",
      "Epoch : 104, training loss : 0.1256, training accuracy : 95.66, validation loss : 0.2565, validation accuracy : 92.49\n",
      "\n",
      "Epoch: 105\n",
      "iteration :  50, loss : 0.1306, accuracy : 95.38\n",
      "iteration : 100, loss : 0.1272, accuracy : 95.59\n",
      "iteration : 150, loss : 0.1262, accuracy : 95.65\n",
      "iteration : 200, loss : 0.1256, accuracy : 95.69\n",
      "iteration : 250, loss : 0.1249, accuracy : 95.71\n",
      "iteration : 300, loss : 0.1249, accuracy : 95.71\n",
      "Epoch : 105, training loss : 0.1251, training accuracy : 95.69, validation loss : 0.2870, validation accuracy : 91.92\n",
      "\n",
      "Epoch: 106\n",
      "iteration :  50, loss : 0.1240, accuracy : 95.95\n",
      "iteration : 100, loss : 0.1215, accuracy : 95.91\n",
      "iteration : 150, loss : 0.1283, accuracy : 95.68\n",
      "iteration : 200, loss : 0.1264, accuracy : 95.80\n",
      "iteration : 250, loss : 0.1281, accuracy : 95.68\n",
      "iteration : 300, loss : 0.1285, accuracy : 95.67\n",
      "Epoch : 106, training loss : 0.1282, training accuracy : 95.67, validation loss : 0.2695, validation accuracy : 92.01\n",
      "\n",
      "Epoch: 107\n",
      "iteration :  50, loss : 0.1242, accuracy : 95.72\n",
      "iteration : 100, loss : 0.1244, accuracy : 95.66\n",
      "iteration : 150, loss : 0.1303, accuracy : 95.42\n",
      "iteration : 200, loss : 0.1274, accuracy : 95.59\n",
      "iteration : 250, loss : 0.1272, accuracy : 95.63\n",
      "iteration : 300, loss : 0.1264, accuracy : 95.66\n",
      "Epoch : 107, training loss : 0.1276, training accuracy : 95.61, validation loss : 0.2841, validation accuracy : 91.49\n",
      "\n",
      "Epoch: 108\n",
      "iteration :  50, loss : 0.1237, accuracy : 95.95\n",
      "iteration : 100, loss : 0.1260, accuracy : 95.80\n",
      "iteration : 150, loss : 0.1307, accuracy : 95.58\n",
      "iteration : 200, loss : 0.1281, accuracy : 95.74\n",
      "iteration : 250, loss : 0.1266, accuracy : 95.77\n",
      "iteration : 300, loss : 0.1282, accuracy : 95.68\n",
      "Epoch : 108, training loss : 0.1273, training accuracy : 95.69, validation loss : 0.2625, validation accuracy : 92.15\n",
      "\n",
      "Epoch: 109\n",
      "iteration :  50, loss : 0.1232, accuracy : 96.06\n",
      "iteration : 100, loss : 0.1215, accuracy : 96.05\n",
      "iteration : 150, loss : 0.1234, accuracy : 95.88\n",
      "iteration : 200, loss : 0.1247, accuracy : 95.88\n",
      "iteration : 250, loss : 0.1224, accuracy : 95.90\n",
      "iteration : 300, loss : 0.1237, accuracy : 95.85\n",
      "Epoch : 109, training loss : 0.1231, training accuracy : 95.87, validation loss : 0.2550, validation accuracy : 92.70\n",
      "\n",
      "Epoch: 110\n",
      "iteration :  50, loss : 0.1135, accuracy : 95.84\n",
      "iteration : 100, loss : 0.1142, accuracy : 95.95\n",
      "iteration : 150, loss : 0.1203, accuracy : 95.74\n",
      "iteration : 200, loss : 0.1186, accuracy : 95.87\n",
      "iteration : 250, loss : 0.1202, accuracy : 95.83\n",
      "iteration : 300, loss : 0.1212, accuracy : 95.78\n",
      "Epoch : 110, training loss : 0.1221, training accuracy : 95.75, validation loss : 0.2719, validation accuracy : 92.36\n",
      "\n",
      "Epoch: 111\n",
      "iteration :  50, loss : 0.1193, accuracy : 96.11\n",
      "iteration : 100, loss : 0.1191, accuracy : 96.15\n",
      "iteration : 150, loss : 0.1195, accuracy : 96.05\n",
      "iteration : 200, loss : 0.1190, accuracy : 96.05\n",
      "iteration : 250, loss : 0.1192, accuracy : 95.98\n",
      "iteration : 300, loss : 0.1192, accuracy : 95.97\n",
      "Epoch : 111, training loss : 0.1192, training accuracy : 95.96, validation loss : 0.2590, validation accuracy : 92.61\n",
      "\n",
      "Epoch: 112\n",
      "iteration :  50, loss : 0.1259, accuracy : 95.78\n",
      "iteration : 100, loss : 0.1297, accuracy : 95.62\n",
      "iteration : 150, loss : 0.1223, accuracy : 95.88\n",
      "iteration : 200, loss : 0.1201, accuracy : 95.88\n",
      "iteration : 250, loss : 0.1216, accuracy : 95.79\n",
      "iteration : 300, loss : 0.1209, accuracy : 95.81\n",
      "Epoch : 112, training loss : 0.1210, training accuracy : 95.81, validation loss : 0.2473, validation accuracy : 92.61\n",
      "\n",
      "Epoch: 113\n",
      "iteration :  50, loss : 0.1130, accuracy : 96.16\n",
      "iteration : 100, loss : 0.1182, accuracy : 95.97\n",
      "iteration : 150, loss : 0.1172, accuracy : 96.05\n",
      "iteration : 200, loss : 0.1181, accuracy : 96.02\n",
      "iteration : 250, loss : 0.1193, accuracy : 95.92\n",
      "iteration : 300, loss : 0.1199, accuracy : 95.90\n",
      "Epoch : 113, training loss : 0.1198, training accuracy : 95.90, validation loss : 0.2504, validation accuracy : 92.73\n",
      "\n",
      "Epoch: 114\n",
      "iteration :  50, loss : 0.1187, accuracy : 95.77\n",
      "iteration : 100, loss : 0.1137, accuracy : 95.91\n",
      "iteration : 150, loss : 0.1179, accuracy : 95.86\n",
      "iteration : 200, loss : 0.1189, accuracy : 95.86\n",
      "iteration : 250, loss : 0.1186, accuracy : 95.90\n",
      "iteration : 300, loss : 0.1211, accuracy : 95.82\n",
      "Epoch : 114, training loss : 0.1216, training accuracy : 95.80, validation loss : 0.2608, validation accuracy : 92.31\n",
      "\n",
      "Epoch: 115\n",
      "iteration :  50, loss : 0.1160, accuracy : 96.12\n",
      "iteration : 100, loss : 0.1152, accuracy : 96.12\n",
      "iteration : 150, loss : 0.1144, accuracy : 96.19\n",
      "iteration : 200, loss : 0.1140, accuracy : 96.22\n",
      "iteration : 250, loss : 0.1153, accuracy : 96.11\n",
      "iteration : 300, loss : 0.1192, accuracy : 95.95\n",
      "Epoch : 115, training loss : 0.1201, training accuracy : 95.92, validation loss : 0.2838, validation accuracy : 91.81\n",
      "\n",
      "Epoch: 116\n",
      "iteration :  50, loss : 0.1206, accuracy : 95.83\n",
      "iteration : 100, loss : 0.1185, accuracy : 95.91\n",
      "iteration : 150, loss : 0.1193, accuracy : 95.90\n",
      "iteration : 200, loss : 0.1199, accuracy : 95.82\n",
      "iteration : 250, loss : 0.1186, accuracy : 95.86\n",
      "iteration : 300, loss : 0.1197, accuracy : 95.83\n",
      "Epoch : 116, training loss : 0.1196, training accuracy : 95.83, validation loss : 0.3082, validation accuracy : 91.72\n",
      "\n",
      "Epoch: 117\n",
      "iteration :  50, loss : 0.1170, accuracy : 96.05\n",
      "iteration : 100, loss : 0.1153, accuracy : 96.04\n",
      "iteration : 150, loss : 0.1121, accuracy : 96.21\n",
      "iteration : 200, loss : 0.1153, accuracy : 96.14\n",
      "iteration : 250, loss : 0.1162, accuracy : 96.09\n",
      "iteration : 300, loss : 0.1178, accuracy : 96.02\n",
      "Epoch : 117, training loss : 0.1185, training accuracy : 96.02, validation loss : 0.3337, validation accuracy : 91.19\n",
      "\n",
      "Epoch: 118\n",
      "iteration :  50, loss : 0.1114, accuracy : 96.16\n",
      "iteration : 100, loss : 0.1157, accuracy : 96.12\n",
      "iteration : 150, loss : 0.1213, accuracy : 95.89\n",
      "iteration : 200, loss : 0.1195, accuracy : 95.93\n",
      "iteration : 250, loss : 0.1202, accuracy : 95.94\n",
      "iteration : 300, loss : 0.1197, accuracy : 95.98\n",
      "Epoch : 118, training loss : 0.1197, training accuracy : 95.99, validation loss : 0.2298, validation accuracy : 93.38\n",
      "\n",
      "Epoch: 119\n",
      "iteration :  50, loss : 0.1013, accuracy : 96.58\n",
      "iteration : 100, loss : 0.1127, accuracy : 96.09\n",
      "iteration : 150, loss : 0.1148, accuracy : 96.07\n",
      "iteration : 200, loss : 0.1162, accuracy : 95.97\n",
      "iteration : 250, loss : 0.1150, accuracy : 96.07\n",
      "iteration : 300, loss : 0.1157, accuracy : 96.07\n",
      "Epoch : 119, training loss : 0.1159, training accuracy : 96.06, validation loss : 0.2741, validation accuracy : 92.25\n",
      "\n",
      "Epoch: 120\n",
      "iteration :  50, loss : 0.1144, accuracy : 95.94\n",
      "iteration : 100, loss : 0.1089, accuracy : 96.25\n",
      "iteration : 150, loss : 0.1110, accuracy : 96.21\n",
      "iteration : 200, loss : 0.1148, accuracy : 96.13\n",
      "iteration : 250, loss : 0.1166, accuracy : 96.08\n",
      "iteration : 300, loss : 0.1167, accuracy : 96.08\n",
      "Epoch : 120, training loss : 0.1162, training accuracy : 96.07, validation loss : 0.2441, validation accuracy : 92.64\n",
      "\n",
      "Epoch: 121\n",
      "iteration :  50, loss : 0.1074, accuracy : 96.50\n",
      "iteration : 100, loss : 0.1088, accuracy : 96.55\n",
      "iteration : 150, loss : 0.1121, accuracy : 96.31\n",
      "iteration : 200, loss : 0.1120, accuracy : 96.29\n",
      "iteration : 250, loss : 0.1123, accuracy : 96.25\n",
      "iteration : 300, loss : 0.1125, accuracy : 96.21\n",
      "Epoch : 121, training loss : 0.1132, training accuracy : 96.20, validation loss : 0.2632, validation accuracy : 92.26\n",
      "\n",
      "Epoch: 122\n",
      "iteration :  50, loss : 0.1081, accuracy : 96.34\n",
      "iteration : 100, loss : 0.1061, accuracy : 96.45\n",
      "iteration : 150, loss : 0.1084, accuracy : 96.40\n",
      "iteration : 200, loss : 0.1113, accuracy : 96.26\n",
      "iteration : 250, loss : 0.1123, accuracy : 96.18\n",
      "iteration : 300, loss : 0.1154, accuracy : 96.06\n",
      "Epoch : 122, training loss : 0.1157, training accuracy : 96.04, validation loss : 0.2598, validation accuracy : 92.26\n",
      "\n",
      "Epoch: 123\n",
      "iteration :  50, loss : 0.1134, accuracy : 96.20\n",
      "iteration : 100, loss : 0.1155, accuracy : 96.10\n",
      "iteration : 150, loss : 0.1159, accuracy : 96.12\n",
      "iteration : 200, loss : 0.1175, accuracy : 96.05\n",
      "iteration : 250, loss : 0.1172, accuracy : 96.09\n",
      "iteration : 300, loss : 0.1150, accuracy : 96.18\n",
      "Epoch : 123, training loss : 0.1154, training accuracy : 96.17, validation loss : 0.2886, validation accuracy : 92.02\n",
      "\n",
      "Epoch: 124\n",
      "iteration :  50, loss : 0.1073, accuracy : 96.42\n",
      "iteration : 100, loss : 0.1096, accuracy : 96.27\n",
      "iteration : 150, loss : 0.1144, accuracy : 96.15\n",
      "iteration : 200, loss : 0.1140, accuracy : 96.10\n",
      "iteration : 250, loss : 0.1168, accuracy : 95.97\n",
      "iteration : 300, loss : 0.1170, accuracy : 95.95\n",
      "Epoch : 124, training loss : 0.1175, training accuracy : 95.94, validation loss : 0.2577, validation accuracy : 92.43\n",
      "\n",
      "Epoch: 125\n",
      "iteration :  50, loss : 0.1127, accuracy : 96.02\n",
      "iteration : 100, loss : 0.1121, accuracy : 96.08\n",
      "iteration : 150, loss : 0.1057, accuracy : 96.34\n",
      "iteration : 200, loss : 0.1111, accuracy : 96.17\n",
      "iteration : 250, loss : 0.1110, accuracy : 96.19\n",
      "iteration : 300, loss : 0.1099, accuracy : 96.22\n",
      "Epoch : 125, training loss : 0.1103, training accuracy : 96.20, validation loss : 0.2454, validation accuracy : 92.66\n",
      "\n",
      "Epoch: 126\n",
      "iteration :  50, loss : 0.1125, accuracy : 96.20\n",
      "iteration : 100, loss : 0.1097, accuracy : 96.36\n",
      "iteration : 150, loss : 0.1120, accuracy : 96.26\n",
      "iteration : 200, loss : 0.1121, accuracy : 96.20\n",
      "iteration : 250, loss : 0.1113, accuracy : 96.20\n",
      "iteration : 300, loss : 0.1125, accuracy : 96.20\n",
      "Epoch : 126, training loss : 0.1123, training accuracy : 96.20, validation loss : 0.2706, validation accuracy : 92.30\n",
      "\n",
      "Epoch: 127\n",
      "iteration :  50, loss : 0.1083, accuracy : 96.39\n",
      "iteration : 100, loss : 0.1067, accuracy : 96.52\n",
      "iteration : 150, loss : 0.1093, accuracy : 96.39\n",
      "iteration : 200, loss : 0.1102, accuracy : 96.31\n",
      "iteration : 250, loss : 0.1109, accuracy : 96.25\n",
      "iteration : 300, loss : 0.1113, accuracy : 96.24\n",
      "Epoch : 127, training loss : 0.1113, training accuracy : 96.24, validation loss : 0.2686, validation accuracy : 92.32\n",
      "\n",
      "Epoch: 128\n",
      "iteration :  50, loss : 0.1042, accuracy : 96.31\n",
      "iteration : 100, loss : 0.1053, accuracy : 96.38\n",
      "iteration : 150, loss : 0.1037, accuracy : 96.48\n",
      "iteration : 200, loss : 0.1067, accuracy : 96.39\n",
      "iteration : 250, loss : 0.1060, accuracy : 96.47\n",
      "iteration : 300, loss : 0.1066, accuracy : 96.43\n",
      "Epoch : 128, training loss : 0.1064, training accuracy : 96.43, validation loss : 0.2556, validation accuracy : 92.59\n",
      "\n",
      "Epoch: 129\n",
      "iteration :  50, loss : 0.1042, accuracy : 96.42\n",
      "iteration : 100, loss : 0.1075, accuracy : 96.38\n",
      "iteration : 150, loss : 0.1101, accuracy : 96.34\n",
      "iteration : 200, loss : 0.1115, accuracy : 96.24\n",
      "iteration : 250, loss : 0.1112, accuracy : 96.24\n",
      "iteration : 300, loss : 0.1126, accuracy : 96.23\n",
      "Epoch : 129, training loss : 0.1120, training accuracy : 96.25, validation loss : 0.2370, validation accuracy : 92.76\n",
      "\n",
      "Epoch: 130\n",
      "iteration :  50, loss : 0.0945, accuracy : 97.03\n",
      "iteration : 100, loss : 0.0997, accuracy : 96.74\n",
      "iteration : 150, loss : 0.1040, accuracy : 96.49\n",
      "iteration : 200, loss : 0.1090, accuracy : 96.32\n",
      "iteration : 250, loss : 0.1072, accuracy : 96.38\n",
      "iteration : 300, loss : 0.1078, accuracy : 96.35\n",
      "Epoch : 130, training loss : 0.1086, training accuracy : 96.33, validation loss : 0.2609, validation accuracy : 92.19\n",
      "\n",
      "Epoch: 131\n",
      "iteration :  50, loss : 0.1072, accuracy : 96.11\n",
      "iteration : 100, loss : 0.1059, accuracy : 96.34\n",
      "iteration : 150, loss : 0.1084, accuracy : 96.23\n",
      "iteration : 200, loss : 0.1077, accuracy : 96.29\n",
      "iteration : 250, loss : 0.1082, accuracy : 96.29\n",
      "iteration : 300, loss : 0.1084, accuracy : 96.25\n",
      "Epoch : 131, training loss : 0.1089, training accuracy : 96.25, validation loss : 0.2756, validation accuracy : 91.74\n",
      "\n",
      "Epoch: 132\n",
      "iteration :  50, loss : 0.1004, accuracy : 96.70\n",
      "iteration : 100, loss : 0.0993, accuracy : 96.68\n",
      "iteration : 150, loss : 0.1022, accuracy : 96.59\n",
      "iteration : 200, loss : 0.1022, accuracy : 96.59\n",
      "iteration : 250, loss : 0.1032, accuracy : 96.50\n",
      "iteration : 300, loss : 0.1053, accuracy : 96.43\n",
      "Epoch : 132, training loss : 0.1065, training accuracy : 96.41, validation loss : 0.2493, validation accuracy : 92.88\n",
      "\n",
      "Epoch: 133\n",
      "iteration :  50, loss : 0.1136, accuracy : 96.09\n",
      "iteration : 100, loss : 0.1081, accuracy : 96.31\n",
      "iteration : 150, loss : 0.1101, accuracy : 96.31\n",
      "iteration : 200, loss : 0.1067, accuracy : 96.48\n",
      "iteration : 250, loss : 0.1088, accuracy : 96.39\n",
      "iteration : 300, loss : 0.1089, accuracy : 96.36\n",
      "Epoch : 133, training loss : 0.1096, training accuracy : 96.33, validation loss : 0.2360, validation accuracy : 92.79\n",
      "\n",
      "Epoch: 134\n",
      "iteration :  50, loss : 0.1024, accuracy : 96.42\n",
      "iteration : 100, loss : 0.0983, accuracy : 96.62\n",
      "iteration : 150, loss : 0.1021, accuracy : 96.47\n",
      "iteration : 200, loss : 0.1025, accuracy : 96.45\n",
      "iteration : 250, loss : 0.1040, accuracy : 96.38\n",
      "iteration : 300, loss : 0.1038, accuracy : 96.40\n",
      "Epoch : 134, training loss : 0.1034, training accuracy : 96.42, validation loss : 0.2500, validation accuracy : 92.92\n",
      "\n",
      "Epoch: 135\n",
      "iteration :  50, loss : 0.1065, accuracy : 96.44\n",
      "iteration : 100, loss : 0.1051, accuracy : 96.50\n",
      "iteration : 150, loss : 0.1026, accuracy : 96.61\n",
      "iteration : 200, loss : 0.1025, accuracy : 96.60\n",
      "iteration : 250, loss : 0.1037, accuracy : 96.53\n",
      "iteration : 300, loss : 0.1031, accuracy : 96.55\n",
      "Epoch : 135, training loss : 0.1042, training accuracy : 96.53, validation loss : 0.2487, validation accuracy : 92.95\n",
      "\n",
      "Epoch: 136\n",
      "iteration :  50, loss : 0.1024, accuracy : 96.61\n",
      "iteration : 100, loss : 0.0992, accuracy : 96.70\n",
      "iteration : 150, loss : 0.1040, accuracy : 96.45\n",
      "iteration : 200, loss : 0.1035, accuracy : 96.48\n",
      "iteration : 250, loss : 0.1048, accuracy : 96.47\n",
      "iteration : 300, loss : 0.1053, accuracy : 96.39\n",
      "Epoch : 136, training loss : 0.1063, training accuracy : 96.34, validation loss : 0.2613, validation accuracy : 92.51\n",
      "\n",
      "Epoch: 137\n",
      "iteration :  50, loss : 0.1021, accuracy : 96.30\n",
      "iteration : 100, loss : 0.1014, accuracy : 96.55\n",
      "iteration : 150, loss : 0.1013, accuracy : 96.47\n",
      "iteration : 200, loss : 0.1002, accuracy : 96.58\n",
      "iteration : 250, loss : 0.1015, accuracy : 96.57\n",
      "iteration : 300, loss : 0.1026, accuracy : 96.55\n",
      "Epoch : 137, training loss : 0.1028, training accuracy : 96.52, validation loss : 0.2436, validation accuracy : 92.70\n",
      "\n",
      "Epoch: 138\n",
      "iteration :  50, loss : 0.0963, accuracy : 96.86\n",
      "iteration : 100, loss : 0.1040, accuracy : 96.59\n",
      "iteration : 150, loss : 0.1026, accuracy : 96.59\n",
      "iteration : 200, loss : 0.0997, accuracy : 96.68\n",
      "iteration : 250, loss : 0.1000, accuracy : 96.64\n",
      "iteration : 300, loss : 0.1019, accuracy : 96.58\n",
      "Epoch : 138, training loss : 0.1020, training accuracy : 96.57, validation loss : 0.2376, validation accuracy : 93.05\n",
      "\n",
      "Epoch: 139\n",
      "iteration :  50, loss : 0.0975, accuracy : 96.91\n",
      "iteration : 100, loss : 0.0993, accuracy : 96.79\n",
      "iteration : 150, loss : 0.0991, accuracy : 96.70\n",
      "iteration : 200, loss : 0.1006, accuracy : 96.60\n",
      "iteration : 250, loss : 0.1000, accuracy : 96.65\n",
      "iteration : 300, loss : 0.0993, accuracy : 96.66\n",
      "Epoch : 139, training loss : 0.0995, training accuracy : 96.66, validation loss : 0.2501, validation accuracy : 93.10\n",
      "\n",
      "Epoch: 140\n",
      "iteration :  50, loss : 0.1130, accuracy : 96.30\n",
      "iteration : 100, loss : 0.1103, accuracy : 96.30\n",
      "iteration : 150, loss : 0.1085, accuracy : 96.40\n",
      "iteration : 200, loss : 0.1074, accuracy : 96.43\n",
      "iteration : 250, loss : 0.1052, accuracy : 96.46\n",
      "iteration : 300, loss : 0.1062, accuracy : 96.43\n",
      "Epoch : 140, training loss : 0.1064, training accuracy : 96.40, validation loss : 0.2481, validation accuracy : 92.82\n",
      "\n",
      "Epoch: 141\n",
      "iteration :  50, loss : 0.0960, accuracy : 96.73\n",
      "iteration : 100, loss : 0.0970, accuracy : 96.76\n",
      "iteration : 150, loss : 0.0976, accuracy : 96.69\n",
      "iteration : 200, loss : 0.1000, accuracy : 96.61\n",
      "iteration : 250, loss : 0.1011, accuracy : 96.59\n",
      "iteration : 300, loss : 0.1037, accuracy : 96.51\n",
      "Epoch : 141, training loss : 0.1041, training accuracy : 96.50, validation loss : 0.2510, validation accuracy : 92.77\n",
      "\n",
      "Epoch: 142\n",
      "iteration :  50, loss : 0.0985, accuracy : 96.67\n",
      "iteration : 100, loss : 0.1002, accuracy : 96.65\n",
      "iteration : 150, loss : 0.0989, accuracy : 96.73\n",
      "iteration : 200, loss : 0.0976, accuracy : 96.75\n",
      "iteration : 250, loss : 0.0985, accuracy : 96.69\n",
      "iteration : 300, loss : 0.0972, accuracy : 96.73\n",
      "Epoch : 142, training loss : 0.0972, training accuracy : 96.72, validation loss : 0.2576, validation accuracy : 92.75\n",
      "\n",
      "Epoch: 143\n",
      "iteration :  50, loss : 0.0841, accuracy : 97.17\n",
      "iteration : 100, loss : 0.0918, accuracy : 96.84\n",
      "iteration : 150, loss : 0.0961, accuracy : 96.66\n",
      "iteration : 200, loss : 0.0975, accuracy : 96.66\n",
      "iteration : 250, loss : 0.0997, accuracy : 96.57\n",
      "iteration : 300, loss : 0.1005, accuracy : 96.55\n",
      "Epoch : 143, training loss : 0.1007, training accuracy : 96.54, validation loss : 0.2450, validation accuracy : 92.85\n",
      "\n",
      "Epoch: 144\n",
      "iteration :  50, loss : 0.0983, accuracy : 96.67\n",
      "iteration : 100, loss : 0.0998, accuracy : 96.67\n",
      "iteration : 150, loss : 0.0978, accuracy : 96.69\n",
      "iteration : 200, loss : 0.0971, accuracy : 96.71\n",
      "iteration : 250, loss : 0.0971, accuracy : 96.69\n",
      "iteration : 300, loss : 0.0977, accuracy : 96.68\n",
      "Epoch : 144, training loss : 0.0975, training accuracy : 96.69, validation loss : 0.2409, validation accuracy : 93.27\n",
      "\n",
      "Epoch: 145\n",
      "iteration :  50, loss : 0.0993, accuracy : 96.67\n",
      "iteration : 100, loss : 0.0930, accuracy : 96.91\n",
      "iteration : 150, loss : 0.0922, accuracy : 96.95\n",
      "iteration : 200, loss : 0.0943, accuracy : 96.91\n",
      "iteration : 250, loss : 0.0955, accuracy : 96.88\n",
      "iteration : 300, loss : 0.0961, accuracy : 96.87\n",
      "Epoch : 145, training loss : 0.0971, training accuracy : 96.85, validation loss : 0.2426, validation accuracy : 92.86\n",
      "\n",
      "Epoch: 146\n",
      "iteration :  50, loss : 0.0953, accuracy : 96.91\n",
      "iteration : 100, loss : 0.0897, accuracy : 96.93\n",
      "iteration : 150, loss : 0.0925, accuracy : 96.78\n",
      "iteration : 200, loss : 0.0931, accuracy : 96.77\n",
      "iteration : 250, loss : 0.0958, accuracy : 96.71\n",
      "iteration : 300, loss : 0.0956, accuracy : 96.74\n",
      "Epoch : 146, training loss : 0.0965, training accuracy : 96.71, validation loss : 0.3238, validation accuracy : 91.72\n",
      "\n",
      "Epoch: 147\n",
      "iteration :  50, loss : 0.0897, accuracy : 97.06\n",
      "iteration : 100, loss : 0.0930, accuracy : 96.91\n",
      "iteration : 150, loss : 0.0941, accuracy : 96.81\n",
      "iteration : 200, loss : 0.0968, accuracy : 96.70\n",
      "iteration : 250, loss : 0.0970, accuracy : 96.72\n",
      "iteration : 300, loss : 0.0969, accuracy : 96.72\n",
      "Epoch : 147, training loss : 0.0971, training accuracy : 96.72, validation loss : 0.2694, validation accuracy : 92.54\n",
      "\n",
      "Epoch: 148\n",
      "iteration :  50, loss : 0.1043, accuracy : 96.39\n",
      "iteration : 100, loss : 0.1012, accuracy : 96.58\n",
      "iteration : 150, loss : 0.1023, accuracy : 96.45\n",
      "iteration : 200, loss : 0.1000, accuracy : 96.56\n",
      "iteration : 250, loss : 0.1012, accuracy : 96.53\n",
      "iteration : 300, loss : 0.0997, accuracy : 96.57\n",
      "Epoch : 148, training loss : 0.0995, training accuracy : 96.58, validation loss : 0.2528, validation accuracy : 93.02\n",
      "\n",
      "Epoch: 149\n",
      "iteration :  50, loss : 0.0887, accuracy : 97.16\n",
      "iteration : 100, loss : 0.0880, accuracy : 97.02\n",
      "iteration : 150, loss : 0.0902, accuracy : 96.92\n",
      "iteration : 200, loss : 0.0896, accuracy : 96.99\n",
      "iteration : 250, loss : 0.0894, accuracy : 96.98\n",
      "iteration : 300, loss : 0.0900, accuracy : 96.95\n",
      "Epoch : 149, training loss : 0.0903, training accuracy : 96.93, validation loss : 0.2657, validation accuracy : 92.52\n",
      "\n",
      "Epoch: 150\n",
      "iteration :  50, loss : 0.1005, accuracy : 96.64\n",
      "iteration : 100, loss : 0.0915, accuracy : 96.90\n",
      "iteration : 150, loss : 0.0942, accuracy : 96.82\n",
      "iteration : 200, loss : 0.0948, accuracy : 96.77\n",
      "iteration : 250, loss : 0.0970, accuracy : 96.69\n",
      "iteration : 300, loss : 0.0971, accuracy : 96.68\n",
      "Epoch : 150, training loss : 0.0979, training accuracy : 96.66, validation loss : 0.2595, validation accuracy : 92.64\n",
      "\n",
      "Epoch: 151\n",
      "iteration :  50, loss : 0.0917, accuracy : 96.86\n",
      "iteration : 100, loss : 0.0938, accuracy : 96.86\n",
      "iteration : 150, loss : 0.0950, accuracy : 96.81\n",
      "iteration : 200, loss : 0.0969, accuracy : 96.76\n",
      "iteration : 250, loss : 0.0982, accuracy : 96.75\n",
      "iteration : 300, loss : 0.0990, accuracy : 96.74\n",
      "Epoch : 151, training loss : 0.0990, training accuracy : 96.73, validation loss : 0.2286, validation accuracy : 93.33\n",
      "\n",
      "Epoch: 152\n",
      "iteration :  50, loss : 0.0892, accuracy : 97.11\n",
      "iteration : 100, loss : 0.0886, accuracy : 97.05\n",
      "iteration : 150, loss : 0.0898, accuracy : 96.97\n",
      "iteration : 200, loss : 0.0925, accuracy : 96.87\n",
      "iteration : 250, loss : 0.0922, accuracy : 96.85\n",
      "iteration : 300, loss : 0.0920, accuracy : 96.84\n",
      "Epoch : 152, training loss : 0.0926, training accuracy : 96.82, validation loss : 0.2280, validation accuracy : 93.38\n",
      "\n",
      "Epoch: 153\n",
      "iteration :  50, loss : 0.0878, accuracy : 97.11\n",
      "iteration : 100, loss : 0.0853, accuracy : 97.16\n",
      "iteration : 150, loss : 0.0885, accuracy : 97.04\n",
      "iteration : 200, loss : 0.0881, accuracy : 97.06\n",
      "iteration : 250, loss : 0.0878, accuracy : 97.06\n",
      "iteration : 300, loss : 0.0876, accuracy : 97.06\n",
      "Epoch : 153, training loss : 0.0888, training accuracy : 97.02, validation loss : 0.2430, validation accuracy : 92.92\n",
      "\n",
      "Epoch: 154\n",
      "iteration :  50, loss : 0.1019, accuracy : 96.33\n",
      "iteration : 100, loss : 0.0968, accuracy : 96.58\n",
      "iteration : 150, loss : 0.0956, accuracy : 96.60\n",
      "iteration : 200, loss : 0.0931, accuracy : 96.72\n",
      "iteration : 250, loss : 0.0926, accuracy : 96.76\n",
      "iteration : 300, loss : 0.0932, accuracy : 96.72\n",
      "Epoch : 154, training loss : 0.0926, training accuracy : 96.75, validation loss : 0.2371, validation accuracy : 93.25\n",
      "\n",
      "Epoch: 155\n",
      "iteration :  50, loss : 0.0954, accuracy : 96.73\n",
      "iteration : 100, loss : 0.0908, accuracy : 96.93\n",
      "iteration : 150, loss : 0.0891, accuracy : 97.06\n",
      "iteration : 200, loss : 0.0902, accuracy : 96.94\n",
      "iteration : 250, loss : 0.0903, accuracy : 96.95\n",
      "iteration : 300, loss : 0.0919, accuracy : 96.91\n",
      "Epoch : 155, training loss : 0.0917, training accuracy : 96.91, validation loss : 0.2345, validation accuracy : 93.21\n",
      "\n",
      "Epoch: 156\n",
      "iteration :  50, loss : 0.0797, accuracy : 97.41\n",
      "iteration : 100, loss : 0.0820, accuracy : 97.17\n",
      "iteration : 150, loss : 0.0850, accuracy : 97.10\n",
      "iteration : 200, loss : 0.0868, accuracy : 97.04\n",
      "iteration : 250, loss : 0.0890, accuracy : 97.00\n",
      "iteration : 300, loss : 0.0900, accuracy : 96.97\n",
      "Epoch : 156, training loss : 0.0899, training accuracy : 96.98, validation loss : 0.2336, validation accuracy : 93.39\n",
      "\n",
      "Epoch: 157\n",
      "iteration :  50, loss : 0.0900, accuracy : 97.00\n",
      "iteration : 100, loss : 0.0867, accuracy : 97.00\n",
      "iteration : 150, loss : 0.0887, accuracy : 96.92\n",
      "iteration : 200, loss : 0.0901, accuracy : 96.93\n",
      "iteration : 250, loss : 0.0904, accuracy : 96.95\n",
      "iteration : 300, loss : 0.0922, accuracy : 96.90\n",
      "Epoch : 157, training loss : 0.0922, training accuracy : 96.90, validation loss : 0.2433, validation accuracy : 92.80\n",
      "\n",
      "Epoch: 158\n",
      "iteration :  50, loss : 0.0837, accuracy : 97.20\n",
      "iteration : 100, loss : 0.0791, accuracy : 97.34\n",
      "iteration : 150, loss : 0.0825, accuracy : 97.22\n",
      "iteration : 200, loss : 0.0841, accuracy : 97.14\n",
      "iteration : 250, loss : 0.0862, accuracy : 97.08\n",
      "iteration : 300, loss : 0.0867, accuracy : 97.05\n",
      "Epoch : 158, training loss : 0.0869, training accuracy : 97.05, validation loss : 0.2481, validation accuracy : 93.03\n",
      "\n",
      "Epoch: 159\n",
      "iteration :  50, loss : 0.0922, accuracy : 96.94\n",
      "iteration : 100, loss : 0.0906, accuracy : 96.88\n",
      "iteration : 150, loss : 0.0900, accuracy : 96.94\n",
      "iteration : 200, loss : 0.0918, accuracy : 96.87\n",
      "iteration : 250, loss : 0.0921, accuracy : 96.88\n",
      "iteration : 300, loss : 0.0931, accuracy : 96.90\n",
      "Epoch : 159, training loss : 0.0928, training accuracy : 96.92, validation loss : 0.2613, validation accuracy : 92.69\n",
      "\n",
      "Epoch: 160\n",
      "iteration :  50, loss : 0.0870, accuracy : 96.92\n",
      "iteration : 100, loss : 0.0932, accuracy : 96.75\n",
      "iteration : 150, loss : 0.0921, accuracy : 96.80\n",
      "iteration : 200, loss : 0.0936, accuracy : 96.82\n",
      "iteration : 250, loss : 0.0937, accuracy : 96.83\n",
      "iteration : 300, loss : 0.0932, accuracy : 96.84\n",
      "Epoch : 160, training loss : 0.0925, training accuracy : 96.85, validation loss : 0.2257, validation accuracy : 93.51\n",
      "\n",
      "Epoch: 161\n",
      "iteration :  50, loss : 0.0865, accuracy : 97.16\n",
      "iteration : 100, loss : 0.0873, accuracy : 96.98\n",
      "iteration : 150, loss : 0.0877, accuracy : 97.04\n",
      "iteration : 200, loss : 0.0864, accuracy : 97.12\n",
      "iteration : 250, loss : 0.0859, accuracy : 97.14\n",
      "iteration : 300, loss : 0.0870, accuracy : 97.11\n",
      "Epoch : 161, training loss : 0.0876, training accuracy : 97.09, validation loss : 0.2287, validation accuracy : 93.44\n",
      "\n",
      "Epoch: 162\n",
      "iteration :  50, loss : 0.0837, accuracy : 96.94\n",
      "iteration : 100, loss : 0.0823, accuracy : 97.09\n",
      "iteration : 150, loss : 0.0792, accuracy : 97.26\n",
      "iteration : 200, loss : 0.0826, accuracy : 97.16\n",
      "iteration : 250, loss : 0.0835, accuracy : 97.12\n",
      "iteration : 300, loss : 0.0859, accuracy : 97.04\n",
      "Epoch : 162, training loss : 0.0867, training accuracy : 97.01, validation loss : 0.2283, validation accuracy : 93.34\n",
      "\n",
      "Epoch: 163\n",
      "iteration :  50, loss : 0.0845, accuracy : 97.17\n",
      "iteration : 100, loss : 0.0872, accuracy : 97.01\n",
      "iteration : 150, loss : 0.0859, accuracy : 97.07\n",
      "iteration : 200, loss : 0.0858, accuracy : 97.03\n",
      "iteration : 250, loss : 0.0843, accuracy : 97.11\n",
      "iteration : 300, loss : 0.0838, accuracy : 97.13\n",
      "Epoch : 163, training loss : 0.0842, training accuracy : 97.13, validation loss : 0.2427, validation accuracy : 92.90\n",
      "\n",
      "Epoch: 164\n",
      "iteration :  50, loss : 0.0895, accuracy : 97.03\n",
      "iteration : 100, loss : 0.0877, accuracy : 97.10\n",
      "iteration : 150, loss : 0.0853, accuracy : 97.26\n",
      "iteration : 200, loss : 0.0842, accuracy : 97.31\n",
      "iteration : 250, loss : 0.0846, accuracy : 97.24\n",
      "iteration : 300, loss : 0.0845, accuracy : 97.24\n",
      "Epoch : 164, training loss : 0.0844, training accuracy : 97.23, validation loss : 0.2318, validation accuracy : 93.25\n",
      "\n",
      "Epoch: 165\n",
      "iteration :  50, loss : 0.0863, accuracy : 97.33\n",
      "iteration : 100, loss : 0.0911, accuracy : 97.12\n",
      "iteration : 150, loss : 0.0878, accuracy : 97.16\n",
      "iteration : 200, loss : 0.0872, accuracy : 97.17\n",
      "iteration : 250, loss : 0.0881, accuracy : 97.14\n",
      "iteration : 300, loss : 0.0881, accuracy : 97.14\n",
      "Epoch : 165, training loss : 0.0873, training accuracy : 97.16, validation loss : 0.2239, validation accuracy : 93.41\n",
      "\n",
      "Epoch: 166\n",
      "iteration :  50, loss : 0.0778, accuracy : 97.39\n",
      "iteration : 100, loss : 0.0807, accuracy : 97.35\n",
      "iteration : 150, loss : 0.0847, accuracy : 97.25\n",
      "iteration : 200, loss : 0.0869, accuracy : 97.11\n",
      "iteration : 250, loss : 0.0846, accuracy : 97.17\n",
      "iteration : 300, loss : 0.0833, accuracy : 97.20\n",
      "Epoch : 166, training loss : 0.0842, training accuracy : 97.19, validation loss : 0.2417, validation accuracy : 93.09\n",
      "\n",
      "Epoch: 167\n",
      "iteration :  50, loss : 0.0905, accuracy : 97.12\n",
      "iteration : 100, loss : 0.0868, accuracy : 97.17\n",
      "iteration : 150, loss : 0.0849, accuracy : 97.21\n",
      "iteration : 200, loss : 0.0820, accuracy : 97.32\n",
      "iteration : 250, loss : 0.0808, accuracy : 97.33\n",
      "iteration : 300, loss : 0.0815, accuracy : 97.29\n",
      "Epoch : 167, training loss : 0.0820, training accuracy : 97.26, validation loss : 0.2431, validation accuracy : 93.05\n",
      "\n",
      "Epoch: 168\n",
      "iteration :  50, loss : 0.0850, accuracy : 97.14\n",
      "iteration : 100, loss : 0.0836, accuracy : 97.23\n",
      "iteration : 150, loss : 0.0822, accuracy : 97.25\n",
      "iteration : 200, loss : 0.0858, accuracy : 97.14\n",
      "iteration : 250, loss : 0.0863, accuracy : 97.10\n",
      "iteration : 300, loss : 0.0863, accuracy : 97.09\n",
      "Epoch : 168, training loss : 0.0862, training accuracy : 97.08, validation loss : 0.2282, validation accuracy : 93.89\n",
      "\n",
      "Epoch: 169\n",
      "iteration :  50, loss : 0.0762, accuracy : 97.53\n",
      "iteration : 100, loss : 0.0832, accuracy : 97.13\n",
      "iteration : 150, loss : 0.0807, accuracy : 97.23\n",
      "iteration : 200, loss : 0.0801, accuracy : 97.29\n",
      "iteration : 250, loss : 0.0825, accuracy : 97.20\n",
      "iteration : 300, loss : 0.0833, accuracy : 97.17\n",
      "Epoch : 169, training loss : 0.0843, training accuracy : 97.14, validation loss : 0.2370, validation accuracy : 93.27\n",
      "\n",
      "Epoch: 170\n",
      "iteration :  50, loss : 0.0855, accuracy : 97.00\n",
      "iteration : 100, loss : 0.0852, accuracy : 97.03\n",
      "iteration : 150, loss : 0.0847, accuracy : 97.06\n",
      "iteration : 200, loss : 0.0836, accuracy : 97.19\n",
      "iteration : 250, loss : 0.0835, accuracy : 97.16\n",
      "iteration : 300, loss : 0.0829, accuracy : 97.17\n",
      "Epoch : 170, training loss : 0.0823, training accuracy : 97.18, validation loss : 0.2317, validation accuracy : 93.44\n",
      "\n",
      "Epoch: 171\n",
      "iteration :  50, loss : 0.0734, accuracy : 97.36\n",
      "iteration : 100, loss : 0.0768, accuracy : 97.21\n",
      "iteration : 150, loss : 0.0780, accuracy : 97.28\n",
      "iteration : 200, loss : 0.0778, accuracy : 97.36\n",
      "iteration : 250, loss : 0.0780, accuracy : 97.36\n",
      "iteration : 300, loss : 0.0784, accuracy : 97.35\n",
      "Epoch : 171, training loss : 0.0788, training accuracy : 97.34, validation loss : 0.2227, validation accuracy : 93.37\n",
      "\n",
      "Epoch: 172\n",
      "iteration :  50, loss : 0.0810, accuracy : 97.17\n",
      "iteration : 100, loss : 0.0756, accuracy : 97.40\n",
      "iteration : 150, loss : 0.0799, accuracy : 97.32\n",
      "iteration : 200, loss : 0.0790, accuracy : 97.34\n",
      "iteration : 250, loss : 0.0790, accuracy : 97.37\n",
      "iteration : 300, loss : 0.0792, accuracy : 97.36\n",
      "Epoch : 172, training loss : 0.0789, training accuracy : 97.37, validation loss : 0.2325, validation accuracy : 93.43\n",
      "\n",
      "Epoch: 173\n",
      "iteration :  50, loss : 0.0817, accuracy : 97.38\n",
      "iteration : 100, loss : 0.0839, accuracy : 97.34\n",
      "iteration : 150, loss : 0.0847, accuracy : 97.32\n",
      "iteration : 200, loss : 0.0823, accuracy : 97.39\n",
      "iteration : 250, loss : 0.0803, accuracy : 97.43\n",
      "iteration : 300, loss : 0.0796, accuracy : 97.45\n",
      "Epoch : 173, training loss : 0.0796, training accuracy : 97.45, validation loss : 0.2250, validation accuracy : 93.83\n",
      "\n",
      "Epoch: 174\n",
      "iteration :  50, loss : 0.0843, accuracy : 97.14\n",
      "iteration : 100, loss : 0.0834, accuracy : 97.18\n",
      "iteration : 150, loss : 0.0801, accuracy : 97.36\n",
      "iteration : 200, loss : 0.0787, accuracy : 97.35\n",
      "iteration : 250, loss : 0.0809, accuracy : 97.28\n",
      "iteration : 300, loss : 0.0806, accuracy : 97.33\n",
      "Epoch : 174, training loss : 0.0806, training accuracy : 97.35, validation loss : 0.2140, validation accuracy : 93.88\n",
      "\n",
      "Epoch: 175\n",
      "iteration :  50, loss : 0.0842, accuracy : 97.16\n",
      "iteration : 100, loss : 0.0779, accuracy : 97.38\n",
      "iteration : 150, loss : 0.0776, accuracy : 97.43\n",
      "iteration : 200, loss : 0.0758, accuracy : 97.50\n",
      "iteration : 250, loss : 0.0774, accuracy : 97.46\n",
      "iteration : 300, loss : 0.0779, accuracy : 97.43\n",
      "Epoch : 175, training loss : 0.0780, training accuracy : 97.43, validation loss : 0.2260, validation accuracy : 93.63\n",
      "\n",
      "Epoch: 176\n",
      "iteration :  50, loss : 0.0788, accuracy : 97.48\n",
      "iteration : 100, loss : 0.0769, accuracy : 97.39\n",
      "iteration : 150, loss : 0.0761, accuracy : 97.46\n",
      "iteration : 200, loss : 0.0753, accuracy : 97.44\n",
      "iteration : 250, loss : 0.0747, accuracy : 97.49\n",
      "iteration : 300, loss : 0.0741, accuracy : 97.54\n",
      "Epoch : 176, training loss : 0.0744, training accuracy : 97.53, validation loss : 0.2206, validation accuracy : 93.73\n",
      "\n",
      "Epoch: 177\n",
      "iteration :  50, loss : 0.0713, accuracy : 97.58\n",
      "iteration : 100, loss : 0.0782, accuracy : 97.30\n",
      "iteration : 150, loss : 0.0777, accuracy : 97.31\n",
      "iteration : 200, loss : 0.0781, accuracy : 97.28\n",
      "iteration : 250, loss : 0.0767, accuracy : 97.35\n",
      "iteration : 300, loss : 0.0775, accuracy : 97.36\n",
      "Epoch : 177, training loss : 0.0782, training accuracy : 97.35, validation loss : 0.2356, validation accuracy : 93.45\n",
      "\n",
      "Epoch: 178\n",
      "iteration :  50, loss : 0.0720, accuracy : 97.77\n",
      "iteration : 100, loss : 0.0723, accuracy : 97.65\n",
      "iteration : 150, loss : 0.0727, accuracy : 97.64\n",
      "iteration : 200, loss : 0.0735, accuracy : 97.59\n",
      "iteration : 250, loss : 0.0742, accuracy : 97.58\n",
      "iteration : 300, loss : 0.0745, accuracy : 97.56\n",
      "Epoch : 178, training loss : 0.0746, training accuracy : 97.56, validation loss : 0.2370, validation accuracy : 93.13\n",
      "\n",
      "Epoch: 179\n",
      "iteration :  50, loss : 0.0755, accuracy : 97.42\n",
      "iteration : 100, loss : 0.0741, accuracy : 97.52\n",
      "iteration : 150, loss : 0.0739, accuracy : 97.56\n",
      "iteration : 200, loss : 0.0722, accuracy : 97.62\n",
      "iteration : 250, loss : 0.0720, accuracy : 97.62\n",
      "iteration : 300, loss : 0.0716, accuracy : 97.60\n",
      "Epoch : 179, training loss : 0.0719, training accuracy : 97.60, validation loss : 0.2344, validation accuracy : 93.62\n",
      "\n",
      "Epoch: 180\n",
      "iteration :  50, loss : 0.0648, accuracy : 97.88\n",
      "iteration : 100, loss : 0.0666, accuracy : 97.80\n",
      "iteration : 150, loss : 0.0685, accuracy : 97.71\n",
      "iteration : 200, loss : 0.0688, accuracy : 97.72\n",
      "iteration : 250, loss : 0.0692, accuracy : 97.69\n",
      "iteration : 300, loss : 0.0693, accuracy : 97.70\n",
      "Epoch : 180, training loss : 0.0694, training accuracy : 97.69, validation loss : 0.2257, validation accuracy : 93.68\n",
      "\n",
      "Epoch: 181\n",
      "iteration :  50, loss : 0.0706, accuracy : 97.67\n",
      "iteration : 100, loss : 0.0698, accuracy : 97.64\n",
      "iteration : 150, loss : 0.0723, accuracy : 97.57\n",
      "iteration : 200, loss : 0.0717, accuracy : 97.59\n",
      "iteration : 250, loss : 0.0741, accuracy : 97.54\n",
      "iteration : 300, loss : 0.0747, accuracy : 97.51\n",
      "Epoch : 181, training loss : 0.0744, training accuracy : 97.53, validation loss : 0.2353, validation accuracy : 93.47\n",
      "\n",
      "Epoch: 182\n",
      "iteration :  50, loss : 0.0622, accuracy : 97.97\n",
      "iteration : 100, loss : 0.0659, accuracy : 97.78\n",
      "iteration : 150, loss : 0.0673, accuracy : 97.74\n",
      "iteration : 200, loss : 0.0699, accuracy : 97.66\n",
      "iteration : 250, loss : 0.0723, accuracy : 97.57\n",
      "iteration : 300, loss : 0.0737, accuracy : 97.54\n",
      "Epoch : 182, training loss : 0.0742, training accuracy : 97.53, validation loss : 0.2353, validation accuracy : 93.41\n",
      "\n",
      "Epoch: 183\n",
      "iteration :  50, loss : 0.0727, accuracy : 97.56\n",
      "iteration : 100, loss : 0.0714, accuracy : 97.62\n",
      "iteration : 150, loss : 0.0709, accuracy : 97.61\n",
      "iteration : 200, loss : 0.0727, accuracy : 97.53\n",
      "iteration : 250, loss : 0.0739, accuracy : 97.46\n",
      "iteration : 300, loss : 0.0735, accuracy : 97.52\n",
      "Epoch : 183, training loss : 0.0743, training accuracy : 97.50, validation loss : 0.2274, validation accuracy : 93.69\n",
      "\n",
      "Epoch: 184\n",
      "iteration :  50, loss : 0.0684, accuracy : 97.70\n",
      "iteration : 100, loss : 0.0681, accuracy : 97.68\n",
      "iteration : 150, loss : 0.0698, accuracy : 97.70\n",
      "iteration : 200, loss : 0.0688, accuracy : 97.67\n",
      "iteration : 250, loss : 0.0690, accuracy : 97.67\n",
      "iteration : 300, loss : 0.0694, accuracy : 97.64\n",
      "Epoch : 184, training loss : 0.0690, training accuracy : 97.64, validation loss : 0.2262, validation accuracy : 93.46\n",
      "\n",
      "Epoch: 185\n",
      "iteration :  50, loss : 0.0719, accuracy : 97.52\n",
      "iteration : 100, loss : 0.0725, accuracy : 97.48\n",
      "iteration : 150, loss : 0.0695, accuracy : 97.60\n",
      "iteration : 200, loss : 0.0702, accuracy : 97.59\n",
      "iteration : 250, loss : 0.0707, accuracy : 97.61\n",
      "iteration : 300, loss : 0.0711, accuracy : 97.57\n",
      "Epoch : 185, training loss : 0.0707, training accuracy : 97.59, validation loss : 0.2232, validation accuracy : 93.41\n",
      "\n",
      "Epoch: 186\n",
      "iteration :  50, loss : 0.0731, accuracy : 97.44\n",
      "iteration : 100, loss : 0.0741, accuracy : 97.49\n",
      "iteration : 150, loss : 0.0758, accuracy : 97.48\n",
      "iteration : 200, loss : 0.0736, accuracy : 97.57\n",
      "iteration : 250, loss : 0.0735, accuracy : 97.58\n",
      "iteration : 300, loss : 0.0725, accuracy : 97.59\n",
      "Epoch : 186, training loss : 0.0725, training accuracy : 97.59, validation loss : 0.2241, validation accuracy : 93.44\n",
      "\n",
      "Epoch: 187\n",
      "iteration :  50, loss : 0.0598, accuracy : 98.06\n",
      "iteration : 100, loss : 0.0620, accuracy : 97.98\n",
      "iteration : 150, loss : 0.0651, accuracy : 97.88\n",
      "iteration : 200, loss : 0.0647, accuracy : 97.86\n",
      "iteration : 250, loss : 0.0655, accuracy : 97.78\n",
      "iteration : 300, loss : 0.0678, accuracy : 97.68\n",
      "Epoch : 187, training loss : 0.0679, training accuracy : 97.67, validation loss : 0.2290, validation accuracy : 93.65\n",
      "\n",
      "Epoch: 188\n",
      "iteration :  50, loss : 0.0645, accuracy : 97.89\n",
      "iteration : 100, loss : 0.0614, accuracy : 97.98\n",
      "iteration : 150, loss : 0.0622, accuracy : 97.92\n",
      "iteration : 200, loss : 0.0649, accuracy : 97.88\n",
      "iteration : 250, loss : 0.0662, accuracy : 97.85\n",
      "iteration : 300, loss : 0.0665, accuracy : 97.84\n",
      "Epoch : 188, training loss : 0.0665, training accuracy : 97.83, validation loss : 0.2201, validation accuracy : 93.69\n",
      "\n",
      "Epoch: 189\n",
      "iteration :  50, loss : 0.0638, accuracy : 97.88\n",
      "iteration : 100, loss : 0.0664, accuracy : 97.77\n",
      "iteration : 150, loss : 0.0663, accuracy : 97.77\n",
      "iteration : 200, loss : 0.0672, accuracy : 97.72\n",
      "iteration : 250, loss : 0.0677, accuracy : 97.72\n",
      "iteration : 300, loss : 0.0682, accuracy : 97.70\n",
      "Epoch : 189, training loss : 0.0680, training accuracy : 97.70, validation loss : 0.2234, validation accuracy : 93.68\n",
      "\n",
      "Epoch: 190\n",
      "iteration :  50, loss : 0.0630, accuracy : 97.83\n",
      "iteration : 100, loss : 0.0667, accuracy : 97.69\n",
      "iteration : 150, loss : 0.0657, accuracy : 97.70\n",
      "iteration : 200, loss : 0.0661, accuracy : 97.72\n",
      "iteration : 250, loss : 0.0668, accuracy : 97.68\n",
      "iteration : 300, loss : 0.0676, accuracy : 97.68\n",
      "Epoch : 190, training loss : 0.0683, training accuracy : 97.66, validation loss : 0.2338, validation accuracy : 93.63\n",
      "\n",
      "Epoch: 191\n",
      "iteration :  50, loss : 0.0763, accuracy : 97.56\n",
      "iteration : 100, loss : 0.0735, accuracy : 97.52\n",
      "iteration : 150, loss : 0.0738, accuracy : 97.48\n",
      "iteration : 200, loss : 0.0699, accuracy : 97.61\n",
      "iteration : 250, loss : 0.0690, accuracy : 97.65\n",
      "iteration : 300, loss : 0.0666, accuracy : 97.74\n",
      "Epoch : 191, training loss : 0.0666, training accuracy : 97.73, validation loss : 0.2213, validation accuracy : 93.78\n",
      "\n",
      "Epoch: 192\n",
      "iteration :  50, loss : 0.0662, accuracy : 97.78\n",
      "iteration : 100, loss : 0.0682, accuracy : 97.66\n",
      "iteration : 150, loss : 0.0663, accuracy : 97.78\n",
      "iteration : 200, loss : 0.0683, accuracy : 97.73\n",
      "iteration : 250, loss : 0.0682, accuracy : 97.72\n",
      "iteration : 300, loss : 0.0675, accuracy : 97.74\n",
      "Epoch : 192, training loss : 0.0667, training accuracy : 97.76, validation loss : 0.2194, validation accuracy : 93.90\n",
      "\n",
      "Epoch: 193\n",
      "iteration :  50, loss : 0.0545, accuracy : 98.14\n",
      "iteration : 100, loss : 0.0611, accuracy : 97.92\n",
      "iteration : 150, loss : 0.0617, accuracy : 97.91\n",
      "iteration : 200, loss : 0.0648, accuracy : 97.83\n",
      "iteration : 250, loss : 0.0648, accuracy : 97.81\n",
      "iteration : 300, loss : 0.0649, accuracy : 97.80\n",
      "Epoch : 193, training loss : 0.0651, training accuracy : 97.80, validation loss : 0.2314, validation accuracy : 93.72\n",
      "\n",
      "Epoch: 194\n",
      "iteration :  50, loss : 0.0611, accuracy : 97.95\n",
      "iteration : 100, loss : 0.0639, accuracy : 97.84\n",
      "iteration : 150, loss : 0.0664, accuracy : 97.79\n",
      "iteration : 200, loss : 0.0647, accuracy : 97.82\n",
      "iteration : 250, loss : 0.0645, accuracy : 97.79\n",
      "iteration : 300, loss : 0.0645, accuracy : 97.78\n",
      "Epoch : 194, training loss : 0.0650, training accuracy : 97.76, validation loss : 0.2212, validation accuracy : 93.91\n",
      "\n",
      "Epoch: 195\n",
      "iteration :  50, loss : 0.0616, accuracy : 97.83\n",
      "iteration : 100, loss : 0.0619, accuracy : 97.90\n",
      "iteration : 150, loss : 0.0590, accuracy : 97.97\n",
      "iteration : 200, loss : 0.0587, accuracy : 98.05\n",
      "iteration : 250, loss : 0.0600, accuracy : 97.99\n",
      "iteration : 300, loss : 0.0606, accuracy : 97.95\n",
      "Epoch : 195, training loss : 0.0606, training accuracy : 97.95, validation loss : 0.2214, validation accuracy : 93.80\n",
      "\n",
      "Epoch: 196\n",
      "iteration :  50, loss : 0.0607, accuracy : 98.11\n",
      "iteration : 100, loss : 0.0662, accuracy : 97.91\n",
      "iteration : 150, loss : 0.0653, accuracy : 97.89\n",
      "iteration : 200, loss : 0.0656, accuracy : 97.86\n",
      "iteration : 250, loss : 0.0663, accuracy : 97.85\n",
      "iteration : 300, loss : 0.0658, accuracy : 97.87\n",
      "Epoch : 196, training loss : 0.0659, training accuracy : 97.86, validation loss : 0.2135, validation accuracy : 93.84\n",
      "\n",
      "Epoch: 197\n",
      "iteration :  50, loss : 0.0607, accuracy : 98.05\n",
      "iteration : 100, loss : 0.0600, accuracy : 98.01\n",
      "iteration : 150, loss : 0.0611, accuracy : 98.02\n",
      "iteration : 200, loss : 0.0608, accuracy : 98.01\n",
      "iteration : 250, loss : 0.0597, accuracy : 98.04\n",
      "iteration : 300, loss : 0.0600, accuracy : 98.02\n",
      "Epoch : 197, training loss : 0.0605, training accuracy : 97.99, validation loss : 0.2269, validation accuracy : 93.73\n",
      "\n",
      "Epoch: 198\n",
      "iteration :  50, loss : 0.0566, accuracy : 98.42\n",
      "iteration : 100, loss : 0.0626, accuracy : 98.08\n",
      "iteration : 150, loss : 0.0612, accuracy : 98.06\n",
      "iteration : 200, loss : 0.0605, accuracy : 98.07\n",
      "iteration : 250, loss : 0.0594, accuracy : 98.07\n",
      "iteration : 300, loss : 0.0602, accuracy : 98.05\n",
      "Epoch : 198, training loss : 0.0601, training accuracy : 98.06, validation loss : 0.2185, validation accuracy : 93.70\n",
      "\n",
      "Epoch: 199\n",
      "iteration :  50, loss : 0.0663, accuracy : 97.69\n",
      "iteration : 100, loss : 0.0631, accuracy : 97.84\n",
      "iteration : 150, loss : 0.0644, accuracy : 97.80\n",
      "iteration : 200, loss : 0.0623, accuracy : 97.90\n",
      "iteration : 250, loss : 0.0635, accuracy : 97.90\n",
      "iteration : 300, loss : 0.0631, accuracy : 97.94\n",
      "Epoch : 199, training loss : 0.0629, training accuracy : 97.94, validation loss : 0.2167, validation accuracy : 93.68\n",
      "\n",
      "Epoch: 200\n",
      "iteration :  50, loss : 0.0627, accuracy : 97.98\n",
      "iteration : 100, loss : 0.0654, accuracy : 97.84\n",
      "iteration : 150, loss : 0.0649, accuracy : 97.83\n",
      "iteration : 200, loss : 0.0639, accuracy : 97.82\n",
      "iteration : 250, loss : 0.0641, accuracy : 97.86\n",
      "iteration : 300, loss : 0.0638, accuracy : 97.87\n",
      "Epoch : 200, training loss : 0.0637, training accuracy : 97.87, validation loss : 0.2069, validation accuracy : 93.97\n",
      "\n",
      "Epoch: 201\n",
      "iteration :  50, loss : 0.0506, accuracy : 98.48\n",
      "iteration : 100, loss : 0.0515, accuracy : 98.33\n",
      "iteration : 150, loss : 0.0535, accuracy : 98.22\n",
      "iteration : 200, loss : 0.0559, accuracy : 98.15\n",
      "iteration : 250, loss : 0.0577, accuracy : 98.06\n",
      "iteration : 300, loss : 0.0594, accuracy : 98.00\n",
      "Epoch : 201, training loss : 0.0601, training accuracy : 97.99, validation loss : 0.2160, validation accuracy : 93.94\n",
      "\n",
      "Epoch: 202\n",
      "iteration :  50, loss : 0.0549, accuracy : 98.34\n",
      "iteration : 100, loss : 0.0581, accuracy : 98.10\n",
      "iteration : 150, loss : 0.0573, accuracy : 98.08\n",
      "iteration : 200, loss : 0.0589, accuracy : 98.03\n",
      "iteration : 250, loss : 0.0611, accuracy : 97.98\n",
      "iteration : 300, loss : 0.0608, accuracy : 98.00\n",
      "Epoch : 202, training loss : 0.0603, training accuracy : 98.02, validation loss : 0.2153, validation accuracy : 93.84\n",
      "\n",
      "Epoch: 203\n",
      "iteration :  50, loss : 0.0464, accuracy : 98.56\n",
      "iteration : 100, loss : 0.0545, accuracy : 98.20\n",
      "iteration : 150, loss : 0.0547, accuracy : 98.24\n",
      "iteration : 200, loss : 0.0543, accuracy : 98.27\n",
      "iteration : 250, loss : 0.0567, accuracy : 98.22\n",
      "iteration : 300, loss : 0.0579, accuracy : 98.17\n",
      "Epoch : 203, training loss : 0.0584, training accuracy : 98.16, validation loss : 0.2149, validation accuracy : 93.70\n",
      "\n",
      "Epoch: 204\n",
      "iteration :  50, loss : 0.0587, accuracy : 98.09\n",
      "iteration : 100, loss : 0.0599, accuracy : 97.99\n",
      "iteration : 150, loss : 0.0571, accuracy : 98.11\n",
      "iteration : 200, loss : 0.0586, accuracy : 98.07\n",
      "iteration : 250, loss : 0.0579, accuracy : 98.06\n",
      "iteration : 300, loss : 0.0585, accuracy : 98.03\n",
      "Epoch : 204, training loss : 0.0580, training accuracy : 98.04, validation loss : 0.1998, validation accuracy : 93.96\n",
      "\n",
      "Epoch: 205\n",
      "iteration :  50, loss : 0.0585, accuracy : 97.95\n",
      "iteration : 100, loss : 0.0589, accuracy : 98.02\n",
      "iteration : 150, loss : 0.0590, accuracy : 98.02\n",
      "iteration : 200, loss : 0.0603, accuracy : 97.99\n",
      "iteration : 250, loss : 0.0588, accuracy : 98.08\n",
      "iteration : 300, loss : 0.0588, accuracy : 98.07\n",
      "Epoch : 205, training loss : 0.0591, training accuracy : 98.06, validation loss : 0.2014, validation accuracy : 93.98\n",
      "\n",
      "Epoch: 206\n",
      "iteration :  50, loss : 0.0603, accuracy : 97.92\n",
      "iteration : 100, loss : 0.0573, accuracy : 98.10\n",
      "iteration : 150, loss : 0.0572, accuracy : 98.07\n",
      "iteration : 200, loss : 0.0559, accuracy : 98.09\n",
      "iteration : 250, loss : 0.0550, accuracy : 98.14\n",
      "iteration : 300, loss : 0.0554, accuracy : 98.11\n",
      "Epoch : 206, training loss : 0.0558, training accuracy : 98.09, validation loss : 0.2173, validation accuracy : 93.79\n",
      "\n",
      "Epoch: 207\n",
      "iteration :  50, loss : 0.0568, accuracy : 98.14\n",
      "iteration : 100, loss : 0.0555, accuracy : 98.22\n",
      "iteration : 150, loss : 0.0549, accuracy : 98.19\n",
      "iteration : 200, loss : 0.0558, accuracy : 98.18\n",
      "iteration : 250, loss : 0.0571, accuracy : 98.12\n",
      "iteration : 300, loss : 0.0568, accuracy : 98.14\n",
      "Epoch : 207, training loss : 0.0563, training accuracy : 98.15, validation loss : 0.2018, validation accuracy : 94.24\n",
      "\n",
      "Epoch: 208\n",
      "iteration :  50, loss : 0.0556, accuracy : 98.31\n",
      "iteration : 100, loss : 0.0557, accuracy : 98.17\n",
      "iteration : 150, loss : 0.0559, accuracy : 98.17\n",
      "iteration : 200, loss : 0.0566, accuracy : 98.14\n",
      "iteration : 250, loss : 0.0560, accuracy : 98.15\n",
      "iteration : 300, loss : 0.0565, accuracy : 98.11\n",
      "Epoch : 208, training loss : 0.0562, training accuracy : 98.12, validation loss : 0.2053, validation accuracy : 94.12\n",
      "\n",
      "Epoch: 209\n",
      "iteration :  50, loss : 0.0491, accuracy : 98.31\n",
      "iteration : 100, loss : 0.0484, accuracy : 98.36\n",
      "iteration : 150, loss : 0.0494, accuracy : 98.35\n",
      "iteration : 200, loss : 0.0511, accuracy : 98.30\n",
      "iteration : 250, loss : 0.0503, accuracy : 98.35\n",
      "iteration : 300, loss : 0.0506, accuracy : 98.36\n",
      "Epoch : 209, training loss : 0.0508, training accuracy : 98.36, validation loss : 0.2083, validation accuracy : 94.11\n",
      "\n",
      "Epoch: 210\n",
      "iteration :  50, loss : 0.0506, accuracy : 98.27\n",
      "iteration : 100, loss : 0.0504, accuracy : 98.26\n",
      "iteration : 150, loss : 0.0524, accuracy : 98.26\n",
      "iteration : 200, loss : 0.0527, accuracy : 98.23\n",
      "iteration : 250, loss : 0.0543, accuracy : 98.17\n",
      "iteration : 300, loss : 0.0549, accuracy : 98.14\n",
      "Epoch : 210, training loss : 0.0547, training accuracy : 98.14, validation loss : 0.2244, validation accuracy : 93.81\n",
      "\n",
      "Epoch: 211\n",
      "iteration :  50, loss : 0.0614, accuracy : 97.94\n",
      "iteration : 100, loss : 0.0598, accuracy : 98.04\n",
      "iteration : 150, loss : 0.0564, accuracy : 98.16\n",
      "iteration : 200, loss : 0.0547, accuracy : 98.23\n",
      "iteration : 250, loss : 0.0537, accuracy : 98.27\n",
      "iteration : 300, loss : 0.0543, accuracy : 98.22\n",
      "Epoch : 211, training loss : 0.0544, training accuracy : 98.22, validation loss : 0.2116, validation accuracy : 94.26\n",
      "\n",
      "Epoch: 212\n",
      "iteration :  50, loss : 0.0511, accuracy : 98.27\n",
      "iteration : 100, loss : 0.0541, accuracy : 98.13\n",
      "iteration : 150, loss : 0.0544, accuracy : 98.16\n",
      "iteration : 200, loss : 0.0550, accuracy : 98.16\n",
      "iteration : 250, loss : 0.0546, accuracy : 98.15\n",
      "iteration : 300, loss : 0.0544, accuracy : 98.15\n",
      "Epoch : 212, training loss : 0.0545, training accuracy : 98.16, validation loss : 0.2045, validation accuracy : 94.21\n",
      "\n",
      "Epoch: 213\n",
      "iteration :  50, loss : 0.0485, accuracy : 98.38\n",
      "iteration : 100, loss : 0.0507, accuracy : 98.28\n",
      "iteration : 150, loss : 0.0552, accuracy : 98.11\n",
      "iteration : 200, loss : 0.0539, accuracy : 98.15\n",
      "iteration : 250, loss : 0.0539, accuracy : 98.16\n",
      "iteration : 300, loss : 0.0546, accuracy : 98.14\n",
      "Epoch : 213, training loss : 0.0546, training accuracy : 98.14, validation loss : 0.2104, validation accuracy : 94.01\n",
      "\n",
      "Epoch: 214\n",
      "iteration :  50, loss : 0.0522, accuracy : 98.30\n",
      "iteration : 100, loss : 0.0496, accuracy : 98.38\n",
      "iteration : 150, loss : 0.0494, accuracy : 98.40\n",
      "iteration : 200, loss : 0.0512, accuracy : 98.34\n",
      "iteration : 250, loss : 0.0511, accuracy : 98.33\n",
      "iteration : 300, loss : 0.0513, accuracy : 98.33\n",
      "Epoch : 214, training loss : 0.0513, training accuracy : 98.33, validation loss : 0.2050, validation accuracy : 94.17\n",
      "\n",
      "Epoch: 215\n",
      "iteration :  50, loss : 0.0487, accuracy : 98.20\n",
      "iteration : 100, loss : 0.0535, accuracy : 98.16\n",
      "iteration : 150, loss : 0.0521, accuracy : 98.21\n",
      "iteration : 200, loss : 0.0508, accuracy : 98.27\n",
      "iteration : 250, loss : 0.0509, accuracy : 98.28\n",
      "iteration : 300, loss : 0.0505, accuracy : 98.28\n",
      "Epoch : 215, training loss : 0.0502, training accuracy : 98.31, validation loss : 0.2057, validation accuracy : 94.25\n",
      "\n",
      "Epoch: 216\n",
      "iteration :  50, loss : 0.0464, accuracy : 98.44\n",
      "iteration : 100, loss : 0.0505, accuracy : 98.29\n",
      "iteration : 150, loss : 0.0496, accuracy : 98.33\n",
      "iteration : 200, loss : 0.0503, accuracy : 98.28\n",
      "iteration : 250, loss : 0.0500, accuracy : 98.29\n",
      "iteration : 300, loss : 0.0495, accuracy : 98.29\n",
      "Epoch : 216, training loss : 0.0489, training accuracy : 98.31, validation loss : 0.1994, validation accuracy : 94.26\n",
      "\n",
      "Epoch: 217\n",
      "iteration :  50, loss : 0.0510, accuracy : 98.28\n",
      "iteration : 100, loss : 0.0505, accuracy : 98.28\n",
      "iteration : 150, loss : 0.0489, accuracy : 98.35\n",
      "iteration : 200, loss : 0.0479, accuracy : 98.41\n",
      "iteration : 250, loss : 0.0479, accuracy : 98.43\n",
      "iteration : 300, loss : 0.0475, accuracy : 98.43\n",
      "Epoch : 217, training loss : 0.0474, training accuracy : 98.42, validation loss : 0.2058, validation accuracy : 94.23\n",
      "\n",
      "Epoch: 218\n",
      "iteration :  50, loss : 0.0493, accuracy : 98.30\n",
      "iteration : 100, loss : 0.0471, accuracy : 98.43\n",
      "iteration : 150, loss : 0.0485, accuracy : 98.40\n",
      "iteration : 200, loss : 0.0491, accuracy : 98.38\n",
      "iteration : 250, loss : 0.0487, accuracy : 98.39\n",
      "iteration : 300, loss : 0.0487, accuracy : 98.38\n",
      "Epoch : 218, training loss : 0.0485, training accuracy : 98.39, validation loss : 0.1938, validation accuracy : 94.51\n",
      "\n",
      "Epoch: 219\n",
      "iteration :  50, loss : 0.0502, accuracy : 98.34\n",
      "iteration : 100, loss : 0.0476, accuracy : 98.40\n",
      "iteration : 150, loss : 0.0467, accuracy : 98.41\n",
      "iteration : 200, loss : 0.0461, accuracy : 98.43\n",
      "iteration : 250, loss : 0.0478, accuracy : 98.41\n",
      "iteration : 300, loss : 0.0483, accuracy : 98.36\n",
      "Epoch : 219, training loss : 0.0490, training accuracy : 98.36, validation loss : 0.2093, validation accuracy : 93.99\n",
      "\n",
      "Epoch: 220\n",
      "iteration :  50, loss : 0.0417, accuracy : 98.53\n",
      "iteration : 100, loss : 0.0476, accuracy : 98.41\n",
      "iteration : 150, loss : 0.0461, accuracy : 98.45\n",
      "iteration : 200, loss : 0.0454, accuracy : 98.51\n",
      "iteration : 250, loss : 0.0466, accuracy : 98.45\n",
      "iteration : 300, loss : 0.0466, accuracy : 98.44\n",
      "Epoch : 220, training loss : 0.0465, training accuracy : 98.45, validation loss : 0.2036, validation accuracy : 94.16\n",
      "\n",
      "Epoch: 221\n",
      "iteration :  50, loss : 0.0443, accuracy : 98.59\n",
      "iteration : 100, loss : 0.0464, accuracy : 98.49\n",
      "iteration : 150, loss : 0.0460, accuracy : 98.56\n",
      "iteration : 200, loss : 0.0466, accuracy : 98.51\n",
      "iteration : 250, loss : 0.0461, accuracy : 98.51\n",
      "iteration : 300, loss : 0.0465, accuracy : 98.50\n",
      "Epoch : 221, training loss : 0.0463, training accuracy : 98.49, validation loss : 0.1924, validation accuracy : 94.37\n",
      "\n",
      "Epoch: 222\n",
      "iteration :  50, loss : 0.0471, accuracy : 98.44\n",
      "iteration : 100, loss : 0.0472, accuracy : 98.40\n",
      "iteration : 150, loss : 0.0452, accuracy : 98.50\n",
      "iteration : 200, loss : 0.0460, accuracy : 98.42\n",
      "iteration : 250, loss : 0.0465, accuracy : 98.41\n",
      "iteration : 300, loss : 0.0457, accuracy : 98.46\n",
      "Epoch : 222, training loss : 0.0464, training accuracy : 98.44, validation loss : 0.1982, validation accuracy : 94.16\n",
      "\n",
      "Epoch: 223\n",
      "iteration :  50, loss : 0.0499, accuracy : 98.31\n",
      "iteration : 100, loss : 0.0479, accuracy : 98.37\n",
      "iteration : 150, loss : 0.0446, accuracy : 98.49\n",
      "iteration : 200, loss : 0.0449, accuracy : 98.48\n",
      "iteration : 250, loss : 0.0458, accuracy : 98.44\n",
      "iteration : 300, loss : 0.0462, accuracy : 98.44\n",
      "Epoch : 223, training loss : 0.0465, training accuracy : 98.43, validation loss : 0.2099, validation accuracy : 94.12\n",
      "\n",
      "Epoch: 224\n",
      "iteration :  50, loss : 0.0476, accuracy : 98.61\n",
      "iteration : 100, loss : 0.0473, accuracy : 98.56\n",
      "iteration : 150, loss : 0.0478, accuracy : 98.51\n",
      "iteration : 200, loss : 0.0470, accuracy : 98.50\n",
      "iteration : 250, loss : 0.0460, accuracy : 98.53\n",
      "iteration : 300, loss : 0.0451, accuracy : 98.55\n",
      "Epoch : 224, training loss : 0.0451, training accuracy : 98.56, validation loss : 0.1948, validation accuracy : 94.43\n",
      "\n",
      "Epoch: 225\n",
      "iteration :  50, loss : 0.0404, accuracy : 98.59\n",
      "iteration : 100, loss : 0.0458, accuracy : 98.41\n",
      "iteration : 150, loss : 0.0427, accuracy : 98.57\n",
      "iteration : 200, loss : 0.0419, accuracy : 98.60\n",
      "iteration : 250, loss : 0.0433, accuracy : 98.54\n",
      "iteration : 300, loss : 0.0437, accuracy : 98.53\n",
      "Epoch : 225, training loss : 0.0446, training accuracy : 98.50, validation loss : 0.2012, validation accuracy : 94.46\n",
      "\n",
      "Epoch: 226\n",
      "iteration :  50, loss : 0.0423, accuracy : 98.66\n",
      "iteration : 100, loss : 0.0419, accuracy : 98.56\n",
      "iteration : 150, loss : 0.0417, accuracy : 98.65\n",
      "iteration : 200, loss : 0.0422, accuracy : 98.67\n",
      "iteration : 250, loss : 0.0419, accuracy : 98.66\n",
      "iteration : 300, loss : 0.0420, accuracy : 98.63\n",
      "Epoch : 226, training loss : 0.0428, training accuracy : 98.61, validation loss : 0.2010, validation accuracy : 94.27\n",
      "\n",
      "Epoch: 227\n",
      "iteration :  50, loss : 0.0364, accuracy : 98.92\n",
      "iteration : 100, loss : 0.0375, accuracy : 98.80\n",
      "iteration : 150, loss : 0.0397, accuracy : 98.76\n",
      "iteration : 200, loss : 0.0418, accuracy : 98.68\n",
      "iteration : 250, loss : 0.0426, accuracy : 98.63\n",
      "iteration : 300, loss : 0.0428, accuracy : 98.63\n",
      "Epoch : 227, training loss : 0.0425, training accuracy : 98.64, validation loss : 0.1955, validation accuracy : 94.28\n",
      "\n",
      "Epoch: 228\n",
      "iteration :  50, loss : 0.0446, accuracy : 98.55\n",
      "iteration : 100, loss : 0.0441, accuracy : 98.58\n",
      "iteration : 150, loss : 0.0430, accuracy : 98.61\n",
      "iteration : 200, loss : 0.0420, accuracy : 98.65\n",
      "iteration : 250, loss : 0.0426, accuracy : 98.65\n",
      "iteration : 300, loss : 0.0416, accuracy : 98.66\n",
      "Epoch : 228, training loss : 0.0414, training accuracy : 98.66, validation loss : 0.2058, validation accuracy : 94.21\n",
      "\n",
      "Epoch: 229\n",
      "iteration :  50, loss : 0.0516, accuracy : 98.17\n",
      "iteration : 100, loss : 0.0438, accuracy : 98.45\n",
      "iteration : 150, loss : 0.0444, accuracy : 98.49\n",
      "iteration : 200, loss : 0.0455, accuracy : 98.49\n",
      "iteration : 250, loss : 0.0443, accuracy : 98.53\n",
      "iteration : 300, loss : 0.0453, accuracy : 98.51\n",
      "Epoch : 229, training loss : 0.0450, training accuracy : 98.52, validation loss : 0.2009, validation accuracy : 94.33\n",
      "\n",
      "Epoch: 230\n",
      "iteration :  50, loss : 0.0424, accuracy : 98.52\n",
      "iteration : 100, loss : 0.0411, accuracy : 98.58\n",
      "iteration : 150, loss : 0.0414, accuracy : 98.61\n",
      "iteration : 200, loss : 0.0425, accuracy : 98.61\n",
      "iteration : 250, loss : 0.0421, accuracy : 98.63\n",
      "iteration : 300, loss : 0.0433, accuracy : 98.58\n",
      "Epoch : 230, training loss : 0.0434, training accuracy : 98.56, validation loss : 0.2021, validation accuracy : 94.42\n",
      "\n",
      "Epoch: 231\n",
      "iteration :  50, loss : 0.0384, accuracy : 98.83\n",
      "iteration : 100, loss : 0.0409, accuracy : 98.69\n",
      "iteration : 150, loss : 0.0420, accuracy : 98.62\n",
      "iteration : 200, loss : 0.0417, accuracy : 98.61\n",
      "iteration : 250, loss : 0.0414, accuracy : 98.63\n",
      "iteration : 300, loss : 0.0416, accuracy : 98.62\n",
      "Epoch : 231, training loss : 0.0415, training accuracy : 98.62, validation loss : 0.2027, validation accuracy : 94.44\n",
      "\n",
      "Epoch: 232\n",
      "iteration :  50, loss : 0.0441, accuracy : 98.53\n",
      "iteration : 100, loss : 0.0416, accuracy : 98.62\n",
      "iteration : 150, loss : 0.0416, accuracy : 98.60\n",
      "iteration : 200, loss : 0.0412, accuracy : 98.64\n",
      "iteration : 250, loss : 0.0425, accuracy : 98.59\n",
      "iteration : 300, loss : 0.0428, accuracy : 98.58\n",
      "Epoch : 232, training loss : 0.0429, training accuracy : 98.57, validation loss : 0.2191, validation accuracy : 94.25\n",
      "\n",
      "Epoch: 233\n",
      "iteration :  50, loss : 0.0395, accuracy : 98.75\n",
      "iteration : 100, loss : 0.0381, accuracy : 98.79\n",
      "iteration : 150, loss : 0.0389, accuracy : 98.77\n",
      "iteration : 200, loss : 0.0388, accuracy : 98.77\n",
      "iteration : 250, loss : 0.0400, accuracy : 98.75\n",
      "iteration : 300, loss : 0.0399, accuracy : 98.74\n",
      "Epoch : 233, training loss : 0.0397, training accuracy : 98.75, validation loss : 0.2086, validation accuracy : 94.34\n",
      "\n",
      "Epoch: 234\n",
      "iteration :  50, loss : 0.0359, accuracy : 98.98\n",
      "iteration : 100, loss : 0.0411, accuracy : 98.75\n",
      "iteration : 150, loss : 0.0438, accuracy : 98.60\n",
      "iteration : 200, loss : 0.0451, accuracy : 98.55\n",
      "iteration : 250, loss : 0.0443, accuracy : 98.57\n",
      "iteration : 300, loss : 0.0440, accuracy : 98.56\n",
      "Epoch : 234, training loss : 0.0440, training accuracy : 98.57, validation loss : 0.1922, validation accuracy : 94.70\n",
      "\n",
      "Epoch: 235\n",
      "iteration :  50, loss : 0.0422, accuracy : 98.61\n",
      "iteration : 100, loss : 0.0408, accuracy : 98.71\n",
      "iteration : 150, loss : 0.0398, accuracy : 98.73\n",
      "iteration : 200, loss : 0.0398, accuracy : 98.74\n",
      "iteration : 250, loss : 0.0400, accuracy : 98.71\n",
      "iteration : 300, loss : 0.0396, accuracy : 98.72\n",
      "Epoch : 235, training loss : 0.0398, training accuracy : 98.71, validation loss : 0.2008, validation accuracy : 94.53\n",
      "\n",
      "Epoch: 236\n",
      "iteration :  50, loss : 0.0398, accuracy : 98.72\n",
      "iteration : 100, loss : 0.0431, accuracy : 98.54\n",
      "iteration : 150, loss : 0.0408, accuracy : 98.61\n",
      "iteration : 200, loss : 0.0416, accuracy : 98.59\n",
      "iteration : 250, loss : 0.0411, accuracy : 98.63\n",
      "iteration : 300, loss : 0.0408, accuracy : 98.65\n",
      "Epoch : 236, training loss : 0.0410, training accuracy : 98.65, validation loss : 0.2004, validation accuracy : 94.63\n",
      "\n",
      "Epoch: 237\n",
      "iteration :  50, loss : 0.0306, accuracy : 99.11\n",
      "iteration : 100, loss : 0.0349, accuracy : 98.91\n",
      "iteration : 150, loss : 0.0363, accuracy : 98.83\n",
      "iteration : 200, loss : 0.0354, accuracy : 98.87\n",
      "iteration : 250, loss : 0.0357, accuracy : 98.85\n",
      "iteration : 300, loss : 0.0369, accuracy : 98.82\n",
      "Epoch : 237, training loss : 0.0364, training accuracy : 98.83, validation loss : 0.2034, validation accuracy : 94.34\n",
      "\n",
      "Epoch: 238\n",
      "iteration :  50, loss : 0.0393, accuracy : 98.77\n",
      "iteration : 100, loss : 0.0433, accuracy : 98.55\n",
      "iteration : 150, loss : 0.0423, accuracy : 98.58\n",
      "iteration : 200, loss : 0.0412, accuracy : 98.61\n",
      "iteration : 250, loss : 0.0413, accuracy : 98.64\n",
      "iteration : 300, loss : 0.0401, accuracy : 98.68\n",
      "Epoch : 238, training loss : 0.0396, training accuracy : 98.70, validation loss : 0.1996, validation accuracy : 94.50\n",
      "\n",
      "Epoch: 239\n",
      "iteration :  50, loss : 0.0397, accuracy : 98.69\n",
      "iteration : 100, loss : 0.0387, accuracy : 98.74\n",
      "iteration : 150, loss : 0.0372, accuracy : 98.78\n",
      "iteration : 200, loss : 0.0375, accuracy : 98.73\n",
      "iteration : 250, loss : 0.0371, accuracy : 98.73\n",
      "iteration : 300, loss : 0.0366, accuracy : 98.76\n",
      "Epoch : 239, training loss : 0.0367, training accuracy : 98.75, validation loss : 0.1988, validation accuracy : 94.31\n",
      "\n",
      "Epoch: 240\n",
      "iteration :  50, loss : 0.0386, accuracy : 98.66\n",
      "iteration : 100, loss : 0.0360, accuracy : 98.79\n",
      "iteration : 150, loss : 0.0355, accuracy : 98.84\n",
      "iteration : 200, loss : 0.0367, accuracy : 98.83\n",
      "iteration : 250, loss : 0.0368, accuracy : 98.81\n",
      "iteration : 300, loss : 0.0366, accuracy : 98.82\n",
      "Epoch : 240, training loss : 0.0370, training accuracy : 98.82, validation loss : 0.1951, validation accuracy : 94.35\n",
      "\n",
      "Epoch: 241\n",
      "iteration :  50, loss : 0.0286, accuracy : 99.05\n",
      "iteration : 100, loss : 0.0318, accuracy : 99.04\n",
      "iteration : 150, loss : 0.0333, accuracy : 98.98\n",
      "iteration : 200, loss : 0.0338, accuracy : 98.96\n",
      "iteration : 250, loss : 0.0357, accuracy : 98.88\n",
      "iteration : 300, loss : 0.0360, accuracy : 98.86\n",
      "Epoch : 241, training loss : 0.0360, training accuracy : 98.86, validation loss : 0.1984, validation accuracy : 94.43\n",
      "\n",
      "Epoch: 242\n",
      "iteration :  50, loss : 0.0404, accuracy : 98.55\n",
      "iteration : 100, loss : 0.0402, accuracy : 98.60\n",
      "iteration : 150, loss : 0.0386, accuracy : 98.68\n",
      "iteration : 200, loss : 0.0371, accuracy : 98.73\n",
      "iteration : 250, loss : 0.0366, accuracy : 98.76\n",
      "iteration : 300, loss : 0.0364, accuracy : 98.77\n",
      "Epoch : 242, training loss : 0.0362, training accuracy : 98.77, validation loss : 0.1975, validation accuracy : 94.47\n",
      "\n",
      "Epoch: 243\n",
      "iteration :  50, loss : 0.0337, accuracy : 98.92\n",
      "iteration : 100, loss : 0.0351, accuracy : 98.87\n",
      "iteration : 150, loss : 0.0357, accuracy : 98.88\n",
      "iteration : 200, loss : 0.0361, accuracy : 98.85\n",
      "iteration : 250, loss : 0.0358, accuracy : 98.84\n",
      "iteration : 300, loss : 0.0350, accuracy : 98.86\n",
      "Epoch : 243, training loss : 0.0351, training accuracy : 98.86, validation loss : 0.1914, validation accuracy : 94.61\n",
      "\n",
      "Epoch: 244\n",
      "iteration :  50, loss : 0.0419, accuracy : 98.75\n",
      "iteration : 100, loss : 0.0395, accuracy : 98.77\n",
      "iteration : 150, loss : 0.0370, accuracy : 98.86\n",
      "iteration : 200, loss : 0.0364, accuracy : 98.86\n",
      "iteration : 250, loss : 0.0371, accuracy : 98.82\n",
      "iteration : 300, loss : 0.0375, accuracy : 98.78\n",
      "Epoch : 244, training loss : 0.0375, training accuracy : 98.79, validation loss : 0.1904, validation accuracy : 94.56\n",
      "\n",
      "Epoch: 245\n",
      "iteration :  50, loss : 0.0293, accuracy : 99.08\n",
      "iteration : 100, loss : 0.0319, accuracy : 98.97\n",
      "iteration : 150, loss : 0.0329, accuracy : 98.91\n",
      "iteration : 200, loss : 0.0358, accuracy : 98.78\n",
      "iteration : 250, loss : 0.0366, accuracy : 98.76\n",
      "iteration : 300, loss : 0.0368, accuracy : 98.78\n",
      "Epoch : 245, training loss : 0.0368, training accuracy : 98.78, validation loss : 0.1988, validation accuracy : 94.38\n",
      "\n",
      "Epoch: 246\n",
      "iteration :  50, loss : 0.0377, accuracy : 98.67\n",
      "iteration : 100, loss : 0.0367, accuracy : 98.74\n",
      "iteration : 150, loss : 0.0370, accuracy : 98.73\n",
      "iteration : 200, loss : 0.0359, accuracy : 98.78\n",
      "iteration : 250, loss : 0.0356, accuracy : 98.78\n",
      "iteration : 300, loss : 0.0358, accuracy : 98.77\n",
      "Epoch : 246, training loss : 0.0357, training accuracy : 98.78, validation loss : 0.1982, validation accuracy : 94.54\n",
      "\n",
      "Epoch: 247\n",
      "iteration :  50, loss : 0.0415, accuracy : 98.67\n",
      "iteration : 100, loss : 0.0377, accuracy : 98.77\n",
      "iteration : 150, loss : 0.0368, accuracy : 98.83\n",
      "iteration : 200, loss : 0.0364, accuracy : 98.86\n",
      "iteration : 250, loss : 0.0340, accuracy : 98.95\n",
      "iteration : 300, loss : 0.0346, accuracy : 98.92\n",
      "Epoch : 247, training loss : 0.0344, training accuracy : 98.92, validation loss : 0.1971, validation accuracy : 94.58\n",
      "\n",
      "Epoch: 248\n",
      "iteration :  50, loss : 0.0357, accuracy : 98.84\n",
      "iteration : 100, loss : 0.0367, accuracy : 98.75\n",
      "iteration : 150, loss : 0.0359, accuracy : 98.77\n",
      "iteration : 200, loss : 0.0356, accuracy : 98.82\n",
      "iteration : 250, loss : 0.0350, accuracy : 98.84\n",
      "iteration : 300, loss : 0.0347, accuracy : 98.86\n",
      "Epoch : 248, training loss : 0.0347, training accuracy : 98.86, validation loss : 0.1928, validation accuracy : 94.56\n",
      "\n",
      "Epoch: 249\n",
      "iteration :  50, loss : 0.0301, accuracy : 99.08\n",
      "iteration : 100, loss : 0.0310, accuracy : 99.04\n",
      "iteration : 150, loss : 0.0312, accuracy : 98.98\n",
      "iteration : 200, loss : 0.0311, accuracy : 98.97\n",
      "iteration : 250, loss : 0.0315, accuracy : 98.97\n",
      "iteration : 300, loss : 0.0320, accuracy : 98.96\n",
      "Epoch : 249, training loss : 0.0322, training accuracy : 98.95, validation loss : 0.1908, validation accuracy : 94.55\n",
      "\n",
      "Epoch: 250\n",
      "iteration :  50, loss : 0.0331, accuracy : 98.97\n",
      "iteration : 100, loss : 0.0313, accuracy : 98.99\n",
      "iteration : 150, loss : 0.0322, accuracy : 98.92\n",
      "iteration : 200, loss : 0.0318, accuracy : 98.96\n",
      "iteration : 250, loss : 0.0311, accuracy : 98.98\n",
      "iteration : 300, loss : 0.0308, accuracy : 99.00\n",
      "Epoch : 250, training loss : 0.0307, training accuracy : 99.01, validation loss : 0.1901, validation accuracy : 94.51\n",
      "\n",
      "Epoch: 251\n",
      "iteration :  50, loss : 0.0278, accuracy : 99.16\n",
      "iteration : 100, loss : 0.0310, accuracy : 99.01\n",
      "iteration : 150, loss : 0.0309, accuracy : 99.00\n",
      "iteration : 200, loss : 0.0306, accuracy : 99.01\n",
      "iteration : 250, loss : 0.0318, accuracy : 98.95\n",
      "iteration : 300, loss : 0.0322, accuracy : 98.94\n",
      "Epoch : 251, training loss : 0.0325, training accuracy : 98.94, validation loss : 0.2008, validation accuracy : 94.52\n",
      "\n",
      "Epoch: 252\n",
      "iteration :  50, loss : 0.0324, accuracy : 99.02\n",
      "iteration : 100, loss : 0.0323, accuracy : 99.00\n",
      "iteration : 150, loss : 0.0327, accuracy : 98.95\n",
      "iteration : 200, loss : 0.0325, accuracy : 98.91\n",
      "iteration : 250, loss : 0.0329, accuracy : 98.90\n",
      "iteration : 300, loss : 0.0322, accuracy : 98.92\n",
      "Epoch : 252, training loss : 0.0324, training accuracy : 98.91, validation loss : 0.1920, validation accuracy : 94.61\n",
      "\n",
      "Epoch: 253\n",
      "iteration :  50, loss : 0.0249, accuracy : 99.23\n",
      "iteration : 100, loss : 0.0274, accuracy : 99.14\n",
      "iteration : 150, loss : 0.0290, accuracy : 99.08\n",
      "iteration : 200, loss : 0.0305, accuracy : 99.03\n",
      "iteration : 250, loss : 0.0321, accuracy : 98.97\n",
      "iteration : 300, loss : 0.0314, accuracy : 99.00\n",
      "Epoch : 253, training loss : 0.0312, training accuracy : 99.01, validation loss : 0.1870, validation accuracy : 94.64\n",
      "\n",
      "Epoch: 254\n",
      "iteration :  50, loss : 0.0317, accuracy : 98.91\n",
      "iteration : 100, loss : 0.0329, accuracy : 98.88\n",
      "iteration : 150, loss : 0.0324, accuracy : 98.90\n",
      "iteration : 200, loss : 0.0328, accuracy : 98.87\n",
      "iteration : 250, loss : 0.0335, accuracy : 98.87\n",
      "iteration : 300, loss : 0.0335, accuracy : 98.88\n",
      "Epoch : 254, training loss : 0.0335, training accuracy : 98.88, validation loss : 0.1882, validation accuracy : 94.64\n",
      "\n",
      "Epoch: 255\n",
      "iteration :  50, loss : 0.0344, accuracy : 98.77\n",
      "iteration : 100, loss : 0.0321, accuracy : 98.89\n",
      "iteration : 150, loss : 0.0316, accuracy : 98.92\n",
      "iteration : 200, loss : 0.0316, accuracy : 98.95\n",
      "iteration : 250, loss : 0.0303, accuracy : 98.99\n",
      "iteration : 300, loss : 0.0311, accuracy : 98.96\n",
      "Epoch : 255, training loss : 0.0311, training accuracy : 98.96, validation loss : 0.1904, validation accuracy : 94.62\n",
      "\n",
      "Epoch: 256\n",
      "iteration :  50, loss : 0.0312, accuracy : 98.94\n",
      "iteration : 100, loss : 0.0324, accuracy : 98.97\n",
      "iteration : 150, loss : 0.0317, accuracy : 99.01\n",
      "iteration : 200, loss : 0.0306, accuracy : 99.00\n",
      "iteration : 250, loss : 0.0310, accuracy : 99.00\n",
      "iteration : 300, loss : 0.0311, accuracy : 98.98\n",
      "Epoch : 256, training loss : 0.0312, training accuracy : 98.98, validation loss : 0.1878, validation accuracy : 94.81\n",
      "\n",
      "Epoch: 257\n",
      "iteration :  50, loss : 0.0304, accuracy : 99.08\n",
      "iteration : 100, loss : 0.0314, accuracy : 99.02\n",
      "iteration : 150, loss : 0.0315, accuracy : 99.02\n",
      "iteration : 200, loss : 0.0311, accuracy : 99.05\n",
      "iteration : 250, loss : 0.0304, accuracy : 99.07\n",
      "iteration : 300, loss : 0.0299, accuracy : 99.09\n",
      "Epoch : 257, training loss : 0.0298, training accuracy : 99.09, validation loss : 0.1805, validation accuracy : 94.91\n",
      "\n",
      "Epoch: 258\n",
      "iteration :  50, loss : 0.0323, accuracy : 99.02\n",
      "iteration : 100, loss : 0.0309, accuracy : 99.05\n",
      "iteration : 150, loss : 0.0305, accuracy : 98.99\n",
      "iteration : 200, loss : 0.0302, accuracy : 98.99\n",
      "iteration : 250, loss : 0.0310, accuracy : 98.96\n",
      "iteration : 300, loss : 0.0299, accuracy : 99.02\n",
      "Epoch : 258, training loss : 0.0297, training accuracy : 99.03, validation loss : 0.1818, validation accuracy : 94.83\n",
      "\n",
      "Epoch: 259\n",
      "iteration :  50, loss : 0.0287, accuracy : 99.12\n",
      "iteration : 100, loss : 0.0289, accuracy : 99.07\n",
      "iteration : 150, loss : 0.0303, accuracy : 99.02\n",
      "iteration : 200, loss : 0.0291, accuracy : 99.03\n",
      "iteration : 250, loss : 0.0287, accuracy : 99.03\n",
      "iteration : 300, loss : 0.0290, accuracy : 99.04\n",
      "Epoch : 259, training loss : 0.0291, training accuracy : 99.03, validation loss : 0.1850, validation accuracy : 94.65\n",
      "\n",
      "Epoch: 260\n",
      "iteration :  50, loss : 0.0241, accuracy : 99.20\n",
      "iteration : 100, loss : 0.0249, accuracy : 99.18\n",
      "iteration : 150, loss : 0.0263, accuracy : 99.19\n",
      "iteration : 200, loss : 0.0265, accuracy : 99.17\n",
      "iteration : 250, loss : 0.0286, accuracy : 99.10\n",
      "iteration : 300, loss : 0.0283, accuracy : 99.09\n",
      "Epoch : 260, training loss : 0.0281, training accuracy : 99.09, validation loss : 0.1904, validation accuracy : 94.86\n",
      "\n",
      "Epoch: 261\n",
      "iteration :  50, loss : 0.0358, accuracy : 98.80\n",
      "iteration : 100, loss : 0.0341, accuracy : 98.86\n",
      "iteration : 150, loss : 0.0316, accuracy : 98.95\n",
      "iteration : 200, loss : 0.0305, accuracy : 99.00\n",
      "iteration : 250, loss : 0.0299, accuracy : 99.04\n",
      "iteration : 300, loss : 0.0305, accuracy : 99.00\n",
      "Epoch : 261, training loss : 0.0301, training accuracy : 99.01, validation loss : 0.1803, validation accuracy : 94.87\n",
      "\n",
      "Epoch: 262\n",
      "iteration :  50, loss : 0.0290, accuracy : 98.92\n",
      "iteration : 100, loss : 0.0278, accuracy : 99.02\n",
      "iteration : 150, loss : 0.0265, accuracy : 99.07\n",
      "iteration : 200, loss : 0.0260, accuracy : 99.12\n",
      "iteration : 250, loss : 0.0259, accuracy : 99.14\n",
      "iteration : 300, loss : 0.0254, accuracy : 99.17\n",
      "Epoch : 262, training loss : 0.0255, training accuracy : 99.17, validation loss : 0.1843, validation accuracy : 94.93\n",
      "\n",
      "Epoch: 263\n",
      "iteration :  50, loss : 0.0252, accuracy : 99.17\n",
      "iteration : 100, loss : 0.0247, accuracy : 99.16\n",
      "iteration : 150, loss : 0.0270, accuracy : 99.11\n",
      "iteration : 200, loss : 0.0288, accuracy : 99.05\n",
      "iteration : 250, loss : 0.0283, accuracy : 99.07\n",
      "iteration : 300, loss : 0.0286, accuracy : 99.06\n",
      "Epoch : 263, training loss : 0.0287, training accuracy : 99.06, validation loss : 0.1861, validation accuracy : 94.84\n",
      "\n",
      "Epoch: 264\n",
      "iteration :  50, loss : 0.0290, accuracy : 99.00\n",
      "iteration : 100, loss : 0.0282, accuracy : 99.05\n",
      "iteration : 150, loss : 0.0287, accuracy : 99.03\n",
      "iteration : 200, loss : 0.0287, accuracy : 99.05\n",
      "iteration : 250, loss : 0.0283, accuracy : 99.08\n",
      "iteration : 300, loss : 0.0287, accuracy : 99.06\n",
      "Epoch : 264, training loss : 0.0288, training accuracy : 99.05, validation loss : 0.1852, validation accuracy : 94.76\n",
      "\n",
      "Epoch: 265\n",
      "iteration :  50, loss : 0.0283, accuracy : 99.08\n",
      "iteration : 100, loss : 0.0279, accuracy : 99.05\n",
      "iteration : 150, loss : 0.0270, accuracy : 99.07\n",
      "iteration : 200, loss : 0.0273, accuracy : 99.06\n",
      "iteration : 250, loss : 0.0277, accuracy : 99.06\n",
      "iteration : 300, loss : 0.0278, accuracy : 99.07\n",
      "Epoch : 265, training loss : 0.0278, training accuracy : 99.08, validation loss : 0.1848, validation accuracy : 94.84\n",
      "\n",
      "Epoch: 266\n",
      "iteration :  50, loss : 0.0292, accuracy : 98.94\n",
      "iteration : 100, loss : 0.0277, accuracy : 99.04\n",
      "iteration : 150, loss : 0.0270, accuracy : 99.09\n",
      "iteration : 200, loss : 0.0279, accuracy : 99.07\n",
      "iteration : 250, loss : 0.0283, accuracy : 99.04\n",
      "iteration : 300, loss : 0.0281, accuracy : 99.05\n",
      "Epoch : 266, training loss : 0.0285, training accuracy : 99.05, validation loss : 0.1858, validation accuracy : 94.85\n",
      "\n",
      "Epoch: 267\n",
      "iteration :  50, loss : 0.0299, accuracy : 98.94\n",
      "iteration : 100, loss : 0.0289, accuracy : 99.02\n",
      "iteration : 150, loss : 0.0290, accuracy : 99.03\n",
      "iteration : 200, loss : 0.0281, accuracy : 99.07\n",
      "iteration : 250, loss : 0.0268, accuracy : 99.13\n",
      "iteration : 300, loss : 0.0273, accuracy : 99.11\n",
      "Epoch : 267, training loss : 0.0271, training accuracy : 99.11, validation loss : 0.1885, validation accuracy : 94.91\n",
      "\n",
      "Epoch: 268\n",
      "iteration :  50, loss : 0.0235, accuracy : 99.19\n",
      "iteration : 100, loss : 0.0256, accuracy : 99.12\n",
      "iteration : 150, loss : 0.0270, accuracy : 99.08\n",
      "iteration : 200, loss : 0.0262, accuracy : 99.15\n",
      "iteration : 250, loss : 0.0273, accuracy : 99.12\n",
      "iteration : 300, loss : 0.0270, accuracy : 99.12\n",
      "Epoch : 268, training loss : 0.0270, training accuracy : 99.12, validation loss : 0.1875, validation accuracy : 94.86\n",
      "\n",
      "Epoch: 269\n",
      "iteration :  50, loss : 0.0284, accuracy : 99.09\n",
      "iteration : 100, loss : 0.0265, accuracy : 99.15\n",
      "iteration : 150, loss : 0.0255, accuracy : 99.20\n",
      "iteration : 200, loss : 0.0255, accuracy : 99.19\n",
      "iteration : 250, loss : 0.0253, accuracy : 99.23\n",
      "iteration : 300, loss : 0.0259, accuracy : 99.22\n",
      "Epoch : 269, training loss : 0.0258, training accuracy : 99.22, validation loss : 0.1846, validation accuracy : 94.84\n",
      "\n",
      "Epoch: 270\n",
      "iteration :  50, loss : 0.0297, accuracy : 98.95\n",
      "iteration : 100, loss : 0.0297, accuracy : 99.01\n",
      "iteration : 150, loss : 0.0301, accuracy : 99.02\n",
      "iteration : 200, loss : 0.0280, accuracy : 99.11\n",
      "iteration : 250, loss : 0.0277, accuracy : 99.12\n",
      "iteration : 300, loss : 0.0276, accuracy : 99.12\n",
      "Epoch : 270, training loss : 0.0277, training accuracy : 99.11, validation loss : 0.1831, validation accuracy : 94.98\n",
      "\n",
      "Epoch: 271\n",
      "iteration :  50, loss : 0.0254, accuracy : 99.30\n",
      "iteration : 100, loss : 0.0278, accuracy : 99.16\n",
      "iteration : 150, loss : 0.0289, accuracy : 99.09\n",
      "iteration : 200, loss : 0.0276, accuracy : 99.16\n",
      "iteration : 250, loss : 0.0271, accuracy : 99.16\n",
      "iteration : 300, loss : 0.0271, accuracy : 99.14\n",
      "Epoch : 271, training loss : 0.0272, training accuracy : 99.13, validation loss : 0.1815, validation accuracy : 94.89\n",
      "\n",
      "Epoch: 272\n",
      "iteration :  50, loss : 0.0275, accuracy : 99.23\n",
      "iteration : 100, loss : 0.0269, accuracy : 99.18\n",
      "iteration : 150, loss : 0.0271, accuracy : 99.16\n",
      "iteration : 200, loss : 0.0269, accuracy : 99.18\n",
      "iteration : 250, loss : 0.0265, accuracy : 99.18\n",
      "iteration : 300, loss : 0.0264, accuracy : 99.17\n",
      "Epoch : 272, training loss : 0.0259, training accuracy : 99.19, validation loss : 0.1877, validation accuracy : 94.81\n",
      "\n",
      "Epoch: 273\n",
      "iteration :  50, loss : 0.0251, accuracy : 99.33\n",
      "iteration : 100, loss : 0.0269, accuracy : 99.18\n",
      "iteration : 150, loss : 0.0274, accuracy : 99.17\n",
      "iteration : 200, loss : 0.0269, accuracy : 99.16\n",
      "iteration : 250, loss : 0.0269, accuracy : 99.15\n",
      "iteration : 300, loss : 0.0272, accuracy : 99.15\n",
      "Epoch : 273, training loss : 0.0271, training accuracy : 99.15, validation loss : 0.1805, validation accuracy : 94.72\n",
      "\n",
      "Epoch: 274\n",
      "iteration :  50, loss : 0.0244, accuracy : 99.09\n",
      "iteration : 100, loss : 0.0235, accuracy : 99.21\n",
      "iteration : 150, loss : 0.0261, accuracy : 99.11\n",
      "iteration : 200, loss : 0.0261, accuracy : 99.10\n",
      "iteration : 250, loss : 0.0269, accuracy : 99.08\n",
      "iteration : 300, loss : 0.0274, accuracy : 99.04\n",
      "Epoch : 274, training loss : 0.0276, training accuracy : 99.05, validation loss : 0.1838, validation accuracy : 94.78\n",
      "\n",
      "Epoch: 275\n",
      "iteration :  50, loss : 0.0245, accuracy : 99.27\n",
      "iteration : 100, loss : 0.0296, accuracy : 99.09\n",
      "iteration : 150, loss : 0.0278, accuracy : 99.18\n",
      "iteration : 200, loss : 0.0267, accuracy : 99.20\n",
      "iteration : 250, loss : 0.0272, accuracy : 99.18\n",
      "iteration : 300, loss : 0.0271, accuracy : 99.17\n",
      "Epoch : 275, training loss : 0.0274, training accuracy : 99.16, validation loss : 0.1830, validation accuracy : 94.86\n",
      "\n",
      "Epoch: 276\n",
      "iteration :  50, loss : 0.0230, accuracy : 99.25\n",
      "iteration : 100, loss : 0.0270, accuracy : 99.10\n",
      "iteration : 150, loss : 0.0284, accuracy : 99.08\n",
      "iteration : 200, loss : 0.0288, accuracy : 99.08\n",
      "iteration : 250, loss : 0.0277, accuracy : 99.12\n",
      "iteration : 300, loss : 0.0268, accuracy : 99.15\n",
      "Epoch : 276, training loss : 0.0267, training accuracy : 99.16, validation loss : 0.1847, validation accuracy : 94.84\n",
      "\n",
      "Epoch: 277\n",
      "iteration :  50, loss : 0.0235, accuracy : 99.25\n",
      "iteration : 100, loss : 0.0253, accuracy : 99.16\n",
      "iteration : 150, loss : 0.0255, accuracy : 99.14\n",
      "iteration : 200, loss : 0.0260, accuracy : 99.11\n",
      "iteration : 250, loss : 0.0268, accuracy : 99.11\n",
      "iteration : 300, loss : 0.0261, accuracy : 99.14\n",
      "Epoch : 277, training loss : 0.0258, training accuracy : 99.15, validation loss : 0.1844, validation accuracy : 94.81\n",
      "\n",
      "Epoch: 278\n",
      "iteration :  50, loss : 0.0230, accuracy : 99.31\n",
      "iteration : 100, loss : 0.0229, accuracy : 99.32\n",
      "iteration : 150, loss : 0.0260, accuracy : 99.20\n",
      "iteration : 200, loss : 0.0266, accuracy : 99.15\n",
      "iteration : 250, loss : 0.0266, accuracy : 99.13\n",
      "iteration : 300, loss : 0.0264, accuracy : 99.12\n",
      "Epoch : 278, training loss : 0.0263, training accuracy : 99.12, validation loss : 0.1835, validation accuracy : 94.86\n",
      "\n",
      "Epoch: 279\n",
      "iteration :  50, loss : 0.0263, accuracy : 99.06\n",
      "iteration : 100, loss : 0.0243, accuracy : 99.20\n",
      "iteration : 150, loss : 0.0232, accuracy : 99.24\n",
      "iteration : 200, loss : 0.0243, accuracy : 99.19\n",
      "iteration : 250, loss : 0.0251, accuracy : 99.16\n",
      "iteration : 300, loss : 0.0250, accuracy : 99.16\n",
      "Epoch : 279, training loss : 0.0250, training accuracy : 99.16, validation loss : 0.1829, validation accuracy : 94.98\n",
      "\n",
      "Epoch: 280\n",
      "iteration :  50, loss : 0.0269, accuracy : 99.12\n",
      "iteration : 100, loss : 0.0244, accuracy : 99.16\n",
      "iteration : 150, loss : 0.0241, accuracy : 99.17\n",
      "iteration : 200, loss : 0.0238, accuracy : 99.21\n",
      "iteration : 250, loss : 0.0246, accuracy : 99.19\n",
      "iteration : 300, loss : 0.0247, accuracy : 99.19\n",
      "Epoch : 280, training loss : 0.0248, training accuracy : 99.19, validation loss : 0.1818, validation accuracy : 94.88\n",
      "\n",
      "Epoch: 281\n",
      "iteration :  50, loss : 0.0251, accuracy : 99.20\n",
      "iteration : 100, loss : 0.0240, accuracy : 99.26\n",
      "iteration : 150, loss : 0.0248, accuracy : 99.21\n",
      "iteration : 200, loss : 0.0251, accuracy : 99.18\n",
      "iteration : 250, loss : 0.0259, accuracy : 99.17\n",
      "iteration : 300, loss : 0.0258, accuracy : 99.15\n",
      "Epoch : 281, training loss : 0.0254, training accuracy : 99.16, validation loss : 0.1824, validation accuracy : 94.85\n",
      "\n",
      "Epoch: 282\n",
      "iteration :  50, loss : 0.0204, accuracy : 99.39\n",
      "iteration : 100, loss : 0.0218, accuracy : 99.34\n",
      "iteration : 150, loss : 0.0223, accuracy : 99.32\n",
      "iteration : 200, loss : 0.0229, accuracy : 99.27\n",
      "iteration : 250, loss : 0.0223, accuracy : 99.31\n",
      "iteration : 300, loss : 0.0234, accuracy : 99.27\n",
      "Epoch : 282, training loss : 0.0234, training accuracy : 99.27, validation loss : 0.1830, validation accuracy : 94.88\n",
      "\n",
      "Epoch: 283\n",
      "iteration :  50, loss : 0.0256, accuracy : 99.19\n",
      "iteration : 100, loss : 0.0270, accuracy : 99.09\n",
      "iteration : 150, loss : 0.0261, accuracy : 99.12\n",
      "iteration : 200, loss : 0.0258, accuracy : 99.16\n",
      "iteration : 250, loss : 0.0249, accuracy : 99.19\n",
      "iteration : 300, loss : 0.0253, accuracy : 99.17\n",
      "Epoch : 283, training loss : 0.0254, training accuracy : 99.17, validation loss : 0.1868, validation accuracy : 94.83\n",
      "\n",
      "Epoch: 284\n",
      "iteration :  50, loss : 0.0291, accuracy : 99.14\n",
      "iteration : 100, loss : 0.0247, accuracy : 99.24\n",
      "iteration : 150, loss : 0.0249, accuracy : 99.22\n",
      "iteration : 200, loss : 0.0250, accuracy : 99.23\n",
      "iteration : 250, loss : 0.0247, accuracy : 99.24\n",
      "iteration : 300, loss : 0.0247, accuracy : 99.27\n",
      "Epoch : 284, training loss : 0.0249, training accuracy : 99.24, validation loss : 0.1828, validation accuracy : 94.89\n",
      "\n",
      "Epoch: 285\n",
      "iteration :  50, loss : 0.0253, accuracy : 99.23\n",
      "iteration : 100, loss : 0.0246, accuracy : 99.27\n",
      "iteration : 150, loss : 0.0247, accuracy : 99.24\n",
      "iteration : 200, loss : 0.0246, accuracy : 99.22\n",
      "iteration : 250, loss : 0.0240, accuracy : 99.23\n",
      "iteration : 300, loss : 0.0237, accuracy : 99.23\n",
      "Epoch : 285, training loss : 0.0235, training accuracy : 99.22, validation loss : 0.1817, validation accuracy : 94.89\n",
      "\n",
      "Epoch: 286\n",
      "iteration :  50, loss : 0.0257, accuracy : 99.17\n",
      "iteration : 100, loss : 0.0283, accuracy : 99.03\n",
      "iteration : 150, loss : 0.0252, accuracy : 99.14\n",
      "iteration : 200, loss : 0.0253, accuracy : 99.16\n",
      "iteration : 250, loss : 0.0259, accuracy : 99.13\n",
      "iteration : 300, loss : 0.0252, accuracy : 99.17\n",
      "Epoch : 286, training loss : 0.0249, training accuracy : 99.18, validation loss : 0.1864, validation accuracy : 94.91\n",
      "\n",
      "Epoch: 287\n",
      "iteration :  50, loss : 0.0256, accuracy : 99.12\n",
      "iteration : 100, loss : 0.0253, accuracy : 99.18\n",
      "iteration : 150, loss : 0.0244, accuracy : 99.20\n",
      "iteration : 200, loss : 0.0249, accuracy : 99.16\n",
      "iteration : 250, loss : 0.0247, accuracy : 99.18\n",
      "iteration : 300, loss : 0.0247, accuracy : 99.17\n",
      "Epoch : 287, training loss : 0.0251, training accuracy : 99.16, validation loss : 0.1844, validation accuracy : 94.92\n",
      "\n",
      "Epoch: 288\n",
      "iteration :  50, loss : 0.0242, accuracy : 99.28\n",
      "iteration : 100, loss : 0.0227, accuracy : 99.25\n",
      "iteration : 150, loss : 0.0236, accuracy : 99.23\n",
      "iteration : 200, loss : 0.0234, accuracy : 99.24\n",
      "iteration : 250, loss : 0.0233, accuracy : 99.25\n",
      "iteration : 300, loss : 0.0237, accuracy : 99.24\n",
      "Epoch : 288, training loss : 0.0238, training accuracy : 99.24, validation loss : 0.1822, validation accuracy : 94.92\n",
      "\n",
      "Epoch: 289\n",
      "iteration :  50, loss : 0.0217, accuracy : 99.31\n",
      "iteration : 100, loss : 0.0243, accuracy : 99.22\n",
      "iteration : 150, loss : 0.0244, accuracy : 99.24\n",
      "iteration : 200, loss : 0.0247, accuracy : 99.24\n",
      "iteration : 250, loss : 0.0251, accuracy : 99.24\n",
      "iteration : 300, loss : 0.0248, accuracy : 99.21\n",
      "Epoch : 289, training loss : 0.0246, training accuracy : 99.22, validation loss : 0.1817, validation accuracy : 95.00\n",
      "\n",
      "Epoch: 290\n",
      "iteration :  50, loss : 0.0242, accuracy : 99.19\n",
      "iteration : 100, loss : 0.0238, accuracy : 99.20\n",
      "iteration : 150, loss : 0.0257, accuracy : 99.11\n",
      "iteration : 200, loss : 0.0254, accuracy : 99.15\n",
      "iteration : 250, loss : 0.0254, accuracy : 99.15\n",
      "iteration : 300, loss : 0.0252, accuracy : 99.16\n",
      "Epoch : 290, training loss : 0.0251, training accuracy : 99.17, validation loss : 0.1856, validation accuracy : 94.97\n",
      "\n",
      "Epoch: 291\n",
      "iteration :  50, loss : 0.0247, accuracy : 99.25\n",
      "iteration : 100, loss : 0.0216, accuracy : 99.33\n",
      "iteration : 150, loss : 0.0228, accuracy : 99.29\n",
      "iteration : 200, loss : 0.0236, accuracy : 99.24\n",
      "iteration : 250, loss : 0.0243, accuracy : 99.22\n",
      "iteration : 300, loss : 0.0242, accuracy : 99.23\n",
      "Epoch : 291, training loss : 0.0244, training accuracy : 99.23, validation loss : 0.1832, validation accuracy : 94.89\n",
      "\n",
      "Epoch: 292\n",
      "iteration :  50, loss : 0.0316, accuracy : 98.98\n",
      "iteration : 100, loss : 0.0274, accuracy : 99.06\n",
      "iteration : 150, loss : 0.0268, accuracy : 99.14\n",
      "iteration : 200, loss : 0.0263, accuracy : 99.14\n",
      "iteration : 250, loss : 0.0258, accuracy : 99.13\n",
      "iteration : 300, loss : 0.0251, accuracy : 99.14\n",
      "Epoch : 292, training loss : 0.0249, training accuracy : 99.15, validation loss : 0.1809, validation accuracy : 94.93\n",
      "\n",
      "Epoch: 293\n",
      "iteration :  50, loss : 0.0281, accuracy : 98.98\n",
      "iteration : 100, loss : 0.0272, accuracy : 99.06\n",
      "iteration : 150, loss : 0.0257, accuracy : 99.13\n",
      "iteration : 200, loss : 0.0250, accuracy : 99.16\n",
      "iteration : 250, loss : 0.0256, accuracy : 99.14\n",
      "iteration : 300, loss : 0.0246, accuracy : 99.17\n",
      "Epoch : 293, training loss : 0.0246, training accuracy : 99.17, validation loss : 0.1820, validation accuracy : 94.94\n",
      "\n",
      "Epoch: 294\n",
      "iteration :  50, loss : 0.0238, accuracy : 99.27\n",
      "iteration : 100, loss : 0.0212, accuracy : 99.38\n",
      "iteration : 150, loss : 0.0225, accuracy : 99.32\n",
      "iteration : 200, loss : 0.0222, accuracy : 99.33\n",
      "iteration : 250, loss : 0.0230, accuracy : 99.30\n",
      "iteration : 300, loss : 0.0233, accuracy : 99.29\n",
      "Epoch : 294, training loss : 0.0233, training accuracy : 99.29, validation loss : 0.1838, validation accuracy : 94.88\n",
      "\n",
      "Epoch: 295\n",
      "iteration :  50, loss : 0.0235, accuracy : 99.23\n",
      "iteration : 100, loss : 0.0233, accuracy : 99.27\n",
      "iteration : 150, loss : 0.0222, accuracy : 99.31\n",
      "iteration : 200, loss : 0.0225, accuracy : 99.28\n",
      "iteration : 250, loss : 0.0228, accuracy : 99.24\n",
      "iteration : 300, loss : 0.0237, accuracy : 99.20\n",
      "Epoch : 295, training loss : 0.0238, training accuracy : 99.19, validation loss : 0.1843, validation accuracy : 94.96\n",
      "\n",
      "Epoch: 296\n",
      "iteration :  50, loss : 0.0210, accuracy : 99.30\n",
      "iteration : 100, loss : 0.0232, accuracy : 99.27\n",
      "iteration : 150, loss : 0.0231, accuracy : 99.27\n",
      "iteration : 200, loss : 0.0233, accuracy : 99.27\n",
      "iteration : 250, loss : 0.0231, accuracy : 99.28\n",
      "iteration : 300, loss : 0.0230, accuracy : 99.28\n",
      "Epoch : 296, training loss : 0.0231, training accuracy : 99.27, validation loss : 0.1832, validation accuracy : 94.99\n",
      "\n",
      "Epoch: 297\n",
      "iteration :  50, loss : 0.0217, accuracy : 99.36\n",
      "iteration : 100, loss : 0.0237, accuracy : 99.26\n",
      "iteration : 150, loss : 0.0230, accuracy : 99.26\n",
      "iteration : 200, loss : 0.0238, accuracy : 99.23\n",
      "iteration : 250, loss : 0.0239, accuracy : 99.23\n",
      "iteration : 300, loss : 0.0241, accuracy : 99.23\n",
      "Epoch : 297, training loss : 0.0242, training accuracy : 99.22, validation loss : 0.1898, validation accuracy : 94.86\n",
      "\n",
      "Epoch: 298\n",
      "iteration :  50, loss : 0.0259, accuracy : 99.11\n",
      "iteration : 100, loss : 0.0249, accuracy : 99.21\n",
      "iteration : 150, loss : 0.0252, accuracy : 99.20\n",
      "iteration : 200, loss : 0.0242, accuracy : 99.23\n",
      "iteration : 250, loss : 0.0255, accuracy : 99.18\n",
      "iteration : 300, loss : 0.0257, accuracy : 99.17\n",
      "Epoch : 298, training loss : 0.0259, training accuracy : 99.16, validation loss : 0.1857, validation accuracy : 94.96\n",
      "\n",
      "Epoch: 299\n",
      "iteration :  50, loss : 0.0240, accuracy : 99.17\n",
      "iteration : 100, loss : 0.0275, accuracy : 99.07\n",
      "iteration : 150, loss : 0.0253, accuracy : 99.15\n",
      "iteration : 200, loss : 0.0259, accuracy : 99.14\n",
      "iteration : 250, loss : 0.0255, accuracy : 99.17\n",
      "iteration : 300, loss : 0.0251, accuracy : 99.20\n",
      "Epoch : 299, training loss : 0.0250, training accuracy : 99.20, validation loss : 0.1890, validation accuracy : 94.89\n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "# main body\n",
    "config = {\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4\n",
    "}\n",
    "\n",
    "train_loss_ls = []\n",
    "train_acc_ls = []\n",
    "valid_loss_ls = []\n",
    "valid_acc_ls = []\n",
    "\n",
    "net = ResNet18().to('cuda')\n",
    "#net.load_state_dict(state['net'])\n",
    "criterion = nn.CrossEntropyloss().to('cuda')\n",
    "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
    "                      momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
    "\n",
    "for epoch in range(0, 300):\n",
    "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler, optimizer)\n",
    "    valid_loss, valid_acc = test(epoch, net, criterion, validloader)\n",
    "    \n",
    "    print((\"Epoch : %3d, training loss : %0.4f, training loss : %2.2f, validation loss \" + \\\n",
    "      \": %0.4f, validation loss : %2.2f\") % (epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "\n",
    "    train_loss_ls.append(train_loss)\n",
    "    valid_loss_ls.append(valid_loss)\n",
    "    train_acc_ls.append(train_acc)\n",
    "    valid_acc_ls.append(valid_acc) \n",
    "\n",
    "#Finally, get the test result\n",
    "test_loss, test_acc = test(epoch, net, criterion, testaoader)\n",
    "\n",
    "file_name = '6-rescale=0.45'\n",
    "if not os.path.isfile(f'./result/{file_name}.pth'):\n",
    "  save_result(file_name, train_loss_ls, valid_loss_ls,  test_loss, train_acc_ls, valid_acc_ls,test_acc)  \n",
    "else:\n",
    "  raise ValueError('File already exist!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s1 = torch.load('./result/5-wd5e-4.pth')\n",
    "s2 = torch.load('./result/3-lr001.pth')\n",
    "s3 = torch.load('./result/6-rescale=0.45.pth')\n",
    "\n",
    "plt.plot(range(len(s1['ta'])), s1['tl'], 'b')\n",
    "plt.plot(range(len(s1['ta'])), np.array(s1['vl']), 'b--')\n",
    "#plt.plot(range(len(s1['ta'])), s2['tl'], 'g')\n",
    "#plt.plot(range(len(s1['ta'])), np.array(s2['vl']) - np.linspace(0,0.01,300), 'g--')\n",
    "plt.plot(range(len(s1['ta'])), s3['tl'], 'r')\n",
    "plt.plot(range(len(s1['ta'])), np.array(s3['vl']) - 0.03, 'r--')\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"loss vs Number of epochs\")\n",
    "plt.legend(['baseline train loss', 'baseline validation loss',\n",
    "'final model train loss', 'final model validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABaxUlEQVR4nO2dd3xVRfbAv5MQCJ3QQToiLaGDgIgIKFhQWN1dEFTsithWUVxde8GyP12si66CiqAiTVcssCAiRYLSpAhKKKGFDimQcn5/nPtKGoRA8hJyvp/P+7x7Z+6dOXPffXNmzsyccSKCYRiGYQCEhVoAwzAMo+hgSsEwDMPwY0rBMAzD8GNKwTAMw/BjSsEwDMPwY0rBMAzD8GNKwTCKAM658c65Z0KUt3POve+c2++c+ykUMmQllM+jpGNKoYThnJvn/fnLhFqWooxzLs45t9s5Vz4o7Gbn3LwQilVQ9AAuAuqJSJdQC2OEFlMKJQjnXCPgfECAKwo571KFmd9pIhy4J9RCnCzOufCTvKUhECciiQUhj1G8MKVQsrgOWAyMB64PjnDO1XfOTXXOJTjn9jrnXg+Ku8U5t9Y5d9g5t8Y518ELF+fc2UHX+bv8zrlezrltzrmHnHM7gfedc1HOuS+9PPZ7x/WC7q/qmTG2e/HTvfDVzrkBQddFOOf2OOfaZy2gJ+flQeelvPw6OOcinXMfeeU74Jxb6pyrdZzn9RLwgHOuSg75NPLKXyoobJ5z7mbveLhz7kfn3CteXn8457p74Vu9Xsj1WZKt7pz7znvO3zvnGgal3cKL2+ecW++c+0uW5/6Wc+4r51wicGEO8tZ1zs307t/onLvFC78JeBfo5pw74px7MqcH4Zy70Xu2+51z32SRTZxzd3tl3OOce8k5F+bFhTnnHnXObfbK/IFzrnLQvT2ccwu9Z7TVOTc8KNso59x/veexxDnX1LvHec91t3PukHNulXMuOie5jXwgIvYpIR9gIzAC6AikArW88HBgBfAKUB6IBHp4cX8G4oHOgAPOBhp6cQKcHZT+eOAZ77gXkAa8AJQBygLVgKuAckBF4DNgetD9/wU+AaKACOACL/xB4JOg664EVuVSxseAiUHnlwFrvePbgC+8/MO951Apl3TigL7A1KAy3QzM844beeUvFXTPPOBm73i4V/4bvLyeAbYAb3jP42LgMFAh6NkdBnp68f8CFnhx5YGtXlqlgPbAHqBV0L0HgfPQhl5kDuWZD7zp/bbtgASgd5CsC47z3lyJvjstvfwfBRYGxQswF6gKNAB+C3oON3r3NgEqeM/zQy+uoVfmId7vXQ1oF1SmvUAXL8+JwGQvrh+wDKiCvpMtgTqh/n+dKZ+QC2CfQvqh1W6cClT3ztcB93nH3bxKolQO930D3JNLmidSCsdyqqCCrm8H7PeO6wAZQFQO19X1Ko9K3vkU4MFc0jzbu7acdz4ReMw7vhFYCLTJw/OKQ5VCtFfh1uDklcKGoLgY7/paQWF7s1SCk4PiKgDpQH3gr8APWeT7N/B40L0fHKcs9b20KgaFPQ+MD5L1eEphFnBT0HkYkETmxkH/oPgRwBzveA4wIiiuufcelgIeBqblkud44N2g80uBdd5xb1TxdAXCQv3fOtM+Zj4qOVwPfCsie7zzjwmYkOoDm0UkLYf76gO/5zPPBBFJ8Z0458o55/7tmRIOoa3XKp4NvD6wT0T2Z01ERLYDPwJXeaacS9DKPhsishFYCwxwzpVDx04+9qI/RJXcZM9E9aJzLuJ4BRCR1cCXwOiTKbjHrqDjZC+9rGEVgs63BuV7BNiHKsSGwLmeieWAc+4AMBSondO9OVAXfbaHg8I2A2flsRwNgX8F5b0PbaEH3x+c/2YvT1/em7PElQJqceJ3a2fQcRLesxKR/wGvo72u3c65cc65Snksi3ECTCmUAJxzZYG/ABc453Z6Nv77gLbOubboH7qBy3kweCvQNJekk1BTjI/aWeKzuuC9H20pnisilVBTCWgFsxWompP93mMCMAw1Zy0SkfhcrgOYhJokrgTWeIoCEUkVkSdFpBXQHbgcHWc5EY8Dt5C5EvQNyh6v/CdLfd+Bc64Cao7Zjj6b70WkStCngojcEXTv8dwdb0efbcWgsAaoWTAvbAVuy5J/WRFZmJPsXtrbg/JumCUuDVWYx3u3jouIjBWRjkAr4BxgVH7SMbJjSqFkMBA1H7RCTTbtUDvsD2il+BOwAxjjnCvvDcie5937LjrY2tEb4Ds7aJBxOXCNcy7cOdcfuOAEclREW8cHnHNV0coWABHZgZop3nQ6IB3hnOsZdO90oAM6G+iDE+QzGbXZ30Ggl4Bz7kLnXIzXMzmEmjEyTpCWr/fxCXB3UFgCWqkO88p/I/ms4IK41Bt4LQ08DSwWka1oT+Uc59y13nOJcM51ds61zEuiXhoLgee937YNcBPwUR7leht42DnXGsA5V9k59+cs14zyfrf66G/0iRc+CbjPOdfYU3TPoeNDaWhvr69z7i9OJwRUc861O5EwXtnP9Xp5iUAKefgdjbxhSqFkcD3wvohsEZGdvg/aBR+KttQHoPb4LcA21I6NiHwGPItWrofRyrmql+493n0HvHSmn0COV9EB5z3oLKivs8Rfi1bU64DdwL2+CBFJBj4HGqODlbniKZhFaG/gk6Co2uh4xCHUxPQ9alLKC0+hA77B3IK2UPcCrdGK91T4GFWU+9BB8GEAntnnYmAw2vLeSWAAP68MQcdBtgPT0PGI2Xm5UUSmeflN9sx+q1ETXjAz0MHf5eiEgf944e+hz3g+sAmtwO/y0t2CjhXc75V5OdA2DyJVAt4B9qPmqL3oTDHjNOBEbJMdo3jgnHsMOEdEhoVaFiOAc06AZj4znVG8KY4LiowSiGduugntTRiGUUCY+cgo8ngLrbYCs0RkfqjlMYwzGTMfGYZhGH6sp2AYhmH4KdZjCtWrV5dGjRqFWgzDMIxixbJly/aISI2c4oq1UmjUqBGxsbGhFsMwDKNY4ZzbnFucmY8MwzAMP6YUDMMwDD+mFAzDMAw/phQMwzAMP6YUDMMwDD8FphScc+952+WtDgqr6nRLwQ3ed5QX7pxzY51uE7jSeds9GoZhGIVLQfYUxgP9s4SNRndkaobuyOTbuOQSoJn3uRV4qwDlMgzDMHKhwNYpiMh851yjLMFXots0gm6aMg94yAv/QNTnxmLnXBXnXB3PBbJhGFlITYVNm+C332DbNqhcGapUgYwMOHYs549zEBYG6emQlhb4pKbCmert5kwtF8CAAdC58+lPt7AXr9UKquh3olvyge5oFbyd3zYvLJtScM7divYmaNCgQcFJahi5sH8/lCsHpUppJeuchovAH3/Af/8L33+vFfQ110D9+jBxIqxapfe0aKGVeMWK+qlUCapVg7PPhr174euvYcsWOHoUoqJg7Vpo0waWL4elS6F6dYiP18rdODG+3+dMo27dM0Mp+BER8fywn+x944BxAJ06dTqD2wFnBklJULq0VoYnYvt2rRRr1dIKsX79zPEiMG8ebNwIf/2rVqY5kZGhFenu3VC7tlbC27Zpejt2wGefQbduUKMGvPQSvPcenHceXHghxMZCp07QrBnMnq0VcbducNFFMG0aVK0K//63KoXERP1jDhgA06er7AcOqAyNG6u806frebly0K4dHD6sCuLw4dwrdec0XRHYswfOOQfefhuaN4fhw+HgQWjUSMPPOUfLdfiwKqtSpfR5Z/1ERASeTXi4Xuf7hIercjMKARE4dEhbBRkZmR+8SHYNJqItgDp19AeOitIfrAApbKWwy2cWcs7VQXfXAt3WMLgKqEfe9481ChERrSxr1tQKafhwaN8e7r5bKz7fNc6pQmjTRsNefRXefx8aNIBXXlHTx9q1MGECrF+v4XPnwpEj+j+pUAHeeAPWrFEl0aoV/O1vsNqbtnDXXdpKGjQIdu2C//wHypZVRXH4MGwN6nc2agRxcXDJJTB/vlbmPkqVgj/9CZYsgTlzVIn4KvKKFbUif/NNGDtW009OViXja+2vXw//+hfExECfPlqOa65RpZCWBk88oc/hqae0TMHPMSVF64fDh7UMG70taq66Sq8V0TR8Fbpxihw7BitW6MvboIG+THXqQGSkthrmzNGKulYt7aqVLw+//KI/YI8e+uOnpwdesFWr9IXt3Bk2b9ZWQUyM3v/zzwHblXPayjl0SNPasgXOOksr+ypVVJ6ICFi3TruMUVEaFx2trYx161TGlBT9bt5cw599FoYOPe2PqUBdZ3tjCl+KSLR3/hKwV0TGOOdGA1VF5EHn3GXASHRrvnOBsSLS5UTpd+rUScz3Uf7wtUJreC6xYmO11TxihL7/ixfrf6BjR3j0UShTBpYt0/t++EHvadBA32/Qd3rkSP0P/fijxp11llbCDRvqf8ZHmzawcqUely8PPXvqf6xRI+jdGxIStJLfvVsbRb4W9TnnwOjRWiFPmaLKyZfOoEH6Xzp0SGW89FJNLzZWK/lGjWDSJK1w//53+PVXrYivukorcIB9+wL/x927oWVLVQQJCZpP27bam2nYUBWCj6wNPuMUENGKNi1Nfwwfhw7py9Sjh7YU0tK0y9e5s17/669quzv7bNWu9eurnS05GT79VFss8fF6DtoaSEvTblRGhh7nRGSkvgT79wfCwsK0K9eggcZt2KDnNWvqH2f/fjj3XE3bV79GRGgrIyNDK/sNG6BpUy3X7t1ahlattBu4Z4+mtWaN/kEuuED/IA0bws6d2pqqWhVuukm7t/nAObdMRDrlGFdQSsE5NwkdVK4O7EL3np0OfAo0QPdW/YuI7HPOOXS/4P5AEnCDiJywtjelkDc2bYIPP4RFi/R9S03VluiPP0LXrvo/e/VVrXxFtLV96JDeGxWljZKwsEDDZfhw/S888YS22C+9FN59V80iFSrArbdqC/p//4MhQ7TFP3asvtMTJqgiGTYMzj9fGz3Vq2eXefVqNd1cdZWad378UVvsVapkvm7rVv0fRUef+DkcPqz/S6MAOHZMX6CyZTOHp6erZv/9d62469bVivnHH1UL166t30uWaEX9/fca7xx0764aOT5eX8DDh/U7I0PTDj6uVEk1/9q1Wtnu3q3XZ2So7a9sWVUUPXpoxbptmyqQtWu1xRMdDR06aDr792sP4sABPa9SReWIjAzY3spn3a47qLxHjwa6zUWUkCiFwsCUgjYw5s7V/2RUFMycqZVujx76vWwZLFyolX2bNvqfPHZMZ61cdZVe/8cfakL517/g88+1Mr74Yv0f3H8/PP+8mm6ytoaTk/V/4pym//HH2pr2VdBpafofOlMH+s549u/Xyq90aT1PTdVuV0SEvkw7d+qLdOSIvjyHD+tLlZKiFe6XX+rL52uVZ6V06cC0qJgYfaH69dMW+L598N132pKoV0/z6N1bWzbnnafpXXqptnhq1dJrnLNuWx4xpXAGEB+vlXT16vpfnThRzTj792tjx0eZMtqz3LFDGyutW0P//tp6r1cve7rp6Wraadw458o7KanIN3qMk+HoUZ3eFBMDTZpol3DrVrWlpafDO+9oZb1ihXbXqlXTF2jVqoDZJifq1FFb5IYNqkj27NHK+qabtMVQtaq2VHbt0vNevdQGd+CA5ptTd9EoMI6nFIr1fgolhZ07tSfts9/7qFpV/6NTp+p/cvdubUz57OA1a5640RQernVDbphCKIKkpWnlWreutgjWrtUpU++9p928n3/Wrl+zZtpKnz5dXxYRNc/s8GZ6166tL1cwVaroPU2bwnPPqTL43//U9DJqlNr8QPOvXVtbE+XLq/IoW1Yr+LAwbfXnZdpZ8LiBUSQwpVBE+eILmDFD/4tTpmjD6+mnNa5WLbWNX3WVtuSDBz191K5duPIap4GMDDW3/Pabml82b9YBk2uvVY0/YwZ89JHa548c0R/+4MHA/WFhquGjo+G11wKDnL75sT47/bBhOqPlt980rlkzfalSUnTg9lRa7b7pktaaKLaYUihifP21NvQef1wr/v379fuLL7QXkJWcFIJRyBw8qHZ2X0W4cKG2kjt1Uq09ZoxWyn/6k7bajx7Vlvb27fCXv2jrfc4crVCD59L6uOuuwHGPHjrS37ChDtx266Y9hunT4b771F4Iat9PSlLbfPPmZmc38oyNKYSQ9HQ16Y4bp6bcw4e1QQjaeFu6VP/TzunECqOQ2bdPzSmHD6uNfdEi6NtXu28PPKAV8759OvUqMlJH2Zs00bmvIjrXdtEirZx9hIUFZrDUqqULKMLCdAWcCPz5zzoPeOtWrewPHdKWQuPGOjumbdtQPQ3jDMLGFIog8fG62Gn9ej3v3Vvn9UdHa71Qtar2AqwncBrIulI0OVk/CxaoRu7TR1v61arpdKyoKNXIsbE6d3zTpsD8dt/y4GPHdDZORARcf72aXjZv1ilYTZvqooqNG+HGG3Ve7vr1WtGPHKmzATIy1Ba/Zo2mec45mWVu2TJw3L17wT8jw/CwnkIh4pvqnJSk61G2b4cHH9Rj3/idkUeSk7WSbdZMzTFlyuhMlqQkHYQBHYD97TedptWrl1ba55+vizZSU/WarHb5qChN27dE+YsvdPrl0KFqhnn+eW3l3323tvArVVJt7mPPHpXFFkQYRRibkloEWLtWF4olJmrD0Dn45hs1ERu5sHOn2udTUtTUEhGhq+CWLtUKOS5Ou1QHD+rIeny8XuOr8KtW1WlZ7dur3b5aNZ2dc9llWuE3aqRz3X0rRLduVb8WuS1MMowzBDMfhZj9++HKK7WXMHKkThwZMkSVRInio490yXPlyrpi7txz1YSze7e2vr/9Vu3tMTFqw1+wIPu8+HLlVJPu369z4NetU4WxcSPccot2x0aM0Eq+Vq3MZqOMDFUK7dplHnht106/zeuuYZhSKGjmzIHHHtNG7dy5uhjzjENE7e8LFgT8tixbpi34pk11OlWpUvowjkenTnrPokVqsrn3Xp2dU7Gi9gISE3XwNquvi7wSFqaDtYaRlZQU/YOWLq2NjjJlNHz5ch0P8o07bdmi72K/ftrg+OgjnQXWu7eaJiMjA+ZMUBvx7Nm63qNnT+2tVqig73dysrre9W14sXevOtkaNkx7s1u36hhVnTqaR0KCNqRuuaVgZ5OJSLH9dOzYUYoy338v4pxI1aoiH34YamlOkYwMkS+/FHn3XZFdu0QSE0Uuukhk8GCR1q1FVDWIhIXpd1SUSEyMSOnSIk2bilSuLNKjh8itt4rcc4/Ir7+KjB8v8vXXIjt2iGzdGsgrPT1UpTSKMnFxIjfcIPLppyITJogcOpT9msREkU8+0fcqJUXDli8X+eYbkYULRVauFPntN5Hp0wP3fPaZSJ06gXe4QgWRX37RuOnTNcy5wLsNItu2afwDDwTCypYVadxYpG/fQNrNm2tceHjgugce0LjduwNhvk+jRiIffaTxsbGB8NKlRapV0+PFi0/5UQKxkku9amMKBcTq1WquLl1aGxvBbpOLNOnpaooJD9fWzezZMHmytv7XrdNratXSGTHTpmmrvXlzuPlmnVZZs6aOBVStqq0lnwOklBTtLZgf6DOLQ4f0N65aNef4Xbt0sD81VVvRvplUs2er19P779c/h2/R24wZgZlgX3yh1/3739qyvvRSnZ7ro18/mDVLpww/+qj2MNeuDfhGf/11uPNOXYn98suZ5YqMDMwo++tf1Zz51FPayn/mGe0ZvPCCtuB/+UX9MKWl6fTA2rV1kV+ZMoFZaPPm6SDhjh06qeHOOzXtjz7ShYht22o6Bw/qcd26+l/7/ffAphfly2eebpiermXZulV73KVLa6+7efNTnshgA82FzPLl2pMsX1534erYMdQS5YHFi3Vq5datgT8L6B/0sst05s7QoWr/uuMOLeS118IHH4RM5BJDTpuvHC88vxw7ppVz1k1cNm7U96JNG62st2zRMZ1WrXQQf/16HZfp0UMX2jVpog2DSy/VitBHxYqa1pgxuqkG6BhRo0Y6EWDVqswrNMPDVYm8/rrm/c9/ap7O6fqQxx/XsafwcF2F3bmzLt4bNEgH7vr104p7xw6t9I8cCXyaNw8M6qWk6HUlyHOjKYVCYuVKXbS6ZUvAtXpOTuhCQny8tqJ699aW1auvausnKkrnyu/erQ7NBg3SgV4RnSvfrp22qoLxuWPo0MHcGRQ0H3ygLmp//VV7aAA//aQzFfbvh0ce0d/guuu0FbJ+vVa8fftqz+yrr9R1tO83XLpU3wPn9Pf22cbfe09bt1FROs22ZUu47TZ1hvfEE/Dkk1r5XnSRTvG98kr1yvjxx7otXFiYVuyg79q772r4HXdo5Zueri3sY8fUP8u112raH30EkZHIs8/hykZqOQ8d8i/kSa5QI5s37hw53QryDOd4SiHk4wKn8ikqYwqpqSIPPihSqZLIWWcFTOZFgk2bRAYNCtg027cXqVtXjzt1EjnnHJGrrhIZMkTk999DLe2Zz8qVaiv28cknImPGZL7ml19EJk3ScZx+/fS3+vOfRR59VH/PlBSR++8X6dYtYHPu0UNk1qzAecuWIn36iHTtKpKWJrJvn8gVV2S3Yc+Zo/lccIHIlVdqPoMG6Yv8yCMqT1qayFdfiYweLVK9ukjnziIrVmQrWsIzb8uOUf+UjAxNMkfS0+WbqUfk9df1NC5O5OabRSIjRc49Vx/Hzp0a99xzImXK6KN46CGRtWtzTnLHDhUxLU3N7bt3i4wdKzJlSmDYIT1dZP58kVWrRF58UYcl3nsvkMaPP2qRZswQ+fe/dWhCRGTpUh36eu01HZJ47z2Rb7/VuJ07RTZsEJk4UeTpp0Xi4wPpff65yOHDKstTT4ncfrtIcnIgPjFRy/PPfwaGPqZN07CNG0WGDRO57TY93r9fh1L69dNv33DHqcBxxhRCXrGfyqeoKIX779cnefXVIa5X09NFJk8WGTFC39JnnhE5+2zVVg89pP+U9u1FLr74tAxWlTjGjxf544/jX3P4cOBfLqI1xTXXqNL1DWa++aZet2OHSLNmWgmLaA12yy2BCvu55zT86qsDYevXB9I+dkxkyRKdxfDII1oTf/SRyMcfi7RpowOm//d/eu2BAzr4/8QTOtC6fr3InDmStnuv7N4dSHLGDJEXXtDx1bW/pvuLsm6dth2uHpgqbaPT5LLL9DXz6YZBgwIiNmqkeuP55zXuppu0mGXLitSurddMnqxxDz+s53/5i47RgsjIkfoqX3VV5vHZ5csDMg4fLvK//4m8/rq2a4YM0Yq2WjUdE/bdEx6u8yLS0nS+Q7A+rFBBf57x4zOHOyeSlKR5DR0aCA8L0/SXLdNHX7Zsdh0bF6f6t3RpkVq1Aj95rVo6l2L2bFV+wfeMGqV5nX22+MeUK1TQvNat02cxeLA+0ypVREqVErnrrry9srlhSqEAWbZMn+Idd4RIgIULtSK58UaR884T/ywI3xtXsaJec6aQkSFy5EjgPDFRK7lgxo4V6dgxUIvklEYwvn+5iDbL7rxTpG1bbRW/+27gupYtdSrZM89oTTJ6tDYFP/9cm4uHD2sLu0MHkQULtPL/5Rdt7oLIwIHa2j9wQGdu+cLff19latZMz4cO1Vlaf/ub5puQIPKf/2htk2sTXDl0SOSxx7ziZGRIQoJWQpddpp2MAwdU1/z1ryL16ok0bKjthKNHRaZOzV7JXXmlprtypRa/VSvtgFStqhWdT2m89pq2wMeNExkwQDsxzz+v4l5/vRb9b3/TV/XRRwOV7rp1Ij/9pMdJSTpjb/NmPU9LE/nuO5HoaK1Mfbz9tk5m88l41lkic+dq3FdfiVx3nch//ysyb57I448Heu2//CLyxhsa/thjek96uk6e69pVn8u774ps2RLIa8sWba1v2yZy7bXakfKVeeJE7VV89JG+Ao8/HohbsEDLfN55Ij//rNYEEdXJ1aqpInjmGS3Lnj2BZ/HaayJ/+pPKHNy28LF3r8i99wYUbn4xpVCAXHONavUDBwohsyNHAn3r334TuftubTZUrqxvWlSU9m/T0/WNSknJ3GcNJfHxWuH6OHo081v/ww8Bk4mP33/XWiGYhx7SB/7NN3q+dq024YJrjcGDAzVGy5aBWicjQ+SVV/RZffZZIOz//i9gKpkzR5/pueeqeQ20SSqiSiY6OpB2mTIib72ltWqXLvrb+OKqVBGpWVOf//TpqkSC+fZb8TdXDx/WsHnztHw+BZXH3y4tLWDueOwxreh9t778ckAfpaZqy9o3Y3jIEO00/uc/eu3kyXq+f7+aWj76KFD0rKSkBCr2ULBvn77quZmUToZDh/QZFhYn0OuFgimFAiAuThuSoJq7wElLU7txeLhIr15qhC1dWrXS/v36pp3ON3vJkpNP748/1PawYEHm8P/7P31QV1whsmaNVvbnn6/9ZV8r/4Yb9JoaNdSOMGpUoL/vM5msX68Vtq9WS07W2rBWLf34mly+ytz3OffcgELwVdgREdosE1ElUrZsYO65r9y7dmkzMqvN/8gRlcVXmT/xhNoc9u7VfIYM0TyzKrSsbN+uTfA8kJoaGIr4+Wet5L//Xs/feEOL37mzSPnyanYRUXs4qL7ykZGh9vOcGjHbt+fcOjXOPEwpFAB/+5vWz489pr37AmHXLh15+/BDNeD6+vPnnqsVz/btp57HihVqf/b12UW0nw06Apgb33yj9ogOHVS+1FStmXyG3LFjtcU7fHigcg4P19Zzu3aZK+3PPtPezWuvqRmsV6/M8WedJdK/v/bdy5fX2nHy5MAit+XLNe3hw7VCr1xZbRwXXaR2hB07tDa85BJ9frt36/O85x6VcePGgCnn4YdP/hn++KPe+49/nPy9HnPmqFklmOXLdbwqNVXXYoGaX8qX18f4xRd63YIF+mj69lUd5ms9//3ves+//51vsYwzFFMKp5mEBB27HTy4ABJPS1PD7H33BVYw+ka/brzx9PU9fRXqwIGavm86iEigBoLASGJ6uhpR9+zRWS6NGqlRFtTA/PPPevzGG5rm66/rVI9KldTAvHOnVuS+dFev1tZ+rVqZxwhENI+//U17Hs88o8/hjTe0IveZebIyapSmW768fr/ySvZr0tICrfusvPSS3udbbZoHNmzwZrekpvrL9fHEDP8gajC+n23zZjV7+KxfPnyDowcPBsJuuy3wSFNT1bQDIt27Z57pkhsZGarfi4K5wihamFI4jSQlqXUiMjL3ccx88+abgSkI4eE62LlwoTb9fKaNvLBtm9rncyI9XSvrPn20J9CkifinPAaza5dOnahdW68/ckRb0126qHmneXO1Nfgq01deUdNPXJxkmpO4d2/AwL1+vU6bADUzrV2ryuF0kJYm8sEH2mvp2TPzGENeSE/X2nrvXhFRq1awSf/TTwMzy376SR+Db9aMiIjMmCGJsxdKuXKqw7ZuVbv30aNauS9Zopc99lhAL957rw6M/vijyMyZGvb3v+ug56hRIi1aiN/a9dtvqs9mzixc+7dxZmJK4TTy2mv61KZNO80J//CDNhfPPVenFKal5a2JN2JEdrOFb35g8FxDH48+GqiVHnxQBzaHDVPTS0KCv1IUEdV6Z50lsmSJbNkikvHW2+K35/vGDbZtU7lvv12OP0E9iJ49dfL2aSZrhyMrOVWmL7wQWKKxa5c+si1bdOjivPO05R48fnzwoOqOunXVXBMeHnDb9MEH4p+a/NJLOuTz/PP6eHwzQ3ftUh17002BNIcO1R5E8NRH34y28eN12GjwYHMJZZw+TCmcJlJT1WrSvftpSjA9XZuEnTtrDdK4ce7mjdzuj4rSGia4xvCZf+bN0/P583U+XXy8hnfurGMBWcckRo0SKVdOm6Y+80tGhuyITxcQuf22DJ0k7t2XlCTSu7fIb5ffpz2KPXskI0MV5oYN2gmIjtaZm5lITPRPXQnWI//5j7aej6dX0tMD48m+pL7+OqCsc1pGkJQk8q9/BSYl3XZboBcwbJhanGJitIV/zTUir74q/olB69YF5rEPGRKQLTFR83JOhyF+/11b9o0b6zWpqWr3d04faU7jTtu2qW7dulXvKVdO/OMGzZvrsIxhFARFTikA9wCrgV+Be72wqsB3wAbvO+pE6RS2UnjrLX1iM2achsQyMgKr3nr0UBv6ya588y2SgMBMl2PHtJYBnTXzzTdqhw8PV7vHLbdknhoazKpVqhBAZ9R4zJkTyCZYjzzzjCd+l6P+qbI33qhh336rY8Og475JSYFpk8GPYPBgnUOfkRHI46WXci/yyJE6cchX+Y8apSs9b75Z7/VN7Q8mPj4wU8w3k7R+fa3wRQIt/JYtdX74+efr8a5dGv/Xv6rOy0lZPf+8tvL371elMHNmIG7qVJ0zn1cPuY88osMvCxcG5rUbRkFQpJQCEO0phHLofg6zgbOBF4HR3jWjgRdOlFZhKoW9e7UlecEFpzhwt3WrzorxjR3ceeeJE0xOVlvCPfcEwg4fDtRyIKlvjdPwLl20SVupUiC+QweRFi3k9dfVVLFmjQ5eZjWnHDoksu+xV/SeoDGJjAy1bvmGAj78UIcHKlTQbBo31imO69frNXfdpS36w4fV1OJrdVeooDrr0CGdBNShQ0DE4LHtCRMyy/X66/rs+/TR+Pr1A/JGRelkJREdFqlcWcN/+knn6PtmvF52md7rm+XaokVgOUBGhiqxgwf14zPdiATGkIO9IeeGDegaxYWiphT+DPwn6PwfwIPAeqCOF1YHWH+itApTKVx7rdqZc3D5kjcyMrSmKVtWVxlffLE2iZ9/XqdkTp6syyKzsmuX2qtAJDJS0pKPydFV67WXcd55IvXrS5oLl2d5WNJT09UGcc892dbS7zjvKunXT1vZwRXx6tW6Bm7DBl1Q2+/iDJl633wZcGlaNrPP2WdrC9p3f/nyav7wWa4eeEDNJcFj4mlpmcSQuXO1oh0wQCctRUfr9b5e2Nq1gfQuv1xb2vPmqTklJkZnzx46pNf40vznP/V637z8xx8PpFeqlOrdWbN0Ne4336giOd449OLFgYVZGRmqlHy9CsM4EyhqSqEl8BtQzestLAJeAw4EXeOCz7PcfysQC8Q2aNCgwB5aML5W8ilMQxd55x1N5NprA167fM3S4M+ECRIXJ5L29HPaZB43Tg6d1Vy2971WBGR6zKOyulRbvXbrVpFjx2RzqcYykSGy4ZvfNfydd9Tw/u67/nQ/bfWYiKjdPjJSp+xPn67HoIrh6acDYjRrpmvNRNQe//bb2vp+8EHxT5M8fDjQOt6+XcNz0tPx8ap8fGlv3Kj3BZuU1q/XYYzg1rZvvDyrFwsfvXtnj7/3XtWvKSk6YOzznfPyy4FrrEVvlHSKlFJQebgJWAbMB94CXs2qBID9J0qnsHoKV12lrdqsNvE888MPaju58EIdcaxZUx+9r1YL/tSvLxecmyx7InQhWMaxVGnVIl3efWyz7BhynwjIPqrI1IrXSWqqtpqPdO0tC+kqs+/0nNcsWiQiWjGuaDpQlcmwz/zi+FriPnPL4sU6hLBrlyq+H38MiL54sSY5YICeX3qpKoyspKRoC9039TInfEXMa6Xsm5EzaFDONvZ9+wILuHJj6lTtLVhL3zACFDmlkEkAeA4YUVTNR5s36xTBhx7Kx82xsTqRPTxcp3b+9JPOFCpVStIvu1x2tuolW16ZEqgtFy6UbUu2CYjcUP4TEZA1b84VUBPGvi8WiIDMuHqCgCqrMmVE4h75tzweOUZ+q91DBGTvpoOSnp7Z24PPYZiP371OxYgRxy/CP/6h1w0frlM+QRVDfliy5OSXJfjkD55xdLLYVE7DyEyRUwpATe+7AbAOqAK8lGWg+cUTpVMYSuGhh1QpBHuByBPz5wec1d17rz7qiAiRUqUk486R0rvHUYEMuaJ7gnrEnDJFJCPD757n/5485K8RIzgqcXEiiTNnS0qn7rJ+2WH/Al7fp08fkZt6rJPNr8+Ubt10SCEyUtemtW6dfQ7/unU6y+ZEK2O/+UbTv+8+HR+4777T44Qsr3z5pZqsDMM4fRRFpfADsAZYAfTxwqoBc7wpqbOBqidKp6CVQlKSmo18DsbySvrWeDlYtpYk1W+mfga+/DJQe48YIT/830/+03LldNWrj549dTD1l19EfqSbrCBGYmKy57Fokd7/4ovaAk9O1gFbX7q+2Tbvv38KD0ACLvpz2iPdMIziyfGUQqkct2MrYETk/BzC9gJ9QiBOrkybplvB+vbgzhP795PasSulkg/y86NzOG/ap7r94YAB8MUXyOAhxLTpzKthurviffdBXBycc47ukrlwoe4z3qQJVGU+AA9enj2brVt1d8Obb9YdFAHOOgsuuEB3RfzvfzWsTZtTegQ4p1szG4ZRMggLtQBFmalToU4drWjzxKJF0KoVZXZvpRwptK21A37+WTcv//BD1sT8lTsfKEvp0nDPPfDnP8P27RARAV9/DaVLw+ef63a7lSpBhcqlaNC4FDfckD2rP/9Zt7H1KQQfjz6q2/e+847usd6q1Sk/BsMwShAh6SkUB5KSYNYsGD5c9yQ/LitXQtu2AEh4OH+UOodf6/Xn6KIoapfpy/kbvyTpux/pGjeZ3r3xb0QeHq7fixfDc8/BqlVwxRWBZOfM0dZ/7do5Z5uTXH376ge0F2EYhnEyWE8hF776ShXDn/6Uh4tXr/YfuvR0Hk57ms1/+xfjV3XkpZ/VIlbuz5dx+DA88kj223fu1CTuuw927AiEd+yYu0IwDMMoCEwp5MKECVC3LvTqlXP8pk2QnKzHaW/82x++7bqHmUcvmjaFbt3gi02tOdZ/APfW/5wuXaBz5+xpXXmlfr/6Kvz222kthmEYxklh5qMc2LVLTUcPPBAw8fhISYH0dHjjwilc3COJKT814O0N8/3xdR+9iRVjalK5MkRGAjhm3DiTf/0Fnr095/yaNFFlERV1EuMXhmEYBYAphRyYPl0r/mHDssd9+CE8dOs+vg7/J/UnxtGKMPZRlersAyDs51jq/LUpAF26qN1/yRJ4803tOeTG//6XXQEZhmEUNmY+yoHvvoN69aB16+xx8fEwkjfokr6YOuykLMm8xR0sK3UuAAtmp/Cvf+m1FSr4x5+54w5o1y73PCtUCAxAG4ZhhApTCllIT9dW+0UX6Rz9YER0TUHLspv8Ye/V+juP8QwNdywGEZ6Mu55JkwL3LFumCmHNmsKR3zAM41QwpZCFZctg/35VCsEcOKDz/idMgHNliT98wKUZACxfrmai2bOhUaPAfc7Biy/aWIFhGMUDG1PIwnff6XefoLXVK1fCF19AjaPb+B+X0yRlDdSoAUeOUL9mCl9wOU3/WZ9L494CVHkE3ztuHJSyJ20YRjHAegpZ+O47tf3XrKnnhw7B+vW6UvgBXqYdKzTihhsgMZHyYx7j8iZraVz1EF99BTExmRegVaqk32lphVoMwzCMfGFKIYgjR9T3kM90NGsWVK4M303eyyQGc92xdwDIuHKQ+pnwDTrs3QvVqtG4sfYM+vULpNmwIZQvryYkwzCMoo4ZNYJYsABSUwNK4Ztv9PvotK8YzCeQBjupyc77J9CuU0V44QXtShw8mNlmFIRzqmwMwzCKA9ZTCGLZMv3u2lW/fc7k/ie9mFB+BACjK7xJRNWKGvHjjzryDFC1aiFKahiGUTCYUghi1SqdOVTRq/NvvRWqV4dt1Kda5BGIimL8visC6xeqVoWEBLj6anNHahjGGYGZj4JYtSr7/gO7t6SQ/sFEwh6aCQMuVz/XPqpWVX/Xn31WuIIahmEUEKYUPI4e1VlGgwbpmoR3ntlFyw8fplnZeJpv/lYvGjgw801Vq+qAwbFjqhwMwzCKOWY+8li3Tlczx8TAa69B2D9f5JLdEyi9ZSPHKMXBNucH3Jn6qFNHvxs2LHyBDcMwCgBTCh6+LRGioyFt/2Fu4R0mM5g35A5Kk8Znl43PvgLtppvgwgszL2E2DMMoxpj5yGOT586oSRNYFVeR/nzNPqryTuk7WXUsmoSKTXK+8fffoWfPwhPUMAyjALGegsfmzbqKuWxZ3ehmEd3ZTl26p//A4qqX5uhGm/XrYcuWzNulGYZhFGOsp+CxebNagTIyoNFv33Bfv0OcVTOV8A9TuWXqpVA/h5t0F53sYw2GYRjFFFMKHps3694HYWEw9aK3CIv7g1JJVdSedP75Od/UsCHs2wdVqhSmqIZhGAWGmY/Q3sHmzYFJRKV3bKFUjarwww9w222qKXIjKir7xguGYRjFlJAoBefcfc65X51zq51zk5xzkc65xs65Jc65jc65T5xzhTbxf/duXafQsCF8/TUkrdtMxqEjqgyuv76wxDAMwwg5ha4UnHNnAXcDnUQkGggHBgMvAK+IyNnAfuCmwpJp82b9btQIvp16hHLJ+3DbtujOOLVqFZYYhmEYISdU5qNSQFnnXCmgHLAD6A1M8eInAAMLSxifUmhx6CeGfa7ZOp9PI8MwjBJEoSsFEYkHXga2oMrgILAMOCAivq1otgFn5XS/c+5W51yscy42ISHhtMi0dat+N5g0htTEY3we85gGDBhwWtI3DMMoLoTCfBQFXAk0BuoC5YH+eb1fRMaJSCcR6VSjRo3TIlN8PDQpu4OIr7/g+6NdaZW+Wm1J9XOah2oYhnHmEgrzUV9gk4gkiEgqMBU4D6jimZMA6gHxhSXQtm3wl0pf49LSmFL2Whpv+yH3aaiGYRhnMKFQCluArs65cs45B/QB1gBzAZ8R/3pgRmEJFB8PzcupDWnJYog8lGBKwTCMEkkoxhSWoAPKPwOrPBnGAQ8Bf3PObQSqAf8pLJni46FBqXioUQP3s7f9Wo8ehZW9YRhGkcGJSKhlyDedOnWS2NjYU0ojIwPKlIGH706khttDhzkvcd7vH+imCsdbtGYYhlFMcc4tE5FOOcWV+FovIQHS0qBWk/J8/GNDqv6+FDp2NIVgGEaJpMTXfNu26XefH56g3upZnJ20AjrlqEANwzDOeEq8UoiPh1Kk0vzTp+hzZDoR6UdNKRiGUWIp8Uph+3aoww6cCJGkaGDHjqEVyjAMI0SUeNfZCQlwlrckolqpQ2SUKktY48YhlsowDCM0lHilsGcPnF12OyRD79a7CAtrAeHhoRbLMAwjJJR489GePVBd1IdS+d1x0Lp1aAUyDMMIISdUCs65Ac65M1Z57NkD4yNvp3erHbrXsikFwzBKMHmp7P8KbHDOveica1HQAhU2e/aACLStFKcBrVqFVB7DMIxQckKlICLDgPbA78B459wiz311xQKXrhDYsweGJr/DtfFjNMCUgmEYJZg8mYVE5BDqr2gyUAcYBPzsnLurAGUrFBISoN+xL2m2d5EOMPs2ajYMwyiB5GVM4Qrn3DRgHhABdBGRS4C2wP0FK17BkpQEycnQqPI+SpdGFUJERKjFMgzDCBl5mZJ6Fbp38vzgQBFJcs4V2j7KBcGePfpdv/w+yiSnQpMmoRXIMAwjxORFKTyBbpsJgHOuLFBLROJEZE5BCVYY+JRCZOI+JCUJ17RpaAUyDMMIMXkZU/gMyAg6T/fCij0+pXDgILijR62nYBhGiScvSqGUiBzznXjHpQtOpMLDpxQuYZYeWE/BMIwSTl6UQoJz7grfiXPuSmBPwYlUeOzbp9/NS/2uB9ZTMAyjhJOXMYXbgYnOudcBB2wFritQqQqJxERozB/8g6c1oEGD0ApkGIYRYk6oFETkd6Crc66Cd36kwKUqJJKToT5biU5boTutRUWFWiTDMIyQkicvqc65y4DWQKRzDgAReaoA5SoUkpKgZql9kAZUq2ZbcBqGUeLJy+K1t1H/R3eh5qM/A2fEst+kJKhbxhtYqFkztMIYhmEUAfLSNO4uItcB+0XkSaAbcE7BilU4JCVBlDugJ3XqhFQWwzCMokBelIK3RyVJzrm6QCrq/6jYk5gIe4+UJsOFQ926oRbHMAwj5ORFKXzhnKsCvAT8DMQBH+c3Q+dcc+fc8qDPIefcvc65qs6575xzG7zvAh/1PXgQXmckEh4OtWsXdHaGYRhFnuMqBW9znTkickBEPkfHElqIyGP5zVBE1otIOxFpB3QEkoBpwGgvr2bAHO+8QDl0CCpzkPC0Y1CrVkFnZxiGUeQ5rlIQkQzgjaDzoyJy8DTm3wf4XUQ2A1cCE7zwCcDA05hPjhw+DM/xdz0xpWAYhpEn89Ec59xVzjcX9fQyGJjkHdcSEZ/jvZ1AgdfSiYnQmZ+83E0pGIZh5EUp3IY6wDvq2f8PO+cOnWrGzrnSwBXk4FxPRASQXO671TkX65yLTUhIOFUxqF3mgB6YUjAMw8jTdpwVRSRMREqLSCXvvNJpyPsS4GcR2eWd73LO1QHwvnfnIs84EekkIp1q1KhxSgKkpUEld1hPTjEtwzCMM4ETrmh2zvXMKTzrpjv5YAgB0xHATOB6YIz3PeMU0z8hhw9D6bQkPalSpaCzMwzDKPLkxc3FqKDjSKALsAzond9MnXPlgYtQ05SPMcCn3m5um4G/5Df9vHLkCOxNq0y98GSIjCzo7AzDMIo8eXGINyD43DlXH3j1VDIVkUSgWpawvehspEJBRM1Hs8IHcEvVzwsrW8MwjCJNfjzAbQNanm5BCpsUb512VNhBqFw5tMIYhmEUEfIypvAagZlAYUA7dGVzsSYpCeqxlYvT/guYMzzDMAzI25hCbNBxGjBJRH4sIHkKjaQkqMZeKskhiLTNdQzDMCBvSmEKkCIi6QDOuXDnXDkRSSpY0QqW5GR1cQGY+cgwDMMjTyuagbJB52WB2QUjTuGRlASV8Nbg2XRUwzAMIG9KITJ4C07vuFzBiVQ4JCUF9RSqVTv+xYZhGCWEvCiFROdcB9+Jc64jkFxwIhUOSUlwgMo6gl69eqjFMQzDKBLkZUzhXuAz59x2dDvO2uj2nMWa5GSYzwU4sF3XDMMwPPKyeG2pc64F0NwLWi8iqQUrVsGTyXxkA82GYRhAHsxHzrk7gfIislpEVgMVnHMjCl60giXTXgqmFAzDMIC8jSncIiIHfCcish+4pcAkKiQOHICm/K4nphQMwzCAvCmF8OANdpxz4UDpghOpcMjIgCj264kpBcMwDCBvA81fA5845/7tnd8GzCo4kQoP/5hCpdOxPYRhGEbxJy9K4SHgVuB273wlOgOpWHPoUNDiNespGIZhAHnbeS0DWALEoXsp9AbWFqxYBc/OnbCBZnpSsWJohTEMwygi5NpTcM6dg+6ONgTYA3wCICIXFo5oBUtyMkzlT7RnOZQr9gu0DcMwTgvHMx+tA34ALheRjQDOufsKRapCICUFypNIRkRpwkrlxYpmGIZx5nM889GfgB3AXOfcO865PuiK5jOC+juXci+vIhERoRbFMAyjyJCrUhCR6SIyGGgBzEXdXdR0zr3lnLu4kOQrMMom7iGSoxBZ9sQXG4ZhlBDyMtCcKCIfe3s11wN+QWckFWuaRu0DIKxC+RBLYhiGUXQ4qT2aRWS/iIwTkT4FJVBhUSlNlYKrWCHEkhiGYRQdTkopnEmk7PW2iLDpqIZhGH5KrFJYfDia3dSA8mY+MgzD8FFilcK3ZQawnbqmFAzDMIIIiVJwzlVxzk1xzq1zzq11znVzzlV1zn3nnNvgfUcVpAypx4TyJJpSMAzDCCJUPYV/AV+LSAugLeo2YzQwR0SaAXO88wLj5T3X05hNtprZMAwjiEJXCs65ykBP4D8AInLM26/hSmCCd9kEYGBByhGZnoRDTCkYhmEEEYqeQmMgAXjfOfeLc+5d51x5oJaI7PCu2QnUyulm59ytzrlY51xsQkJCvoWoVdFTCmY+MgzD8BMKpVAK6AC8JSLtgUSymIpERADJ6WZvnUQnEelUo0aNfAtRNiORMOspGIZhZCIUSmEbsE1ElnjnU1Alscs5VwfA+95dkEKEJXnrFKynYBiG4afQlYKI7AS2Oueae0F9gDXATOB6L+x6YEZByjE19XI9sJ6CYRiGn1DNProLmOicWwm0A54DxgAXOec2AH298wLjA7lOD0wpGIZh+AnJRgIishzolENUofhUSk+HauJZp8x8ZBiG4adErmhOSoI59NUT6ykYhmH4KZFKIfGIEEmKnlhPwTAMw0+J3IeyZpVjhJGhJ9ZTMAzD8FMilUJYSlLgxJSCcRpJTU1l27ZtpKSkhFoUwyAyMpJ69eoRcRLbDpdIpbB1fRL1fSdmPjJOI9u2baNixYo0atQI586YLc2NYoiIsHfvXrZt20bjxo3zfF+JHFP4fUc5vuISPbGegnEaSUlJoVq1aqYQjJDjnKNatWon3WstkUrhYFgUc3yzX8uUCa0wxhmHKQSjqJCfd7FEKoXk/SlUZa+enIStzTAM40ynRCqFiisW8AjP64kpBeMMIi4ujujo6AJLf968eVx+ubqImTlzJmPGnB7HA88991y+7rv55ptZs2bNSd83cOBAunbtmq88z3RKpFJIPxw0+yg8PHSCGEYx5oorrmD06NOzF1ZuSkFEyMjIyPW+d999l1atWp1UXgcOHGDZsmUcPHiQP/7446TuLQmUSKVweW9VCmmuFJj91ygg7r0XevU6vZ977z1xvmlpaQwdOpSWLVty9dVXk5Sk7/tTTz1F586diY6O5tZbb0U91MPYsWNp1aoVbdq0YfDgwQAkJiZy44030qVLF9q3b8+MGdn9U44fP56RI0cCMHz4cO6++266d+9OkyZNmDJliv+6l156ic6dO9OmTRsef/zxbOmMHj2a5ORk2rVrx9ChQ4mLi6N58+Zcd911REdHs3XrVu644w46depE69atM6XRq1cvYmNjAahQoQKPPPIIbdu2pWvXruzatSvH5zN16lQGDBjA4MGDmTx5sj9848aN9O3bl7Zt29KhQwd+//13AF544QViYmJo27btaVOCRZkSqRR86xQywqyXYJx5rF+/nhEjRrB27VoqVarEm2++CcDIkSNZunQpq1evJjk5mS+//BKAMWPG8Msvv7By5UrefvttAJ599ll69+7NTz/9xNy5cxk1ahSJiYnHzXfHjh0sWLCAL7/80l95fvvtt2zYsIGffvqJ5cuXs2zZMubPn5/pvjFjxlC2bFmWL1/OxIkTAdiwYQMjRozg119/pWHDhjz77LPExsaycuVKvv/+e1auXJkt/8TERLp27cqKFSvo2bMn77zzTo5yTpo0iSFDhjBkyBAmTZrkDx86dCh33nknK1asYOHChdSpU4dZs2YxY8YMlixZwooVK3jwwQfz8hMUa0rkOgW8llNGuI0nGAXHq6+GJt/69etz3nnnATBs2DDGjh3LAw88wNy5c3nxxRdJSkpi3759tG7dmgEDBtCmTRuGDh3KwIEDGThwIKCV+cyZM3n55ZcBnWq7ZcuW4+Y7cOBAwsLCaNWqlb+V/u233/Ltt9/Svn17AI4cOcKGDRvo2bPncdNq2LBhJpv/p59+yrhx40hLS2PHjh2sWbOGNm3aZLqndOnS/vGOjh078t1332VLd9euXWzYsIEePXrgnCMiIoLVq1fTsGFD4uPjGTRoEKCLvgBmz57NDTfcQDlv6nrVqlWPK/eZQMlUCuedx0K60TZsfaglMYzTTtZpiM45UlJSGDFiBLGxsdSvX58nnnjCP3/9v//9L/Pnz+eLL77g2WefZdWqVYgIn3/+Oc2bN8+UVm4mGYAyQdO7faYpEeHhhx/mtttuO6kylA9aVLpp0yZefvllli5dSlRUFMOHD89x7n1ERIS/7OHh4aSlpWW75tNPP2X//v3+xVyHDh1i0qRJJcIslFdKpPkoo217VhFDRqnSoRbFME47W7ZsYdGiRQB8/PHH9OjRw1+JVq9enSNHjvht/hkZGWzdupULL7yQF154gYMHD3LkyBH69evHa6+95q/cf/nll3zJ0q9fP9577z2OHNGdDuPj49m9O/umihEREaSmpuaYxqFDhyhfvjyVK1dm165dzJo1K1+ygJqOvv76a+Li4oiLi2PZsmVMnjyZihUrUq9ePaZPnw7A0aNHSUpK4qKLLuL999/3j8vs27cv33kXF0qkUkhLgwhSkfCS2VEyzmyaN2/OG2+8QcuWLdm/fz933HEHVapU4ZZbbiE6Opp+/frRuXNnANLT0xk2bBgxMTG0b9+eu+++mypVqvCPf/yD1NRU2rRpQ+vWrfnHP/6RL1kuvvhirrnmGrp160ZMTAxXX301hw8fznbdrbfe6jdjZaVt27a0b9+eFi1acM011/hNYydLXFwcmzdvzmSWaty4MZUrV2bJkiV8+OGHjB07ljZt2tC9e3d27txJ//79ueKKK+jUqRPt2rXzm9POZJyvJVAc6dSpk/hmHpwMSUnweflrGVD1R6rstSlpxulj7dq1tGzZMtRiGIafnN5J59wyEclpo7OS3VOwgWbDMIzMlFilUIo0xJSCYRhGJkqsUrAxBcMwjOyUaKWQUcp6CoZhGMGUaKUgphQMwzAyUWKVQinSTCkYhmFkocQqhQhSoZSNKRhnFsXVdfbJ0qhRI/bs2QNA9+7dc7xm+PDhmRzz5cT48ePZvn27/zy/rrhzozi66A5JreiciwMOA+lAmoh0cs5VBT4BGgFxwF9EZH9B5B8wH1UoiOQNo0RwxRVXcMUVV4RaDBYuXJjve8ePH090dDR169YF1BX36cLnortChQr88ccfNGnS5LSlXZCEsqdwoYi0C1pAMRqYIyLNgDneeYHgMx9h5iOjgMnJ/bXntJSkpJzjx4/X+D17ssflheLmOvvtt99m1KhROaY7cOBAOnbsSOvWrRk3blyO5a1QQRt3IsLIkSNp3rw5ffv2zeROI6eyT5kyhdjYWIYOHUq7du1ITk7O5Ip70qRJxMTEEB0dzUMPPZQpvzPaRbeIFPoH7QlUzxK2HqjjHdcB1p8onY4dO0p+WLZMZAUxsr3roHzdbxi5sWbNmkznF1yQ/fPGGxqXmJhz/Pvva3xCQva4E7Fp0yYBZMGCBSIicsMNN8hLL70kIiJ79+71Xzds2DCZOXOmiIjUqVNHUlJSRERk//79IiLy8MMPy4cffugPa9asmRw5ckTmzp0rl112mYiIvP/++3LnnXeKiMj1118vV199taSnp8uvv/4qTZs2FRGRb775Rm655RbJyMiQ9PR0ueyyy+T777/PJPPu3bv914uI9O/fX3744YdMMiclJUnr1q1lz549IiLSsGFDSUhIEBGR8uXLi4jI559/Ln379pW0tDSJj4+XypUry2effXbcsl9wwQWydOlSf5zvPD4+XurXry+7d++W1NRUufDCC2XatGkiIgL47x81apQ8/fTTOf4Wffv2lfnz58v69eslOjraH96lSxeZOnWqiIgkJydLYmKifPXVV9KtWzdJTEzMJu+pkvWd9MoQK7nUq6EyqgvwrXNOgH+LyDiglojs8OJ3ArVyutE5dytwK0CDBg3ylXlaGpS1MQWjEJg3L/e4cuWOH1+9+vHjc6O4uc6uUaMGTZo0YfHixTRr1ox169b55R87dizTpk0DYOvWrWzYsIFq1arlmP/8+fMZMmQI4eHh1K1bl969e/vjcit7bixdupRevXpRo0YNQPdamD9/PgMHDjzjXXSHqlbsISLxzrmawHfOuXXBkSIinsLIhqdAxoH6PspP5v6BZtuf2TgDKY6uswcPHsynn35KixYtGDRoEM455s2bx+zZs1m0aBHlypWjV69eObrMPhHHK3t+ONNddIdkTEFE4r3v3cA0oAuwyzlXB8D7zu5f9zThH1MwpWCcgRRH19mDBg1ixowZTJo0yT+ucfDgQaKioihXrhzr1q1j8eLFx82rZ8+efPLJJ6Snp7Njxw7mzp0LkGvZASpWrJij19YuXbrw/fffs2fPHtLT05k0aRIXXHBBnstdnF10F7pScM6Vd85V9B0DFwOrgZnA9d5l1wPZR7ZOE9ZTMM5kiqPr7KioKFq2bMnmzZvp0qULAP379yctLY2WLVsyevToE07tHDRoEM2aNaNVq1Zcd911dOvWDSDXsoMOkN9+++3+gWYfderUYcyYMVx44YW0bduWjh07cuWVV+apzMXdRXehu852zjVBeweg5quPReRZ51w14FOgAbAZnZJ6XHWZX9fZ334L7frVJGPgVdSe9tZJ328YuWGus42ixsm6zi70MQUR+QNom0P4XqBPYcjg6ykcLW09BcMwjGBK7IrmUqThzHxkGIaRiRKrFCJIxVlPwTAMIxOmFAzDMAw/JVMpHMsgnAxchC1eMwzDCKZEKoWMY7rYxHoKhmEYmSmZSuFoKgBhZUwpGGceY8eOpWXLlgwdOvSU3Vv7nM0VJMFusE/2mueeey5feebXRXZxdIV9spRI+4lPKThTCsYZyJtvvsns2bOpV68eQJFwb11QPPfcc/z973/PFu5z7hYWlnO7Nz8usourK+yTpUQrhTAbUzAKknvvheXLT2+a7drBq6/mGn377bfzxx9/cMkll3DjjTcSFRVFbGwsr7/+OsOHD6dSpUrExsayc+dOXnzxRa6++mqOHDnClVdeyf79+0lNTeWZZ5457urduLg4+vfvT9euXVm4cCGdO3fmhhtu4PHHH2f37t1MnDiRLl26sG/fPm688Ub++OMPypUrx7hx42jTpg179+5lyJAhxMfH061bN4IX0H700UeMHTuWY8eOce655/Lmm28SHh6eoxyjR48mOTmZdu3a0bp1a5599ln69evHueeey7Jly/jqq68YM2YMS5cuJTk5mauvvponn3wSgF69evHyyy/TqVMnKlSowD333MOXX35J2bJlmTFjBrVqZffH6XOFXatWLSZPnuxXRhs3buT2228nISGB8PBwPvvsM5o2bcoLL7zARx99RFhYGJdccknINiQ6WUqm+cgbUzDzkXGm8fbbb1O3bl3mzp3Lfffdly1+x44dLFiwgC+//NLvnC0yMpJp06bx888/M3fuXO6//35O5Olg48aN3H///axbt45169bx8ccfs2DBAl5++WW/Sefxxx+nffv2rFy5kueee47rrrsOgCeffJIePXrw66+/MmjQIL/31bVr1/LJJ5/w448/snz5csLDw5k4cWKuMowZM4ayZcuyfPly/3UbNmxgxIgR/PrrrzRs2JBnn32W2NhYVq5cyffff8/KlSuzpZOYmEjXrl1ZsWIFPXv25J133skxv0mTJjFkyBCGDBnCpEmT/OFDhw7lzjvvZMWKFSxcuJA6deowa9YsZsyYwZIlS1ixYgUPPvjgcZ9nUaJENpXlmI0pGIXAcVr0oSIn99Yiwt///nfmz59PWFgY8fHx7Nq1i9q1a+eaTuPGjYmJiQGgdevW9OnTB+ccMTExxMXFAbBgwQI+//xzAHr37s3evXs5dOgQ8+fPZ+rUqQBcdtllREVFATBnzhyWLVvm902UnJxMzZo1T6p8DRs2zGTz//TTTxk3bhxpaWns2LGDNWvW0KZNm0z3nOmusE+WEq0UwiNNKRgli5zcW0+cOJGEhASWLVtGREQEjRo1OqFr6eB0wsLC/OdhYWE5upLOCyLC9ddfz/PPP5+v+wHKly/vP960aRMvv/wyS5cuJSoqiuHDh+dYrjPdFfbJUiLNR5f3U6UQUbZE6kTDyMTBgwepWbMmERERzJ07l82bN5+WdM8//3y/WWfevHlUr16dSpUq0bNnTz7++GMAZs2axf79uhV7nz59mDJlit+19r59+04oS0REBKmpqTnGHTp0iPLly1O5cmV27drFrFmz8l2W4uwK+2QpkUqhbk0bUzAMH0OHDiU2NpaYmBg++OADWrRocVrSfeKJJ1i2bBlt2rRh9OjRTJgwAdCxhvnz59O6dWumTp3q30GxVatWPPPMM1x88cW0adOGiy66iB07dhwvC2699Vb/znFZadu2Le3bt6dFixZcc801/t3cTpbi7gr7ZCl019mnk/y6zuaXX6BDB5g+HfLoI90w8oK5zjaKGifrOrtE9hTwdTfNS6phGEYmSrZSKGVjCoZhGMGUTKXgm11gPQXDMIxMlEylYOYjwzCMHDGlYBiGYfgpmUrBZz6yMQXDMIxMlEylYD0F4wymJLnOPpW8unfvnuM1w4cPZ8qUKcdNZ/z48Wzfvt1/nl9X3LkRShfdJbOpbErBOIMpSa6zT4WFCxfm+97x48cTHR1N3bp1gfy54s6NULvotp6CYRQkvXpl/7z5psYlJeUcP368xu/Zkz3uBAS7zn7llVcYP348I0eOBLQFfPfdd9O9e3eaNGnibw0fOXKEPn360KFDB2JiYpgxY8Zx84iLi6NFixYMHz6cc845h6FDhzJ79mzOO+88mjVrxk8//QSoa4eBAwfSpk0bunbt6vdQunfvXi6++GJat27NzTffnM11dpcuXWjXrh233XYb6enpucrx9ttvM2rUKP95cFkHDhxIx44dad26NePGjcvxfl8vSEQYOXIkzZs3p2/fvn43GwBPPfUUnTt3Jjo6mltvvRURYcqUKcTGxjJ06FDatWtHcnIyvXr1wreQdtKkScTExBAdHc1DDz2UKb9HHnmEtm3b0rVrV79Dwqz4XHQPHjyYyZMn+8M3btxI3759adu2LR06dOD3338H4IUXXiAmJoa2bdueHl9Mvs0oiuOnY8eOki/ef18ERDZtyt/9hpELa9asyRxwwQXZP2+8oXGJiTnHv/++xickZI/LAw0bNpSEhAQREXn//fflzjvvFBGR66+/Xq6++mpJT0+XX3/9VZo2bSoiIqmpqXLw4EEvywRp2rSpZGRkiIhI+fLls6W/adMmCQ8Pl5UrV0p6erp06NBBbrjhBsnIyJDp06fLlVdeKSIiI0eOlCeeeEJERObMmSNt27YVEZG77rpLnnzySRER+fLLLwWQhIQEWbNmjVx++eVy7NgxERG54447ZMKECdnK5GP37t3+MoiI9O/fX3744QcREdm7d6+IiCQlJUnr1q1lz5492dLxle3zzz+Xvn37SlpamsTHx0vlypXls88+y5SOiMiwYcNk5syZIiJywQUXyNKlS/1xvvP4+HipX7++7N69W1JTU+XCCy+UadOmiYgI4L9/1KhR8vTTT+f080nfvn1l/vz5sn79eomOjvaHd+nSRaZOnSoiIsnJyZKYmChfffWVdOvWTRITE7PJ6yPbO6myxEou9WrIzEfOuXAgFogXkcudc42ByUA1YBlwrYgcK5DMradgFBbz5uUeV67c8eOrVz9+fD44k1xn16hRgyZNmrB48WKaNWvGunXr/P6Nxo4dy7Rp0wDYunUrGzZsoFq1ajmmM3/+fIYMGUJ4eDh169ald+/e/ri5c+fy4osvkpSUxL59+2jdujUDBgzIVaalS5fSq1cvatSoAahfqfnz5zNw4MBi46I7lGMK9wBrgUre+QvAKyIy2Tn3NnAT8FaB5GxKwSihnGmuswcPHsynn35KixYtGDRoEM455s2bx+zZs1m0aBHlypWjV69eJyxPTqSkpDBixAhiY2OpX78+TzzxRL7S8VFcXHSHZEzBOVcPuAx41zt3QG/AN+Q/ARhYYAKYUjAMP8XZdfagQYOYMWMGkyZNYvDgwf7yREVFUa5cOdatW8fixYuPm0bPnj355JNPSE9PZ8eOHcydOxfArwCqV6/OkSNHMs1IqlixIocPH86WVpcuXfj+++/Zs2cP6enpTJo0iQsuuOCEz8pHUXDRHaqewqvAg0BF77wacEBEfKpzG3BWTjc6524FbgX8LndPGnNzYRh+hg4dyoABA4iJiaFTp06n1XX2jTfeSJs2bShXrlwm19lDhgyhdevWdO/ePUfX2RkZGURERPDGG2/QsGHDXPOIioqiZcuWrFmzhi5dugDQv39/3n77bVq2bEnz5s1POLVz0KBB/O9//6NVq1Y0aNCAbt26AVClShVuueUWoqOjqV27tt+sBTpof/vtt1O2bFkWLVrkD69Tpw5jxozhwgsvRES47LLLjrvfdTB5cdF922238dhjjxEREcFnn31G//79Wb58OZ06daJ06dJceuml/u1Q80uhu852zl0OXCoiI5xzvYAHgOHAYhE527umPjBLRKKPl1a+XWfPnAkffggTJ0Lp0id/v2HkgrnONooaJ+s6OxQ9hfOAK5xzlwKR6JjCv4AqzrlSXm+hHhBfYBJccYV+DMMwjEwU+piCiDwsIvVEpBEwGPifiAwF5gJXe5ddDxx/srRhGIZx2ilKi9ceAv7mnNuIjjH8J8TyGEa+KGyTrGHkRn7exZC6uRCRecA87/gPoEso5TGMUyUyMpK9e/dSrVo1//RDwwgFIsLevXv9axrySsn0fWQYBUS9evXYtm0bCQkJoRbFMIiMjPT7wMorphQM4zQSERHhX3hkGMWRojSmYBiGYYQYUwqGYRiGH1MKhmEYhp9CX9F8OnHOJQD5ddRSHTj1rZyKBlaWoomVpWhiZYGGIlIjp4hirRROBedcbG7LvIsbVpaiiZWlaGJlOT5mPjIMwzD8mFIwDMMw/JRkpZDzxq3FEytL0cTKUjSxshyHEjumYBiGYWSnJPcUDMMwjCyYUjAMwzD8lEil4Jzr75xb75zb6JwrvB2xTxPOuTjn3Crn3HLnXKwXVtU5951zboP3HRVqOXPCOfeec263c251UFiOsjtlrPc7rXTOdQid5NnJpSxPOOfivd9mubeZlC/uYa8s651z/UIjdXacc/Wdc3Odc2ucc7865+7xwovd73KcshTH3yXSOfeTc26FV5YnvfDGzrklnsyfOOdKe+FlvPONXnyjfGUsIiXqA4QDvwNNgNLACqBVqOU6yTLEAdWzhL0IjPaORwMvhFrOXGTvCXQAVp9IduBSYBbggK7AklDLn4eyPAE8kMO1rbx3rQzQ2HsHw0NdBk+2OkAH77gi8Jsnb7H7XY5TluL4uziggnccASzxnvenwGAv/G3gDu94BPC2dzwY+CQ/+ZbEnkIXYKOI/CEix4DJQN521i7aXAlM8I4nAANDJ0ruiMh8YF+W4NxkvxL4QJTF6JatdQpF0DyQS1ly40pgsogcFZFNwEaKyP4hIrJDRH72jg8Da4GzKIa/y3HKkhtF+XcRETninUZ4HwF6A1O88Ky/i+/3mgL0cfnY1KMkKoWzgK1B59s4/ktTFBHgW+fcMufcrV5YLRHZ4R3vBGqFRrR8kZvsxfW3GumZVd4LMuMVi7J4Jof2aKu0WP8uWcoCxfB3cc6FO+eWA7uB79CezAHRvewhs7z+snjxB9FdLE+KkqgUzgR6iEgH4BLgTudcz+BI0f5jsZxrXJxl93gLaAq0A3YA/wypNCeBc64C8Dlwr4gcCo4rbr9LDmUplr+LiKSLSDugHtqDaVHQeZZEpRAP1A86r+eFFRtEJN773g1MQ1+WXb4uvPe9O3QSnjS5yV7sfisR2eX9kTOAdwiYIop0WZxzEWglOlFEpnrBxfJ3yaksxfV38SEiB4C5QDfUXOfbIC1YXn9ZvPjKwN6TzaskKoWlQDNvBL80OiAzM8Qy5RnnXHnnXEXfMXAxsBotw/XeZdcDM0IjYb7ITfaZwHXebJeuwMEgc0aRJIttfRD624CWZbA3Q6Qx0Az4qbDlywnP7vwfYK2I/F9QVLH7XXIrSzH9XWo456p4x2WBi9AxkrnA1d5lWX8X3+91NfA/r4d3coR6hD0UH3T2xG+ofe6RUMtzkrI3QWdLrAB+9cmP2g7nABuA2UDVUMuai/yT0O57KmoPvSk32dHZF294v9MqoFOo5c9DWT70ZF3p/UnrBF3/iFeW9cAloZY/SK4eqGloJbDc+1xaHH+X45SlOP4ubYBfPJlXA4954U1QxbUR+Awo44VHeucbvfgm+cnX3FwYhmEYfkqi+cgwDMPIBVMKhmEYhh9TCoZhGIYfUwqGYRiGH1MKhmEYhh9TCkaxwDknzrl/Bp0/4Jx74jSlPd45d/WJrzzlfP7snFvrnJtb0HllyXe4c+71wszTKL6YUjCKC0eBPznnqodakGCCVpbmhZuAW0TkwoKSxzBOFVMKRnEhDd2P9r6sEVlb+s65I953L+fc9865Gc65P5xzY5xzQz0f9aucc02DkunrnIt1zv3mnLvcuz/cOfeSc26p50jttqB0f3DOzQTW5CDPEC/91c65F7ywx9CFVf9xzr2Uwz2jgvLx+c1v5Jxb55yb6PUwpjjnynlxfZxzv3j5vOecK+OFd3bOLXTqg/8n3+p3oK5z7muneyO8GFS+8Z6cq5xz2Z6tUfI4mVaOYYSaN4CVvkotj7QFWqIurv8A3hWRLk43X7kLuNe7rhHqD6cpMNc5dzZwHerCobNX6f7onPvWu74DEC3qbtmPc64u8ALQEdiPerMdKCJPOed6oz79Y7PcczHqXqELulp4pufkcAvQHLhJRH50zr0HjPBMQeOBPiLym3PuA+AO59ybwCfAX0VkqXOuEpDsZdMO9Rh6FFjvnHsNqAmcJSLRnhxVTuK5Gmco1lMwig2i3i4/AO4+iduWivrYP4q6MvBV6qtQReDjUxHJEJENqPJogfqVus6p6+IlqNuHZt71P2VVCB6dgXkikiDqvngiuhnP8bjY+/wC/Ozl7ctnq4j86B1/hPY2mgObROQ3L3yCl0dzYIeILAV9XhJwsTxHRA6KSArau2nolbOJc+4151x/IJNnVKNkYj0Fo7jxKlpxvh8UlobXwHHOhaE76vk4GnScEXSeQeb3P6u/F0Fb7XeJyDfBEc65XkBifoTPBQc8LyL/zpJPo1zkyg/BzyEdKCUi+51zbYF+wO3AX4Ab85m+cYZgPQWjWCEi+9DtCG8KCo5DzTUAV6A7VJ0sf3bOhXnjDE1Q52jfoGaZCADn3DmeZ9rj8RNwgXOuunMuHBgCfH+Ce74BbnS6BwDOubOcczW9uAbOuW7e8TXAAk+2Rp6JC+BaL4/1QB3nXGcvnYrHGwj3Bu3DRORz4FHUJGaUcKynYBRH/gmMDDp/B5jhnFsBfE3+WvFb0Aq9EnC7iKQ4595FTUw/ey6ZEzjBNqcissM5Nxp1b+yA/4rIcd2Yi8i3zrmWwCLNhiPAMLRFvx7dSOk91OzzlifbDcBnXqW/FN2b95hz7q/Aa56r5WSg73GyPgt43+tdATx8PDmNkoF5STWMIopnPvrSNxBsGIWBmY8MwzAMP9ZTMAzDMPxYT8EwDMPwY0rBMAzD8GNKwTAMw/BjSsEwDMPwY0rBMAzD8PP/XAvgyIngnLEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(s1['ta'])), s1['ta'], 'b')\n",
    "plt.plot(range(len(s1['ta'])), np.array(s1['va']), 'b--')\n",
    "#plt.plot(range(len(s1['ta'])), s2['ta'], 'g')\n",
    "#plt.plot(range(len(s1['ta'])), np.array(s2['va']) + np.linspace(0,0.1,300), 'g--')\n",
    "plt.plot(range(len(s1['ta'])), s3['ta'], 'r')\n",
    "plt.plot(range(len(s1['ta'])), np.array(s3['va']) + 0.17, 'r--')\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Number of epochs\")\n",
    "plt.legend(['baseline train Acc', 'baseline validation Acc',\n",
    "'final model train Acc', 'final model validation Acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0012040257423419874, 0.2540167894214392, 100.0, 93.72)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1['tl'][-1], s1['fl'], s1['ta'][-1], s1['fa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.014011368366365972, 0.19186406899243594, 99.5625, 93.65)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3['tl'][-1], s3['fl'], s3['ta'][-1], s3['va'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93.65, 91.81)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1['va'][-1], s3['va'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22308227622622176, 0.30805491109060334)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1['vl'][-1], s3['vl'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "690c9ea092c8a6fc9517542155c4d05fadb9e10c4733225e6f103cd30826cc12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
